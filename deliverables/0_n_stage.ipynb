{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> CLEAN THIS PLEASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#Scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    " #Correlation Heatmap\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "#Statistical Test\n",
    "from scipy import stats\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "#Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import KFold, RepeatedKFold, cross_val_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "\n",
    "import pandas as pd  # For creating and handling DataFrames\n",
    "import numpy as np  # For numerical operations\n",
    "from sklearn.feature_selection import chi2  # For Chi-square test\n",
    "from scipy.stats import chi2_contingency  # For Cramér's V calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset\n",
    "X_train_encoded = pd.read_csv('../project_data/X_train_encoded.csv', delimiter=',', index_col=0)\n",
    "X_val_encoded = pd.read_csv('../project_data/X_val_encoded.csv', delimiter=',', index_col=0)\n",
    "\n",
    "y_train = pd.read_csv('../project_data/y_train.csv',delimiter=',', index_col=0)\n",
    "y_val= pd.read_csv('../project_data/y_val.csv', delimiter=',', index_col=0)\n",
    "\n",
    "X_test_encoded = pd.read_csv('../project_data/X_test_encoded.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary classification dataset\n",
    "X_train_encoded_bin = pd.read_csv('../project_data/X_train_encoded_binary.csv', delimiter=',', index_col=0)\n",
    "X_val_encoded_bin = pd.read_csv('../project_data/X_val_encoded_binary.csv', delimiter=',', index_col=0)\n",
    "\n",
    "y_train_bin = pd.read_csv('../project_data/y_train_binary.csv',delimiter=',', index_col=0)\n",
    "y_val_bin= pd.read_csv('../project_data/y_val_binary.csv', delimiter=',', index_col=0)\n",
    "\n",
    "X_test_encoded_bin = pd.read_csv('../project_data/X_test_encoded_binary.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Encoding multiclass target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Encode the multiclass target and exclude 2. NON-COMP from the dataset, so it's easier to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "claim_injury_type_mapping = {\n",
    "    '4. TEMPORARY': 0,\n",
    "    '2. NON-COMP':7,\n",
    "    '5. PPD SCH LOSS': 1,\n",
    "    '3. MED ONLY': 2,\n",
    "    '6. PPD NSL': 3,\n",
    "    '1. CANCELLED': 4,\n",
    "    '8. DEATH':5,\n",
    "    '7. PTD': 6\n",
    "}\n",
    "\n",
    "y_train_encoded = y_train['Claim Injury Type'].map(claim_injury_type_mapping)\n",
    "y_val_encoded = y_val['Claim Injury Type'].map(claim_injury_type_mapping)\n",
    "y_val_encoded_final = y_val['Claim Injury Type'].map(claim_injury_type_mapping)\n",
    "X_val_encoded_final = X_val_encoded.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y_train_not_encoded: (198056,)\n",
      "Classes in y_train_not_encoded: ['5. PPD SCH LOSS' '3. MED ONLY' '4. TEMPORARY' '1. CANCELLED' '8. DEATH'\n",
      " '6. PPD NSL' '7. PTD']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Ensure y_train is a Series (if it's a DataFrame with one column, convert it to Series)\n",
    "y_train = y_train.squeeze()  # This will convert DataFrame with a single column to a Series\n",
    "\n",
    "# Create a mask for rows where '2. NON-COMP' is not present\n",
    "mask = y_train != '2. NON-COMP'\n",
    "\n",
    "# Apply the mask to filter out '2. NON-COMP'\n",
    "y_train_not_encoded = y_train[mask]\n",
    "\n",
    "# Print the shape of the filtered target\n",
    "print(\"Shape of y_train_not_encoded:\", y_train_not_encoded.shape)\n",
    "\n",
    "# Check the unique classes in the filtered target to confirm that '2. NON-COMP' is removed\n",
    "print(\"Classes in y_train_not_encoded:\", y_train_not_encoded.unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_encoded_filtered: (198056, 58)\n",
      "Shape of y_train_encoded_filtered: (198056,)\n",
      "Shape of X_val_encoded_filtered: (84881, 58)\n",
      "Shape of y_val_encoded_filtered: (84881,)\n",
      "Classes in y_train_encoded_filtered: [1 2 0 4 5 3 6]\n",
      "Classes in y_val_encoded_filtered: [2 4 0 1 3 6 5]\n"
     ]
    }
   ],
   "source": [
    "# Create a mask for rows where y_train_encoded is not equal to 7 (non-comp class)\n",
    "mask_train = y_train_encoded != 7\n",
    "\n",
    "# Filter X_train_encoded and y_train_encoded based on the mask\n",
    "X_train_encoded = X_train_encoded[mask_train]\n",
    "y_train_encoded = y_train_encoded[mask_train]\n",
    "\n",
    "# Apply the same mask to y_val_encoded and X_val_encoded\n",
    "mask_val = y_val_encoded != 7\n",
    "X_val_encoded = X_val_encoded[mask_val]\n",
    "y_val_encoded = y_val_encoded[mask_val]\n",
    "\n",
    "# Verify the shapes of the new filtered datasets\n",
    "print(\"Shape of X_train_encoded_filtered:\", X_train_encoded.shape)\n",
    "print(\"Shape of y_train_encoded_filtered:\", y_train_encoded.shape)\n",
    "print(\"Shape of X_val_encoded_filtered:\", X_val_encoded.shape)\n",
    "print(\"Shape of y_val_encoded_filtered:\", y_val_encoded.shape)\n",
    "\n",
    "# Check unique classes in the target to confirm \"non comp\" is removed\n",
    "print(\"Classes in y_train_encoded_filtered:\", y_train_encoded.unique())\n",
    "print(\"Classes in y_val_encoded_filtered:\", y_val_encoded.unique())  # Corrected this line\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Separate Numerical and Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1  Binary separation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Separare the Numerical and Categorical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_columns_bin = ['Age at Injury', \n",
    "                   'IME-4 Count', \n",
    "                   'Number of Dependents',\n",
    "\n",
    "                   'Industry Code_encoded_0',\n",
    "                   'Industry Code_encoded_1', \n",
    "\n",
    "                   'WCIO Cause of Injury Code_encoded_0',\n",
    "                   'WCIO Cause of Injury Code_encoded_1',\n",
    "\n",
    "                   'WCIO Nature of Injury Code_encoded_0',\n",
    "                   'WCIO Nature of Injury Code_encoded_1',\n",
    "\n",
    "                   'WCIO Part Of Body Code_encoded_0', \n",
    "                   'WCIO Part Of Body Code_encoded_1',\n",
    "\n",
    "                   'Industry Code_freq', \n",
    "                   'WCIO Cause of Injury Code_freq',\n",
    "                   'WCIO Nature of Injury Code_freq',\n",
    "                   'WCIO Part Of Body Code_freq',\n",
    "                   'Carrier Type_freq', \n",
    "                   'Carrier Name_freq',\n",
    "                   'Accident Datemonth',\n",
    "                   'Accident Date_Season_Spring', \n",
    "                   'Accident Date_Season_Summer',\n",
    "                   'Accident Date_Season_Winter',\n",
    "                   'Days_between_C-2 Date_Accident Date_log',\n",
    "\n",
    "                   'Days_between_Assembly Date_Accident Date_log',\n",
    "                   'Average Weekly Wage Imputed_log']\n",
    "\n",
    "cat_columns_bin = ['Carrier Type_Self-insured Private Entity',\n",
    "                   'Carrier Type_Self-insured Public Entity', \n",
    "                   'Carrier Type_Special Funds',\n",
    "                   'Carrier Type_State Insurance Fund', \n",
    "                   'C-3 Date_nabinary', \n",
    "                   'Average Weekly Wage_nabinary',\n",
    "                   'First Hearing Date_nabinary',\n",
    "                   'Alternative Dispute Resolution_binary',\n",
    "                   'COVID-19 Indicator_binary',\n",
    "                   'Attorney/Representative_binary']\n",
    "\n",
    "# Create subsets\n",
    "X_train_num_bin = X_train_encoded_bin[num_columns_bin]\n",
    "X_train_cat_bin = X_train_encoded_bin[cat_columns_bin]\n",
    "\n",
    "X_val_num_bin = X_val_encoded_bin[num_columns_bin]\n",
    "X_val_cat_bin = X_val_encoded_bin[cat_columns_bin]\n",
    "\n",
    "X_test_num_bin=X_test_encoded_bin[num_columns_bin]\n",
    "X_test_cat_bin=X_test_encoded_bin[cat_columns_bin]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Multiclass separation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Drop the ones associated the the target class 2. NON-COMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining columns after dropping 'non comp' features:\n",
      "54\n"
     ]
    }
   ],
   "source": [
    "# Drop features associated with \"non comp\" in their name\n",
    "columns_to_drop = [col for col in X_train_encoded.columns if \"non-comp\" in col.lower()]\n",
    "\n",
    "# Drop the columns from the dataset\n",
    "X_train_encoded = X_train_encoded.drop(columns=columns_to_drop)\n",
    "X_val_encoded = X_val_encoded.drop(columns=columns_to_drop)  # If you have validation data\n",
    "X_test_encoded = X_test_encoded.drop(columns=columns_to_drop)  # If you have test data\n",
    "\n",
    "# Verify the remaining columns\n",
    "print(\"Remaining columns after dropping 'non comp' features:\")\n",
    "print(len(X_train_encoded.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Separate the numerical and categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_columns = ['Age at Injury', \n",
    "               'IME-4 Count', \n",
    "               'Number of Dependents',\n",
    "               \n",
    "               'Industry Code_encoded_5. PPD SCH LOSS',\n",
    "               'Industry Code_encoded_3. MED ONLY',\n",
    "               'Industry Code_encoded_4. TEMPORARY',\n",
    "               'Industry Code_encoded_1. CANCELLED', \n",
    "               'Industry Code_encoded_8. DEATH',\n",
    "               'Industry Code_encoded_6. PPD NSL', \n",
    "               'Industry Code_encoded_7. PTD',\n",
    "               \n",
    "               'WCIO Cause of Injury Code_encoded_5. PPD SCH LOSS',\n",
    "               'WCIO Cause of Injury Code_encoded_3. MED ONLY',\n",
    "               'WCIO Cause of Injury Code_encoded_4. TEMPORARY',\n",
    "               'WCIO Cause of Injury Code_encoded_1. CANCELLED',\n",
    "               'WCIO Cause of Injury Code_encoded_8. DEATH',\n",
    "               'WCIO Cause of Injury Code_encoded_6. PPD NSL',\n",
    "               'WCIO Cause of Injury Code_encoded_7. PTD',\n",
    "               \n",
    "               'WCIO Nature of Injury Code_encoded_5. PPD SCH LOSS',\n",
    "               'WCIO Nature of Injury Code_encoded_3. MED ONLY',\n",
    "               'WCIO Nature of Injury Code_encoded_4. TEMPORARY',\n",
    "               'WCIO Nature of Injury Code_encoded_1. CANCELLED',\n",
    "               'WCIO Nature of Injury Code_encoded_8. DEATH',\n",
    "               'WCIO Nature of Injury Code_encoded_6. PPD NSL',\n",
    "               'WCIO Nature of Injury Code_encoded_7. PTD',\n",
    "               \n",
    "               'WCIO Part Of Body Code_encoded_5. PPD SCH LOSS',\n",
    "               'WCIO Part Of Body Code_encoded_3. MED ONLY',\n",
    "               'WCIO Part Of Body Code_encoded_4. TEMPORARY',\n",
    "               'WCIO Part Of Body Code_encoded_1. CANCELLED',\n",
    "               'WCIO Part Of Body Code_encoded_8. DEATH',\n",
    "               'WCIO Part Of Body Code_encoded_6. PPD NSL',\n",
    "               'WCIO Part Of Body Code_encoded_7. PTD',\n",
    "               \n",
    "               'Industry Code_freq',\n",
    "               'WCIO Cause of Injury Code_freq', \n",
    "               'WCIO Nature of Injury Code_freq',\n",
    "               'WCIO Part Of Body Code_freq', \n",
    "               'Carrier Type_freq',\n",
    "               'Carrier Name_freq',\n",
    "               \n",
    "               'Accident Datemonth',\n",
    "               'Accident Date_Season_Spring', \n",
    "               'Accident Date_Season_Summer',\n",
    "               'Accident Date_Season_Winter',\n",
    "\n",
    "               'Days_between_Assembly Date_Accident Date_log',\n",
    "               'Days_between_C-2 Date_Accident Date_log',\n",
    "               'Average Weekly Wage Imputed_log']\n",
    "\n",
    "cat_columns =['Carrier Type_Self-insured Private Entity',\n",
    "               'Carrier Type_Self-insured Public Entity', \n",
    "               'Carrier Type_Special Funds',\n",
    "               'Carrier Type_State Insurance Fund', \n",
    "               'C-3 Date_nabinary', \n",
    "               'Average Weekly Wage_nabinary',\n",
    "               'First Hearing Date_nabinary',\n",
    "               'Alternative Dispute Resolution_binary',\n",
    "               'COVID-19 Indicator_binary',\n",
    "               'Attorney/Representative_binary']\n",
    "\n",
    "\n",
    "# Create subsets\n",
    "X_train_num = X_train_encoded[num_columns]\n",
    "X_train_cat = X_train_encoded[cat_columns]\n",
    "\n",
    "X_val_num = X_val_encoded[num_columns]\n",
    "X_val_cat = X_val_encoded[cat_columns]\n",
    "\n",
    "X_test_num=X_test_encoded[num_columns]\n",
    "X_test_cat=X_test_encoded[cat_columns]\n",
    "\n",
    "X_val_final_num = X_val_encoded_final[num_columns]\n",
    "X_val_final_cat = X_val_encoded_final[cat_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Scale non-binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We decided to use normalization because it does not assume any specific distribution of the data. Instead, it scales all features to fit within a specified range—in our case, [0, 1]. This approach can be particularly effective for data that does not follow a Gaussian distribution or has outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler().fit(X_train_num)\n",
    "X_train_num_scaled = scaler.transform(X_train_num)\n",
    "# print(\"Parameters fitted:\")\n",
    "# for feature, min_val, max_val in zip(X_train_num.columns, scaler.data_min_, scaler.data_max_):\n",
    "    # print(f\"Variable: {feature} | Min: {min_val} | Max: {max_val}\")\n",
    "\n",
    "# Convert the array to a pandas dataframe\n",
    "X_train_num_scaled = pd.DataFrame(X_train_num_scaled, columns = X_train_num.columns).set_index(X_train_encoded.index)\n",
    "# X_train_num_scaled.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_num_scaled = scaler.transform(X_val_num)\n",
    "X_val_num_scaled = pd.DataFrame(X_val_num_scaled, columns = X_val_num.columns).set_index(X_val_encoded.index)\n",
    "# X_val_num_scaled.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_num_scaled = scaler.transform(X_test_num)\n",
    "X_test_num_scaled = pd.DataFrame(X_test_num_scaled, columns = X_test_num.columns).set_index(X_test_encoded.index)\n",
    "# X_test_num_scaled.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Scale binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Peform the same for binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler().fit(X_train_num_bin)\n",
    "X_train_num_scaled_bin = scaler.transform(X_train_num_bin)\n",
    "# print(\"Parameters fitted:\")\n",
    "# for feature, min_val, max_val in zip(X_train_num.columns, scaler.data_min_, scaler.data_max_):\n",
    "    # print(f\"Variable: {feature} | Min: {min_val} | Max: {max_val}\")\n",
    "\n",
    "# Convert the array to a pandas dataframe\n",
    "X_train_num_scaled_bin = pd.DataFrame(X_train_num_scaled_bin, columns = X_train_num_bin.columns).set_index(X_train_encoded_bin.index)\n",
    "# X_train_num_scaled.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_num_scaled_bin = scaler.transform(X_val_num_bin)\n",
    "X_val_num_scaled_bin = pd.DataFrame(X_val_num_scaled_bin, columns = X_val_num_bin.columns).set_index(X_val_encoded_bin.index)\n",
    "# X_val_num_scaled.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_num_scaled_bin = scaler.transform(X_test_num_bin)\n",
    "X_test_num_scaled_bin = pd.DataFrame(X_test_num_scaled_bin, columns = X_test_num_bin.columns).set_index(X_test_encoded_bin.index)\n",
    "# X_test_num_scaled.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Binary: Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "><strong style=\"color:#6fa8dc\">Add this to a separate python file</strong>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Filter Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# squeeze the y_train_bin\n",
    "y_train_bin = y_train_bin.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Applied without the target being encoded as it avoids introducing false ordinal relationships. Since all 3 (Chi-square, Cramer's V, Chi-Square) can handle categorical data directly, it’s better to not encode it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.1 Chi-square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cramers_v(X, y):\n",
    "    \"\"\"\n",
    "    Calculate Cramér's V for a categorical feature and a target.\n",
    "    :param X: Categorical feature (Pandas Series or array-like)\n",
    "    :param y: Target variable (Pandas Series or array-like)\n",
    "    :return: Cramér's V value\n",
    "    \"\"\"\n",
    "    # Create the contingency table\n",
    "    df_contingency = pd.crosstab(X, y)\n",
    "\n",
    "    # Perform chi-square test\n",
    "    chi2, p, dof, expected = chi2_contingency(df_contingency.values)\n",
    "\n",
    "    # Calculate Cramér's V\n",
    "    n = df_contingency.sum().sum()  # Total number of observations\n",
    "    min_dim = min(df_contingency.shape) - 1  # Min between number of rows and columns - 1\n",
    "    cramers_v = np.sqrt(chi2 / (n * min_dim))  # Cramér's V formula\n",
    "\n",
    "    return cramers_v\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> These features have **p-value < 0.05** and a **Cramér's V ≥ 0.1**, which indicates:\n",
    "\n",
    "> - **Strong Statistical Association**: The feature is significantly associated with the target variable.\n",
    "> - **Practical Importance**: Cramér's V suggests a meaningful strength of association."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              Column         Chi2  p-value  Cramér's V\n",
      "3  Carrier Type_State Insurance Fund   4015.95429      0.0    0.111341\n",
      "4                  C-3 Date_nabinary  23479.56453      0.0    0.422936\n",
      "5       Average Weekly Wage_nabinary  86078.08361      0.0    0.765095\n",
      "6        First Hearing Date_nabinary  25585.98391      0.0    0.492375\n",
      "9     Attorney/Representative_binary  77966.98880      0.0    0.532721\n"
     ]
    }
   ],
   "source": [
    "# Perform Chi-square test\n",
    "chi2_values, p_values = chi2(X_train_cat_bin, y_train_bin)\n",
    "\n",
    "# Create DataFrame for Chi-square results\n",
    "chi2_results = pd.DataFrame({\n",
    "    'Column': X_train_cat_bin.columns,\n",
    "    'Chi2': chi2_values.round(5),\n",
    "    'p-value': p_values.round(5)\n",
    "})\n",
    "\n",
    "# Calculate Cramér's V for binary target\n",
    "cramers_v_values = []\n",
    "for var in X_train_cat_bin.columns:\n",
    "    v = cramers_v(X_train_cat_bin[var], y_train_bin)\n",
    "    cramers_v_values.append(v)\n",
    "\n",
    "# Add Cramér's V to DataFrame\n",
    "chi2_results['Cramér\\'s V'] = cramers_v_values\n",
    "\n",
    "# Filter important features\n",
    "chi2_important_features = chi2_results[(chi2_results['p-value'] < 0.05) & (chi2_results['Cramér\\'s V'] >= 0.1)]\n",
    "\n",
    "list_features_chi2_cramer = chi2_important_features['Column'].values\n",
    "\n",
    "print(chi2_important_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2 Mutual Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Given that Mutual Information can measure the dependence between categorical variables and the target. We have decided to keep any features that have mutual informatio > 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Mutual Information</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Average Weekly Wage_nabinary</td>\n",
       "      <td>0.365020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Attorney/Representative_binary</td>\n",
       "      <td>0.155616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>First Hearing Date_nabinary</td>\n",
       "      <td>0.135264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C-3 Date_nabinary</td>\n",
       "      <td>0.093907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Carrier Type_State Insurance Fund</td>\n",
       "      <td>0.006233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Alternative Dispute Resolution_binary</td>\n",
       "      <td>0.002891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>COVID-19 Indicator_binary</td>\n",
       "      <td>0.002841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Carrier Type_Self-insured Private Entity</td>\n",
       "      <td>0.000820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Carrier Type_Self-insured Public Entity</td>\n",
       "      <td>0.000161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Carrier Type_Special Funds</td>\n",
       "      <td>0.000028</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Feature  Mutual Information\n",
       "5              Average Weekly Wage_nabinary            0.365020\n",
       "9            Attorney/Representative_binary            0.155616\n",
       "6               First Hearing Date_nabinary            0.135264\n",
       "4                         C-3 Date_nabinary            0.093907\n",
       "3         Carrier Type_State Insurance Fund            0.006233\n",
       "7     Alternative Dispute Resolution_binary            0.002891\n",
       "8                 COVID-19 Indicator_binary            0.002841\n",
       "0  Carrier Type_Self-insured Private Entity            0.000820\n",
       "1   Carrier Type_Self-insured Public Entity            0.000161\n",
       "2                Carrier Type_Special Funds            0.000028"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "# Calculate mutual information for categorical features\n",
    "mi_scores = mutual_info_classif(X_train_cat_bin, y_train_bin, discrete_features=True)\n",
    "\n",
    "# Create DataFrame of results\n",
    "mi_cat_results = pd.DataFrame({\n",
    "    'Feature': X_train_cat_bin.columns,\n",
    "    'Mutual Information': mi_scores\n",
    "}).sort_values(by='Mutual Information', ascending=False)\n",
    "\n",
    "mi_cat_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Feature  Mutual Information\n",
      "5    Average Weekly Wage_nabinary            0.365020\n",
      "9  Attorney/Representative_binary            0.155616\n",
      "6     First Hearing Date_nabinary            0.135264\n",
      "4               C-3 Date_nabinary            0.093907\n"
     ]
    }
   ],
   "source": [
    "mi_important_features = mi_cat_results[mi_cat_results['Mutual Information'] > 0.01]\n",
    "\n",
    "list_features_mi = mi_cat_results[mi_cat_results['Mutual Information'] > 0.01]['Feature'].values\n",
    "\n",
    "print(mi_important_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.3 Filtered Categorical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Keep if selected by either Chi-square and Cramer's V or Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['First Hearing Date_nabinary',\n",
       " 'Carrier Type_State Insurance Fund',\n",
       " 'Average Weekly Wage_nabinary',\n",
       " 'Attorney/Representative_binary',\n",
       " 'C-3 Date_nabinary']"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select features that appear in at least one of the important feature sets\n",
    "selected_features = list(set(list_features_chi2_cramer) | set(list_features_mi)) \n",
    "selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features =['Carrier Type_State Insurance Fund',\n",
    " 'Average Weekly Wage_nabinary',\n",
    " 'Attorney/Representative_binary',\n",
    " 'C-3 Date_nabinary',\n",
    " 'First Hearing Date_nabinary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only selected important features in the datasets\n",
    "X_train_cat_filtered_bin = X_train_cat_bin[selected_features]\n",
    "X_val_cat_filtered_bin = X_val_cat_bin[selected_features]\n",
    "X_test_cat_filtered_bin = X_test_cat_bin[selected_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Filter Numerical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1 Univariate variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Accident Date_Season_Summer                     0.190404\n",
       "Accident Date_Season_Winter                     0.190007\n",
       "Accident Date_Season_Spring                     0.181340\n",
       "Carrier Name_freq                               0.142347\n",
       "WCIO Nature of Injury Code_freq                 0.136746\n",
       "Industry Code_freq                              0.122438\n",
       "Number of Dependents                            0.111300\n",
       "Carrier Type_freq                               0.104407\n",
       "Accident Datemonth                              0.099277\n",
       "WCIO Part Of Body Code_freq                     0.098588\n",
       "Industry Code_encoded_0                         0.095915\n",
       "Industry Code_encoded_1                         0.095915\n",
       "WCIO Cause of Injury Code_freq                  0.080295\n",
       "WCIO Part Of Body Code_encoded_1                0.054324\n",
       "WCIO Part Of Body Code_encoded_0                0.054324\n",
       "Average Weekly Wage Imputed_log                 0.049334\n",
       "WCIO Nature of Injury Code_encoded_1            0.037289\n",
       "WCIO Nature of Injury Code_encoded_0            0.037289\n",
       "WCIO Cause of Injury Code_encoded_0             0.031492\n",
       "WCIO Cause of Injury Code_encoded_1             0.031492\n",
       "Days_between_C-2 Date_Accident Date_log         0.018723\n",
       "Days_between_Assembly Date_Accident Date_log    0.017405\n",
       "Age at Injury                                   0.013304\n",
       "IME-4 Count                                     0.000689\n",
       "dtype: float64"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_num_scaled_bin.var().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> No need to remove any, as there are no features with zero variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2 Spearman Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature_1</th>\n",
       "      <th>Feature_2</th>\n",
       "      <th>Correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>Days_between_Assembly Date_Accident Date_log</td>\n",
       "      <td>Days_between_C-2 Date_Accident Date_log</td>\n",
       "      <td>0.948592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Industry Code_encoded_0</td>\n",
       "      <td>Industry Code_encoded_1</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>WCIO Cause of Injury Code_encoded_0</td>\n",
       "      <td>WCIO Cause of Injury Code_encoded_1</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>WCIO Nature of Injury Code_encoded_0</td>\n",
       "      <td>WCIO Nature of Injury Code_encoded_1</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>WCIO Part Of Body Code_encoded_0</td>\n",
       "      <td>WCIO Part Of Body Code_encoded_1</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Feature_1  \\\n",
       "549  Days_between_Assembly Date_Accident Date_log   \n",
       "76                        Industry Code_encoded_0   \n",
       "126           WCIO Cause of Injury Code_encoded_0   \n",
       "176          WCIO Nature of Injury Code_encoded_0   \n",
       "226              WCIO Part Of Body Code_encoded_0   \n",
       "\n",
       "                                   Feature_2  Correlation  \n",
       "549  Days_between_C-2 Date_Accident Date_log     0.948592  \n",
       "76                   Industry Code_encoded_1    -1.000000  \n",
       "126      WCIO Cause of Injury Code_encoded_1    -1.000000  \n",
       "176     WCIO Nature of Injury Code_encoded_1    -1.000000  \n",
       "226         WCIO Part Of Body Code_encoded_1    -1.000000  "
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Calculate the Spearman correlation matrix for numerical features\n",
    "cor_spearman = X_train_num_scaled_bin.corr(method='spearman')\n",
    "\n",
    "# Step 2: Flatten the correlation matrix and reset the index\n",
    "correlation_pairs = cor_spearman.unstack().reset_index()\n",
    "\n",
    "# Step 3: Rename columns for clarity\n",
    "correlation_pairs.columns = ['Feature_1', 'Feature_2', 'Correlation']\n",
    "\n",
    "# Step 4: Filter the table for correlations > 0.8 or < -0.8 and exclude self-correlations (diagonal)\n",
    "strong_correlations = correlation_pairs[\n",
    "    ((correlation_pairs['Correlation'] > 0.8) | (correlation_pairs['Correlation'] < -0.8)) & \n",
    "    (correlation_pairs['Feature_1'] != correlation_pairs['Feature_2'])\n",
    "]\n",
    "\n",
    "# Step 5: Remove duplicate pairs by keeping only one order\n",
    "strong_correlations = strong_correlations[\n",
    "    strong_correlations['Feature_1'] < strong_correlations['Feature_2']\n",
    "]\n",
    "\n",
    "# Step 6: Sort by correlation value\n",
    "strong_correlations = strong_correlations.sort_values(by='Correlation', ascending=False)\n",
    "\n",
    "# Step 7: Display the table\n",
    "strong_correlations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Days_between_C-2 Date_Accident Date_log', 'Industry Code_encoded_1', 'WCIO Cause of Injury Code_encoded_1', 'WCIO Nature of Injury Code_encoded_1', 'WCIO Part Of Body Code_encoded_1']\n"
     ]
    }
   ],
   "source": [
    "# List of features with strong correlation (correlation > 0.8 or < -0.8)\n",
    "strong_correlation_features = list(pd.concat([strong_correlations['Feature_2']]).unique())\n",
    "\n",
    "# Display the list of features with strong correlations\n",
    "print(strong_correlation_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <code>WCIO Part Of Body Code_encoded_0</code> and <code>WCIO Part Of Body Code_encoded_1</code> and the other target encoded variables have -1 correlation, which is to be expected because it was binary encoded. We'll remove one of them. As well as remove <code>Days_between_C-2 Date_Accident Date_log</code> as we believe that <code>\tDays_between_Assembly Date_Accident Date_log</code> might be more meaningful since it had no missing values to begin with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.3 Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Mutual Information</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Average Weekly Wage Imputed_log</td>\n",
       "      <td>0.378509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IME-4 Count</td>\n",
       "      <td>0.156404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>WCIO Nature of Injury Code_freq</td>\n",
       "      <td>0.054081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>WCIO Nature of Injury Code_encoded_1</td>\n",
       "      <td>0.053153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>WCIO Nature of Injury Code_encoded_0</td>\n",
       "      <td>0.052874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Carrier Name_freq</td>\n",
       "      <td>0.040810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>WCIO Cause of Injury Code_freq</td>\n",
       "      <td>0.039775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>WCIO Cause of Injury Code_encoded_0</td>\n",
       "      <td>0.037862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>WCIO Cause of Injury Code_encoded_1</td>\n",
       "      <td>0.035482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>WCIO Part Of Body Code_freq</td>\n",
       "      <td>0.035121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>WCIO Part Of Body Code_encoded_0</td>\n",
       "      <td>0.032842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>WCIO Part Of Body Code_encoded_1</td>\n",
       "      <td>0.031811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Carrier Type_freq</td>\n",
       "      <td>0.025895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Industry Code_freq</td>\n",
       "      <td>0.024014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Industry Code_encoded_0</td>\n",
       "      <td>0.021429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Industry Code_encoded_1</td>\n",
       "      <td>0.021302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Age at Injury</td>\n",
       "      <td>0.008896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Accident Date_Season_Summer</td>\n",
       "      <td>0.008329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Accident Date_Season_Winter</td>\n",
       "      <td>0.007825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Accident Date_Season_Spring</td>\n",
       "      <td>0.007427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Number of Dependents</td>\n",
       "      <td>0.005943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Days_between_Assembly Date_Accident Date_log</td>\n",
       "      <td>0.004248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Accident Datemonth</td>\n",
       "      <td>0.003836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Days_between_C-2 Date_Accident Date_log</td>\n",
       "      <td>0.003364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Feature  Mutual Information\n",
       "23               Average Weekly Wage Imputed_log            0.378509\n",
       "1                                    IME-4 Count            0.156404\n",
       "13               WCIO Nature of Injury Code_freq            0.054081\n",
       "8           WCIO Nature of Injury Code_encoded_1            0.053153\n",
       "7           WCIO Nature of Injury Code_encoded_0            0.052874\n",
       "16                             Carrier Name_freq            0.040810\n",
       "12                WCIO Cause of Injury Code_freq            0.039775\n",
       "5            WCIO Cause of Injury Code_encoded_0            0.037862\n",
       "6            WCIO Cause of Injury Code_encoded_1            0.035482\n",
       "14                   WCIO Part Of Body Code_freq            0.035121\n",
       "9               WCIO Part Of Body Code_encoded_0            0.032842\n",
       "10              WCIO Part Of Body Code_encoded_1            0.031811\n",
       "15                             Carrier Type_freq            0.025895\n",
       "11                            Industry Code_freq            0.024014\n",
       "3                        Industry Code_encoded_0            0.021429\n",
       "4                        Industry Code_encoded_1            0.021302\n",
       "0                                  Age at Injury            0.008896\n",
       "19                   Accident Date_Season_Summer            0.008329\n",
       "20                   Accident Date_Season_Winter            0.007825\n",
       "18                   Accident Date_Season_Spring            0.007427\n",
       "2                           Number of Dependents            0.005943\n",
       "22  Days_between_Assembly Date_Accident Date_log            0.004248\n",
       "17                            Accident Datemonth            0.003836\n",
       "21       Days_between_C-2 Date_Accident Date_log            0.003364"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "# Compute mutual information between categorical features and target\n",
    "mutual_info = mutual_info_classif(X_train_num_scaled_bin, y_train_bin)\n",
    "\n",
    "# Display features sorted by mutual information\n",
    "mi_results = pd.DataFrame({\n",
    "    'Feature': X_train_num_scaled_bin.columns,\n",
    "    'Mutual Information': mutual_info\n",
    "}).sort_values(by='Mutual Information', ascending=False)\n",
    "\n",
    "mi_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 Feature  Mutual Information\n",
      "23       Average Weekly Wage Imputed_log            0.378509\n",
      "1                            IME-4 Count            0.156404\n",
      "13       WCIO Nature of Injury Code_freq            0.054081\n",
      "8   WCIO Nature of Injury Code_encoded_1            0.053153\n",
      "7   WCIO Nature of Injury Code_encoded_0            0.052874\n",
      "16                     Carrier Name_freq            0.040810\n",
      "12        WCIO Cause of Injury Code_freq            0.039775\n",
      "5    WCIO Cause of Injury Code_encoded_0            0.037862\n",
      "6    WCIO Cause of Injury Code_encoded_1            0.035482\n",
      "14           WCIO Part Of Body Code_freq            0.035121\n",
      "9       WCIO Part Of Body Code_encoded_0            0.032842\n",
      "10      WCIO Part Of Body Code_encoded_1            0.031811\n",
      "15                     Carrier Type_freq            0.025895\n",
      "11                    Industry Code_freq            0.024014\n",
      "3                Industry Code_encoded_0            0.021429\n",
      "4                Industry Code_encoded_1            0.021302\n"
     ]
    }
   ],
   "source": [
    "# Assuming `mutual_info` is the result from `mutual_info_classif` function\n",
    "mi_results = pd.DataFrame({\n",
    "    'Feature': X_train_num_scaled_bin.columns,\n",
    "    'Mutual Information': mutual_info\n",
    "}).sort_values(by='Mutual Information', ascending=False)\n",
    "\n",
    "# Filter features with Mutual Information > 0.01\n",
    "mi_important_features = mi_results[mi_results['Mutual Information'] > 0.01]\n",
    "\n",
    "# Display the important features\n",
    "print(mi_important_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Threshold of 0.01 as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Age at Injury',\n",
       " 'Accident Date_Season_Summer',\n",
       " 'Accident Date_Season_Winter',\n",
       " 'Accident Date_Season_Spring',\n",
       " 'Number of Dependents',\n",
       " 'Days_between_Assembly Date_Accident Date_log',\n",
       " 'Accident Datemonth',\n",
       " 'Days_between_C-2 Date_Accident Date_log']"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of features with Mutual Information > 0.01\n",
    "mi_non_important = mi_results[mi_results['Mutual Information'] < 0.01]['Feature'].tolist()\n",
    "mi_non_important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.3 Filtered Numerical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Drop features that have high correlation and mutual information < 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features_to_drop =list(set(num_features_to_drop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining features in X_train_num_scaled: Index(['IME-4 Count', 'Industry Code_encoded_0',\n",
      "       'WCIO Cause of Injury Code_encoded_0',\n",
      "       'WCIO Nature of Injury Code_encoded_0',\n",
      "       'WCIO Part Of Body Code_encoded_0', 'Industry Code_freq',\n",
      "       'WCIO Cause of Injury Code_freq', 'WCIO Nature of Injury Code_freq',\n",
      "       'WCIO Part Of Body Code_freq', 'Carrier Type_freq', 'Carrier Name_freq',\n",
      "       'Average Weekly Wage Imputed_log'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# List of features to drop from X_train_num_scaled\n",
    "num_features_to_drop = list(set(strong_correlation_features + mi_non_important))\n",
    "\n",
    "# Drop features\n",
    "X_train_num_scaled_filtered_bin = X_train_num_scaled_bin.drop(columns=num_features_to_drop)\n",
    "X_val_num_scaled_filtered_bin = X_val_num_scaled_bin.drop(columns=num_features_to_drop)\n",
    "X_test_num_scaled_filtered_bin = X_test_num_scaled_bin.drop(columns=num_features_to_drop)\n",
    "\n",
    "# Verify the remaining columns\n",
    "print(\"Remaining features in X_train_num_scaled:\", X_train_num_scaled_filtered_bin.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Combine the Filtered Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Combine the datasets after dropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of combined X_train: (401797, 17)\n"
     ]
    }
   ],
   "source": [
    "# Combine the filtered datasets\n",
    "X_train_bin = pd.concat([X_train_cat_filtered_bin, X_train_num_scaled_filtered_bin], axis=1)\n",
    "X_val_bin =  pd.concat([X_val_cat_filtered_bin, X_val_num_scaled_filtered_bin], axis=1)\n",
    "X_test_bin =  pd.concat([X_test_cat_filtered_bin, X_test_num_scaled_filtered_bin], axis=1)\n",
    "\n",
    "\n",
    "# Verify the shape of the combined dataset\n",
    "print(\"Shape of combined X_train:\", X_train_bin.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Use this for the binary, as we will be dropping later for multiclass in minority"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Feature Selection with All Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.1 RFE WITH Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Feature select with RFE and Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Features based on F1 score: Index(['Carrier Type_State Insurance Fund', 'Average Weekly Wage_nabinary',\n",
      "       'Attorney/Representative_binary', 'C-3 Date_nabinary',\n",
      "       'First Hearing Date_nabinary', 'IME-4 Count', 'Industry Code_encoded_0',\n",
      "       'WCIO Cause of Injury Code_encoded_0',\n",
      "       'WCIO Nature of Injury Code_encoded_0',\n",
      "       'WCIO Part Of Body Code_encoded_0', 'Industry Code_freq',\n",
      "       'WCIO Cause of Injury Code_freq', 'WCIO Nature of Injury Code_freq',\n",
      "       'WCIO Part Of Body Code_freq', 'Carrier Type_freq', 'Carrier Name_freq',\n",
      "       'Average Weekly Wage Imputed_log'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming X_train_bin and y_train_bin are your features and target\n",
    "\n",
    "sample_size = int(0.5 * len(X_train_bin))  # 10% of the dataset\n",
    "X_train_sample = X_train_bin.sample(n=sample_size, random_state=42)\n",
    "y_train_sample = y_train_bin.sample(n=sample_size, random_state=42)\n",
    "\n",
    "# Step 2: Initialize the logistic regression model\n",
    "logistic_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Step 3: Initialize list to store F1 scores for each subset\n",
    "f1_scores = []\n",
    "\n",
    "# Step 4: Loop over all subsets of features\n",
    "for n_features in range(1, X_train_sample.shape[1] + 1):  # For 1 to n features\n",
    "    # Get the top 'n_features' columns based on importance (or use another criteria)\n",
    "    selected_features = X_train_sample.columns[:n_features]\n",
    "    X_train_subset = X_train_sample[selected_features]\n",
    "    \n",
    "    # Step 5: Train the model on the subset\n",
    "    logistic_model.fit(X_train_subset, y_train_sample)\n",
    "    \n",
    "    # Step 6: Make predictions and calculate the F1 score\n",
    "    y_pred = logistic_model.predict(X_val_bin[selected_features])\n",
    "    f1 = f1_score(y_val_bin, y_pred)\n",
    "    \n",
    "    # Store the F1 score\n",
    "    f1_scores.append((selected_features, f1))\n",
    "\n",
    "# Step 7: Sort the F1 scores and select the best features\n",
    "f1_scores.sort(key=lambda x: x[1], reverse=True)  # Sort by F1 score in descending order\n",
    "selected_features_rfe = f1_scores[0][0]  # Get the subset with the best F1 score\n",
    "\n",
    "# Step 8: Print the best features based on F1 score\n",
    "print(\"Best Features based on F1 score:\", selected_features_rfe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> keep this for now but can delete after when confirm above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# from sklearn.feature_selection import RFE\n",
    "# import numpy as np\n",
    "\n",
    "# # Initialize the Logistic Regression model\n",
    "# log_reg_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# # Sample 1% of the data\n",
    "# sample_size = int(0.5 * len(X_train_bin))  # 50% of the dataset\n",
    "# X_train_sample = X_train_bin.sample(n=sample_size, random_state=42)\n",
    "# y_train_sample = y_train_bin.sample(n=sample_size, random_state=42)\n",
    "\n",
    "# # Loop over different numbers of features\n",
    "# feature_counts = range(1, X_train_sample.shape[1] + 1)\n",
    "# scores = []\n",
    "\n",
    "# # Loop through each feature count and evaluate the model's performance\n",
    "# for n in feature_counts:\n",
    "#     rfe = RFE(log_reg_model, n_features_to_select=n)\n",
    "#     rfe.fit(X_train_sample, y_train_sample)\n",
    "    \n",
    "#     # Get selected features\n",
    "#     selected_features = X_train_sample.columns[rfe.support_]\n",
    "    \n",
    "#     # Evaluate performance using cross-validation with F1 score as the metric\n",
    "#     score = cross_val_score(log_reg_model, X_train_sample[selected_features], y_train_sample, cv=5, scoring='f1_macro').mean()\n",
    "#     scores.append(score)\n",
    "\n",
    "# # Find the number of features that gives the highest F1 score\n",
    "# best_n_features = feature_counts[np.argmax(scores)]\n",
    "# print(f\"Optimal number of features: {best_n_features}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features after RFE with Logistic Regression:\n",
      "Index(['Carrier Type_State Insurance Fund', 'Average Weekly Wage_nabinary',\n",
      "       'Attorney/Representative_binary', 'First Hearing Date_nabinary',\n",
      "       'IME-4 Count', 'WCIO Cause of Injury Code_encoded_0',\n",
      "       'WCIO Nature of Injury Code_encoded_0',\n",
      "       'WCIO Part Of Body Code_encoded_0', 'Carrier Type_freq',\n",
      "       'Carrier Name_freq', 'Average Weekly Wage Imputed_log'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.feature_selection import RFE\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# import pandas as pd\n",
    "\n",
    "# # Assuming X_train_bin and y_train_bin are your features and target\n",
    "\n",
    "# # Step 1: Take a smaller sample (e.g., 50% of the data)\n",
    "# sample_size = int(0.5 * len(X_train_bin))  # 10% of the dataset\n",
    "# X_train_sample = X_train_bin.sample(n=sample_size, random_state=42)\n",
    "# y_train_sample = y_train_bin.sample(n=sample_size, random_state=42)\n",
    "\n",
    "# # Step 2: Initialize Logistic Regression\n",
    "# logistic_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# # Step 3: Apply RFE to the sample data\n",
    "# rfe = RFE(logistic_model, n_features_to_select=6)  # Select top 8 features (adjust as needed)\n",
    "# rfe.fit(X_train_sample, y_train_sample)\n",
    "\n",
    "# # Step 4: Get the selected features\n",
    "# selected_features_rfe = X_train_sample.columns[rfe.support_]\n",
    "\n",
    "# # Print selected features\n",
    "# print(\"Selected Features after RFE with Logistic Regression:\")\n",
    "# print(selected_features_rfe)\n",
    "\n",
    "# # Optionally, you can use the selected features to transform the dataset\n",
    "# X_train_selected_rfe = X_train_sample[selected_features_rfe]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.2 Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features by Lasso: Index(['Carrier Type_State Insurance Fund', 'Average Weekly Wage_nabinary',\n",
      "       'Attorney/Representative_binary', 'C-3 Date_nabinary',\n",
      "       'First Hearing Date_nabinary', 'IME-4 Count', 'Industry Code_encoded_0',\n",
      "       'WCIO Cause of Injury Code_encoded_0',\n",
      "       'WCIO Nature of Injury Code_encoded_0',\n",
      "       'WCIO Part Of Body Code_encoded_0', 'Industry Code_freq',\n",
      "       'WCIO Cause of Injury Code_freq', 'WCIO Nature of Injury Code_freq',\n",
      "       'WCIO Part Of Body Code_freq', 'Carrier Type_freq', 'Carrier Name_freq',\n",
      "       'Average Weekly Wage Imputed_log'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "lasso = LassoCV(cv=5)  # Cross-validation to select optimal alpha\n",
    "lasso.fit(X_train_bin, y_train_bin)\n",
    "lasso_features = X_train_bin.columns[lasso.coef_ != 0]\n",
    "print(\"Selected features by Lasso:\", lasso_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.3 Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Find best thrshold for ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha value: 0.1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the model\n",
    "ridge = Ridge()\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(ridge, param_grid, cv=5)\n",
    "grid_search.fit(X_train_bin, y_train_bin)\n",
    "\n",
    "# Get the best alpha\n",
    "best_alpha = grid_search.best_params_['alpha']\n",
    "print(f\"Best alpha value: {best_alpha}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features using Ridge regression: Index(['Carrier Type_State Insurance Fund', 'Average Weekly Wage_nabinary',\n",
      "       'Attorney/Representative_binary', 'IME-4 Count',\n",
      "       'WCIO Nature of Injury Code_encoded_0', 'Carrier Name_freq',\n",
      "       'Average Weekly Wage Imputed_log'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "import numpy as np\n",
    "\n",
    "# Initialize Ridge regression model with cross-validation\n",
    "ridge_model = RidgeCV(cv=5)\n",
    "\n",
    "# Fit the model on the training data\n",
    "ridge_model.fit(X_train_bin, y_train_bin)\n",
    "\n",
    "# Get the feature coefficients\n",
    "coefficients = ridge_model.coef_\n",
    "\n",
    "# Set a threshold to select features (e.g., absolute coefficient > 0.1)\n",
    "threshold = 0.1\n",
    "ridge_features = X_train_bin.columns[np.abs(coefficients) > threshold]\n",
    "\n",
    "print(f\"Selected features using Ridge regression: {ridge_features}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.4 Random Forest Important Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Find best alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importances: [0.00211268 0.2846026  0.08045238 0.02492534 0.03600001 0.05694644\n",
      " 0.02339744 0.03569198 0.02276218 0.03082299 0.02198453 0.03080075\n",
      " 0.01506359 0.02924714 0.00888832 0.07782569 0.21847593]\n",
      "Sorted feature indices: [ 1 16  2 15  5  4  7  9 11 13  3  6  8 10 12 14  0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize the Random Forest Classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=47)\n",
    "\n",
    "# Fit the model on X_train and y_train_encoded\n",
    "rf_model.fit(X_train_bin, y_train_bin)\n",
    "\n",
    "# Get feature importances\n",
    "importances = rf_model.feature_importances_\n",
    "\n",
    "# Sort feature importances in descending order\n",
    "important_indices = importances.argsort()[::-1]\n",
    "\n",
    "# Print the feature importances\n",
    "print(\"Feature importances:\", importances)\n",
    "print(\"Sorted feature indices:\", important_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features explaining 90% of cumulative importance: 11\n",
      "Selected features that explain 90% of cumulative importance: Index(['Average Weekly Wage_nabinary', 'Average Weekly Wage Imputed_log',\n",
      "       'Attorney/Representative_binary', 'Carrier Name_freq', 'IME-4 Count',\n",
      "       'First Hearing Date_nabinary', 'WCIO Cause of Injury Code_encoded_0',\n",
      "       'WCIO Part Of Body Code_encoded_0', 'WCIO Cause of Injury Code_freq',\n",
      "       'WCIO Part Of Body Code_freq', 'C-3 Date_nabinary'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Calculate cumulative sum of feature importances\n",
    "cumulative_importance = np.cumsum(importances[important_indices])\n",
    "\n",
    "# Find the index where cumulative importance exceeds 90%\n",
    "threshold = 0.90\n",
    "index_90 = np.argmax(cumulative_importance >= threshold)\n",
    "\n",
    "# The number of features that explain 90% of the importance\n",
    "num_features_90 = index_90 + 1  # Adding 1 since index starts at 0\n",
    "\n",
    "print(f\"Number of features explaining 90% of cumulative importance: {num_features_90}\")\n",
    "\n",
    "# Select the top features based on this number\n",
    "random_forest_features = X_train_bin.columns[important_indices[:num_features_90]]\n",
    "print(f\"Selected features that explain 90% of cumulative importance: {random_forest_features}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.5 Voting of Best Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "Features selected by at least three methods:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Carrier Type_State Insurance Fund',\n",
       " 'Carrier Name_freq',\n",
       " 'Average Weekly Wage_nabinary',\n",
       " 'Attorney/Representative_binary',\n",
       " 'IME-4 Count',\n",
       " 'Average Weekly Wage Imputed_log',\n",
       " 'WCIO Nature of Injury Code_encoded_0']"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert all feature sets to sets\n",
    "rfe_set = set(selected_features_rfe)\n",
    "lasso_set = set(lasso_features)\n",
    "ridge_set = set(ridge_features)\n",
    "rf_set = set(random_forest_features)\n",
    "\n",
    "# Find features selected by at least three methods\n",
    "final_features_binary_set = (\n",
    "    (lasso_set & rfe_set & ridge_set) |\n",
    "    (lasso_set & ridge_set & rf_set) |\n",
    "    (ridge_set & rfe_set & rf_set)\n",
    ")\n",
    "\n",
    "# Convert the final features set to a list (optional, for easier use later)\n",
    "final_features_binary = list(final_features_binary_set)\n",
    "\n",
    "# Print the selected features\n",
    "print(len(final_features_binary))\n",
    "print(\"Features selected by at least three methods:\")\n",
    "final_features_binary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bin_filter = X_train_bin[final_features_binary]\n",
    "X_val_bin_filter = X_val_bin[final_features_binary]\n",
    "X_test_bin_filter = X_test_bin[final_features_binary]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Binary: Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_pred, dtrain):\n",
    "    \"\"\"\n",
    "    Custom loss function penalizing false negatives for class 0.\n",
    "    Parameters:\n",
    "    - y_pred: Predicted values (log-odds).\n",
    "    - dtrain: DMatrix containing true labels.\n",
    "\n",
    "    Returns:\n",
    "    - grad: Gradient of the loss.\n",
    "    - hess: Hessian of the loss.\n",
    "    \"\"\"\n",
    "    y_true = dtrain.get_label()  # Extract true labels from DMatrix\n",
    "    y_pred = 1 / (1 + np.exp(-y_pred))  # Convert log-odds to probabilities\n",
    "\n",
    "    # Adjust weights for class 0\n",
    "    weight = np.where(y_true == 0, 2.0, 1.0)  # Double penalty for class 0 FN\n",
    "\n",
    "    grad = (y_pred - y_true) * weight  # Gradient\n",
    "    hess = y_pred * (1 - y_pred) * weight  # Hessian\n",
    "    return grad, hess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Convert data to DMatrix format\n",
    "dtrain = xgb.DMatrix(X_train_bin_filter, label=y_train_bin)\n",
    "dval = xgb.DMatrix(X_val_bin_filter, label=y_val_bin)\n",
    "\n",
    "# Parameters for XGBoost\n",
    "params = {\n",
    "    \"objective\": \"binary:logistic\",  # Use binary classification\n",
    "    \"eval_metric\": \"logloss\",  # Standard evaluation metric\n",
    "    \"eta\": 0.1,  # Learning rate\n",
    "    \"max_depth\": 6,  # Maximum depth of trees\n",
    "}\n",
    "\n",
    "# Train the model with custom loss function\n",
    "binary_model = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=100,\n",
    "    obj=custom_loss,  # Custom loss function\n",
    "    evals=[(dval, \"validation\")],\n",
    "    early_stopping_rounds=10,  # Stop if no improvement\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict probabilities\n",
    "y_proba = binary_model.predict(xgb.DMatrix(X_val_bin_filter))\n",
    "\n",
    "# Convert probabilities to binary predictions using a threshold (default is 0.5)\n",
    "threshold = 0.5\n",
    "y_pred_binary = (y_proba > threshold).astype(int)\n",
    "\n",
    "# Evaluate\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_val_bin, y_pred_binary))\n",
    "print(\"F1 Score:\", f1_score(y_val_bin, y_pred_binary, average=\"macro\"))\n",
    "print(classification_report(y_val_bin, y_pred_binary))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier Accuracy: 0.8974\n",
      "XGBClassifier Macro F1 Score: 0.8968\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.83      0.89     84881\n",
      "           1       0.85      0.96      0.90     87319\n",
      "\n",
      "    accuracy                           0.90    172200\n",
      "   macro avg       0.90      0.90      0.90    172200\n",
      "weighted avg       0.90      0.90      0.90    172200\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "binary_model = xgb.XGBClassifier(eval_metric=\"mlogloss\")\n",
    "binary_model.fit(X_train_bin_filter, y_train_bin)\n",
    "\n",
    "# 3. Make predictions\n",
    "y_pred_binary = binary_model.predict(X_val_bin_filter)  # Assuming X_test is available\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_val_bin, y_pred_binary)\n",
    "f1_macro = f1_score(y_val_bin, y_pred_binary, average='macro')  # Macro-averaged F1 score\n",
    "\n",
    "# Print individual model results\n",
    "print(f\"XGBClassifier Accuracy: {accuracy:.4f}\")\n",
    "print(f\"XGBClassifier Macro F1 Score: {f1_macro:.4f}\")\n",
    "print(classification_report(y_val_bin, y_pred_binary))  # Detailed report including precision, recall, and F1 score per class\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Ensure target variable is 1D\n",
    "y_train_bin = y_train_bin.values.ravel() if hasattr(y_train_bin, \"values\") else y_train_bin\n",
    "y_val_bin = y_val_bin.values.ravel() if hasattr(y_val_bin, \"values\") else y_val_bin\n",
    "\n",
    "# Define base models (XGBoost and CatBoost in this case)\n",
    "estimators = [\n",
    "    ('xgb', XGBClassifier(random_state=42, eval_metric=\"mlogloss\")),   \n",
    "    ('catboost', CatBoostClassifier(iterations=1000, random_state=42, verbose=100, early_stopping_rounds=50))  \n",
    "]\n",
    "\n",
    "# Define final estimator (Logistic Regression in this case)\n",
    "final_estimator = LogisticRegression(max_iter=500, random_state=42)\n",
    "\n",
    "# Create the Stacking Classifier model\n",
    "stacking_model = StackingClassifier(estimators=estimators, final_estimator=final_estimator, n_jobs=-1)\n",
    "\n",
    "# Train the stacking model\n",
    "stacking_model.fit(X_train_bin_filter, y_train_bin)\n",
    "\n",
    "# Predict on validation set and evaluate performance\n",
    "y_pred = stacking_model.predict(X_val_bin_filter)\n",
    "print(classification_report(y_val_bin, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Multiclass: Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Filter Categorical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1.1 Chi-square & Cramer's V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Chi-square test\n",
    "chi2_values, p_values = chi2(X_train_cat, y_train_not_encoded)\n",
    "\n",
    "# Create DataFrame for Chi-square results\n",
    "chi2_results = pd.DataFrame({\n",
    "    'Column': X_train_cat.columns,\n",
    "    'Chi2': chi2_values.round(5),\n",
    "    'p-value': p_values.round(5)\n",
    "})\n",
    "\n",
    "# Calculate Cramér's V for binary target\n",
    "cramers_v_values = []\n",
    "for var in X_train_cat.columns:\n",
    "    v = cramers_v(X_train_cat[var], y_train_not_encoded)\n",
    "    cramers_v_values.append(v)\n",
    "\n",
    "# Add Cramér's V to DataFrame\n",
    "chi2_results['Cramér\\'s V'] = cramers_v_values\n",
    "\n",
    "# Filter important features\n",
    "chi2_important_features = chi2_results[(chi2_results['p-value'] < 0.05) & (chi2_results['Cramér\\'s V'] >= 0.1)]\n",
    "\n",
    "list_features_chi2_cramer = chi2_important_features['Column'].values\n",
    "\n",
    "print(chi2_important_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1.2 Mutual Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "# # Calculate mutual information for categorical features\n",
    "# mi_scores = mutual_info_classif(X_train_cat, y_train_not_encoded, discrete_features=True)\n",
    "\n",
    "# # Create DataFrame of results\n",
    "# mi_results = pd.DataFrame({\n",
    "#     'Feature': X_train_cat.columns,\n",
    "#     'Mutual Information': mi_scores\n",
    "# }).sort_values(by='Mutual Information', ascending=False)\n",
    "\n",
    "# mi_results\n",
    "\n",
    "# mi_important_features = mi_results[mi_results['Mutual Information'] > 0.05]\n",
    "\n",
    "# list_features_mi = mi_results[mi_results['Mutual Information'] > 0.05]['Feature'].values\n",
    "\n",
    "# print(mi_important_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1.3 Filtered Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Select features that appear in at least one of the important feature sets\n",
    "# selected_features = list(set(list_features_chi2_cramer) | set(list_features_mi)) \n",
    "# print(selected_features)\n",
    "\n",
    "# # Keep only selected important features in the datasets\n",
    "# X_train_cat_filtered = X_train_cat[selected_features]\n",
    "# X_val_cat_filtered = X_val_cat[selected_features]\n",
    "# X_test_cat_filtered = X_test_cat[selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = ['COVID-19 Indicator_binary', 'Carrier Type_Self-insured Public Entity', 'Average Weekly Wage_nabinary', 'C-3 Date_nabinary', 'First Hearing Date_nabinary', 'Attorney/Representative_binary']\n",
    "X_train_cat_filtered = X_train_cat[selected_features]\n",
    "X_val_cat_filtered = X_val_cat[selected_features]\n",
    "X_test_cat_filtered = X_test_cat[selected_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Filter Numerical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.1 Univariate variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_num_scaled.var().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> No features with variance zero. Don't drop any here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.2 Spearman Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 1: Calculate the Spearman correlation matrix for numerical features\n",
    "# cor_spearman = X_train_num_scaled.corr(method='spearman')\n",
    "\n",
    "# # Step 2: Flatten the correlation matrix and reset the index\n",
    "# correlation_pairs = cor_spearman.unstack().reset_index()\n",
    "\n",
    "# # Step 3: Rename columns for clarity\n",
    "# correlation_pairs.columns = ['Feature_1', 'Feature_2', 'Correlation']\n",
    "\n",
    "# # Step 4: Filter the table for correlations > 0.8 or < -0.8 and exclude self-correlations (diagonal)\n",
    "# strong_correlations = correlation_pairs[\n",
    "#     ((correlation_pairs['Correlation'] > 0.9) | (correlation_pairs['Correlation'] < -0.9)) & \n",
    "#     (correlation_pairs['Feature_1'] != correlation_pairs['Feature_2'])\n",
    "# ]\n",
    "\n",
    "# # Step 5: Remove duplicate pairs by keeping only one order\n",
    "# strong_correlations = strong_correlations[\n",
    "#     strong_correlations['Feature_1'] < strong_correlations['Feature_2']\n",
    "# ]\n",
    "\n",
    "# # Step 6: Sort by correlation value\n",
    "# strong_correlations = strong_correlations.sort_values(by='Correlation', ascending=False)\n",
    "\n",
    "# # Step 7: Display the table\n",
    "# strong_correlations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.3 Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "# # Compute mutual information between categorical features and target\n",
    "# mutual_info = mutual_info_classif(X_train_num_scaled, y_train_encoded)\n",
    "\n",
    "# # Display features sorted by mutual information\n",
    "# mi_results = pd.DataFrame({\n",
    "#     'Feature': X_train_num_scaled.columns,\n",
    "#     'Mutual Information': mutual_info\n",
    "# }).sort_values(by='Mutual Information', ascending=False)\n",
    "\n",
    "# mi_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assuming `mutual_info` is the result from `mutual_info_classif` function\n",
    "# mi_results = pd.DataFrame({\n",
    "#     'Feature': X_train_num_scaled.columns,\n",
    "#     'Mutual Information': mutual_info\n",
    "# }).sort_values(by='Mutual Information', ascending=False)\n",
    "\n",
    "# # Filter features with Mutual Information > 0.05\n",
    "# mi_important_features = mi_results[mi_results['Mutual Information'] > 0.01]\n",
    "\n",
    "# # Display the important features\n",
    "# print(mi_important_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.3 Filtered Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of features to drop from X_train_num_scaled\n",
    "num_features_to_drop = [\n",
    "    'Days_between_C-2 Date Imputed_Accident Date_log'\n",
    "    ]\n",
    "\n",
    "# Drop features\n",
    "X_train_num_scaled_filtered = X_train_num_scaled.drop(columns=num_features_to_drop)\n",
    "X_val_num_scaled_filtered = X_val_num_scaled.drop(columns=num_features_to_drop)\n",
    "X_test_num_scaled_filtered = X_test_num_scaled.drop(columns=num_features_to_drop)\n",
    "\n",
    "# Verify the remaining columns\n",
    "print(\"Remaining features in X_train_num_scaled:\", X_train_num_scaled_filtered.columns)\n",
    "print(len(X_train_num_scaled_filtered.columns))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Combine the Filtered Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the filtered datasets\n",
    "X_train = pd.concat([X_train_cat_filtered, X_train_num_scaled_filtered], axis=1)\n",
    "X_val =  pd.concat([X_val_cat_filtered, X_val_num_scaled_filtered], axis=1)\n",
    "X_test =  pd.concat([X_test_cat_filtered, X_test_num_scaled_filtered], axis=1)\n",
    "\n",
    "\n",
    "# Verify the shape of the combined dataset\n",
    "print(\"Shape of combined X_train:\", X_train.shape)\n",
    "print(\"Shape of combined X_train:\", X_val.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Feature Selection All Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.4.1 Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "lasso = LassoCV(cv=5)  # Cross-validation to select optimal alpha\n",
    "lasso.fit(X_train, y_train_encoded)\n",
    "lasso_features = X_train.columns[lasso.coef_ != 0]\n",
    "print(\"Selected features by Lasso:\", lasso_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.4.2 Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "import numpy as np\n",
    "\n",
    "# Initialize Ridge regression model with cross-validation\n",
    "ridge_model = RidgeCV(cv=5)\n",
    "\n",
    "# Fit the model on the training data\n",
    "ridge_model.fit(X_train, y_train_encoded)\n",
    "\n",
    "# Get the feature coefficients\n",
    "coefficients = ridge_model.coef_\n",
    "\n",
    "# Set a threshold to select features (e.g., absolute coefficient > 0.01)\n",
    "threshold = 0.01\n",
    "ridge_features = X_train.columns[np.abs(coefficients) > threshold]\n",
    "\n",
    "print(f\"Selected features using Ridge regression: {ridge_features}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.4.3 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize the Random Forest Classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=47)\n",
    "\n",
    "# Fit the model on X_train and y_train_encoded\n",
    "rf_model.fit(X_train, y_train_encoded)\n",
    "\n",
    "# Get feature importances\n",
    "importances = rf_model.feature_importances_\n",
    "\n",
    "# Sort feature importances in descending order\n",
    "important_indices = importances.argsort()[::-1]\n",
    "\n",
    "# Print the feature importances\n",
    "print(\"Feature importances:\", importances)\n",
    "print(\"Sorted feature indices:\", important_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Calculate cumulative sum of feature importances\n",
    "cumulative_importance = np.cumsum(importances[important_indices])\n",
    "\n",
    "# Find the index where cumulative importance exceeds 90%\n",
    "threshold = 0.90\n",
    "index_90 = np.argmax(cumulative_importance >= threshold)\n",
    "\n",
    "# The number of features that explain 90% of the importance\n",
    "num_features_90 = index_90 + 1  # Adding 1 since index starts at 0\n",
    "\n",
    "print(f\"Number of features explaining 90% of cumulative importance: {num_features_90}\")\n",
    "\n",
    "# Select the top features based on this number\n",
    "random_forest_features = X_train.columns[important_indices[:num_features_90]]\n",
    "print(f\"Selected features that explain 90% of cumulative importance: {random_forest_features}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Initialize the XGBoost Classifier without 'use_label_encoder'\n",
    "xgb_model = XGBClassifier(n_estimators=100, random_state=47, eval_metric=\"mlogloss\")\n",
    "\n",
    "# Fit the model on X_train and y_train_encoded\n",
    "xgb_model.fit(X_train, y_train_encoded)\n",
    "\n",
    "# Get feature importances\n",
    "importances = xgb_model.feature_importances_\n",
    "\n",
    "# Create a DataFrame with feature names and their importances\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': X_train.columns,  # Assumes X_train is a pandas DataFrame\n",
    "    'Importance': importances\n",
    "})\n",
    "\n",
    "# Sort the DataFrame by importance in descending order\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Print the sorted feature names and their importance\n",
    "print(feature_importance_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Calculate cumulative sum of feature importances\n",
    "cumulative_importance = np.cumsum(feature_importance_df['Importance'])\n",
    "\n",
    "# Find the index where cumulative importance exceeds 90%\n",
    "threshold = 0.90\n",
    "index_90 = np.argmax(cumulative_importance >= threshold)\n",
    "\n",
    "# The number of features that explain 90% of the importance\n",
    "num_features_90 = index_90 + 1  # Adding 1 since index starts at 0\n",
    "\n",
    "print(f\"Number of features explaining 90% of cumulative importance: {num_features_90}\")\n",
    "\n",
    "# Select the top features based on this number\n",
    "xgboost_features = feature_importance_df['Feature'].iloc[:num_features_90]\n",
    "print(f\"Selected features that explain 90% of cumulative importance:\\n{xgboost_features}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.4.5 Voting for the Best Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all feature sets to sets\n",
    "lasso_set = set(lasso_features)\n",
    "ridge_set = set(ridge_features)\n",
    "rf_set = set(random_forest_features)\n",
    "xgb_set = set(xgboost_features)\n",
    "\n",
    "# Find features selected by at least three methods\n",
    "final_features_set = (\n",
    "    (lasso_set & ridge_set & rf_set) |  # Features in all three: Lasso, Ridge, RF\n",
    "    (lasso_set & ridge_set & xgb_set) |  # Features in Lasso, Ridge, XGBoost\n",
    "    (lasso_set & rf_set & xgb_set) |     # Features in Lasso, RF, XGBoost\n",
    "    (ridge_set & rf_set & xgb_set)      # Features in Ridge, RF, XGBoost\n",
    ")\n",
    "\n",
    "# Convert the final features set to a list (optional, for easier use later)\n",
    "final_features = list(final_features_set)\n",
    "\n",
    "# Print the selected features\n",
    "print(len(final_features))\n",
    "print(\"Features selected by at least three methods:\")\n",
    "print(final_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Apply to datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assume the following sets represent the selected features from each method\n",
    "lasso_set = set(lasso_features)\n",
    "ridge_set = set(ridge_features)\n",
    "rf_set = set(random_forest_features)\n",
    "xgb_set = set(xgboost_features)\n",
    "\n",
    "# Combine all unique features from all sets\n",
    "all_features = list(set(lasso_set) | set(ridge_set) | set(rf_set) | set(xgb_set))\n",
    "\n",
    "# Create a DataFrame to show the feature selection results\n",
    "feature_selection_table = pd.DataFrame({\n",
    "    'Feature': all_features,\n",
    "    'Lasso': [feature in lasso_set for feature in all_features],\n",
    "    'Ridge': [feature in ridge_set for feature in all_features],\n",
    "    'Random Forest': [feature in rf_set for feature in all_features],\n",
    "    'XGBoost': [feature in xgb_set for feature in all_features]\n",
    "})\n",
    "\n",
    "# Add a column to count how many methods selected each feature\n",
    "feature_selection_table['Selected by 2 or more'] = (\n",
    "    feature_selection_table[['Lasso', 'Ridge', 'Random Forest', 'XGBoost']].sum(axis=1) >= 3\n",
    ")\n",
    "\n",
    "# Extract lists of features selected by each method\n",
    "lasso_selected_features = feature_selection_table[feature_selection_table['Lasso']]['Feature'].tolist()\n",
    "ridge_selected_features = feature_selection_table[feature_selection_table['Ridge']]['Feature'].tolist()\n",
    "rf_selected_features = feature_selection_table[feature_selection_table['Random Forest']]['Feature'].tolist()\n",
    "xgb_selected_features = feature_selection_table[feature_selection_table['XGBoost']]['Feature'].tolist()\n",
    "\n",
    "# Extract features selected by 3 or more methods\n",
    "selected_by_3_or_more = feature_selection_table[feature_selection_table['Selected by 2 or more']]['Feature'].tolist()\n",
    "\n",
    "# Function to highlight selected features for Lasso, Ridge, RF, and XGBoost\n",
    "def highlight_selected(val):\n",
    "    return 'background-color: #aed6f1' if val else ''\n",
    "\n",
    "def highlight_selected_by_3_or_more(val):\n",
    "    return 'background-color: #abebc6' if val else ''\n",
    "\n",
    "# Apply the styling to each column using apply and map\n",
    "styled_table = feature_selection_table.style.apply(\n",
    "    lambda x: ['background-color: #aed6f1' if val else '' for val in x], \n",
    "    subset=['Lasso', 'Ridge', 'Random Forest', 'XGBoost']\n",
    ").apply(\n",
    "    lambda x: ['background-color: #abebc6' if val else '' for val in x], \n",
    "    subset=['Selected by 2 or more']\n",
    ")\n",
    "\n",
    "# Display the styled table\n",
    "styled_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_by_3_or_more = ['Accident Datemonth',\n",
    " 'Industry Code_encoded_4. TEMPORARY',\n",
    " 'WCIO Part Of Body Code_encoded_5. PPD SCH LOSS',\n",
    " 'Average Weekly Wage Imputed_log',\n",
    " 'C-3 Date_nabinary',\n",
    " 'WCIO Cause of Injury Code_encoded_3. MED ONLY',\n",
    " 'COVID-19 Indicator_binary',\n",
    " 'Industry Code_encoded_3. MED ONLY',\n",
    " 'Age at Injury',\n",
    " 'WCIO Part Of Body Code_freq',\n",
    " 'WCIO Cause of Injury Code_freq',\n",
    " 'WCIO Nature of Injury Code_encoded_5. PPD SCH LOSS',\n",
    " 'IME-4 Count',\n",
    " 'WCIO Cause of Injury Code_encoded_1. CANCELLED',\n",
    " 'WCIO Cause of Injury Code_encoded_6. PPD NSL',\n",
    " 'WCIO Part Of Body Code_encoded_4. TEMPORARY',\n",
    " 'WCIO Cause of Injury Code_encoded_4. TEMPORARY',\n",
    " 'WCIO Nature of Injury Code_encoded_1. CANCELLED',\n",
    " 'Industry Code_freq',\n",
    " 'Days_between_Assembly Date_Accident Date_log',\n",
    " 'WCIO Part Of Body Code_encoded_8. DEATH',\n",
    " 'WCIO Cause of Injury Code_encoded_5. PPD SCH LOSS',\n",
    " 'Carrier Type_Self-insured Public Entity',\n",
    " 'Carrier Name_freq',\n",
    " 'WCIO Nature of Injury Code_encoded_3. MED ONLY',\n",
    " 'Average Weekly Wage_nabinary',\n",
    " 'Carrier Type Imputed_freq',\n",
    " 'Attorney/Representative_binary',\n",
    " 'WCIO Part Of Body Code_encoded_6. PPD NSL',\n",
    " 'First Hearing Date_nabinary',\n",
    " 'Industry Code_encoded_1. CANCELLED',\n",
    " 'WCIO Part Of Body Code_encoded_3. MED ONLY',\n",
    " 'WCIO Nature of Injury Code_encoded_6. PPD NSL',\n",
    " 'WCIO Nature of Injury Code_encoded_4. TEMPORARY',\n",
    " 'Industry Code_encoded_5. PPD SCH LOSS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_filter = X_train[selected_by_3_or_more]\n",
    "X_val_filter = X_val[selected_by_3_or_more]\n",
    "X_test_filter = X_test[selected_by_3_or_more]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Multiclass Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTETomek \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `y` is your target variable\n",
    "class_distribution = y_train_encoded.value_counts()\n",
    "class_percentages = y_train_encoded.value_counts(normalize=True) * 100\n",
    "\n",
    "# Combine into a single DataFrame for better visualization\n",
    "distribution_df = pd.DataFrame({\n",
    "    'Class': class_distribution.index,\n",
    "    'Count': class_distribution.values,\n",
    "    'Percentage (%)': class_percentages.values\n",
    "})\n",
    "print(distribution_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "\n",
    "\n",
    "# Define undersampling strategy based on the actual class distribution\n",
    "undersample_strategy = {\n",
    "    0: 50621,\n",
    "    2: 47591,\n",
    "    1: 33513,\n",
    "    4: 8191,\n",
    "    4: 2924,\n",
    "    5: 324,\n",
    "    6:67\n",
    "}\n",
    "\n",
    "# Apply undersampling to majority classes\n",
    "undersampler = RandomUnderSampler(sampling_strategy=undersample_strategy, random_state=42)\n",
    "X_resampled, y_resampled = undersampler.fit_resample(X_train_filter, y_train_encoded)\n",
    "\n",
    "# # Define SMOTE strategy for oversampling the minority classes\n",
    "# smote = SMOTE(sampling_strategy={\n",
    "#     7:807,\n",
    "\n",
    "#     6: 670,   # Oversample class '5' to 3,000\n",
    "# }, random_state=42)\n",
    "\n",
    "# # Apply SMOTE to oversample minority classes\n",
    "# X_resampled, y_resampled = smote.fit_resample(X_resampled, y_resampled)\n",
    "\n",
    "# Check the new class distribution after resampling\n",
    "unique_classes_final, class_counts_final = np.unique(y_resampled, return_counts=True)\n",
    "class_distribution_final = dict(zip(unique_classes_final, class_counts_final))\n",
    "print(\"New Class Distribution after Resampling:\", class_distribution_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights = np.ones(len(y_train_encoded))\n",
    "# weights[y_train_encoded == 0] = 10\n",
    "\n",
    "\n",
    "weights = np.ones(len(y_resampled))\n",
    "weights[y_resampled == 0] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "multiclass_model = xgb.XGBClassifier(eval_metric=\"mlogloss\")\n",
    "# multiclass_model.fit(X_train_filter, y_train_encoded, sample_weight=weights)\n",
    "multiclass_model.fit(X_resampled, y_resampled, sample_weight=weights)\n",
    "\n",
    "# 3. Make predictions\n",
    "y_pred = multiclass_model.predict(X_val_filter)  # Assuming X_test is available\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_val_encoded, y_pred)\n",
    "f1_macro = f1_score(y_val_encoded, y_pred, average='macro')  # Macro-averaged F1 score\n",
    "\n",
    "# Print individual model results\n",
    "print(f\"XGBClassifier Accuracy: {accuracy:.4f}\")\n",
    "print(f\"XGBClassifier Macro F1 Score: {f1_macro:.4f}\")\n",
    "print(classification_report(y_val_encoded, y_pred))  # Detailed report including precision, recall, and F1 score per class\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Two-stage Model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict probabilities for binary classification (whether it's non-comp or not)\n",
    "y_proba_binary = stacking_model.predict(X_test_bin_filter)\n",
    "\n",
    "# y_proba_binary = binary_model.predict(xgb.DMatrix(X_test_bin_filter))\n",
    "\n",
    "# Convert probabilities to binary predictions (0 for not non-comp, 1 for non-comp)\n",
    "threshold = 0.5  # Default threshold, you can adjust based on recall\n",
    "y_pred_binary = (y_proba_binary > threshold).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the indices of instances predicted as not non-comp (i.e., predicted as 0)\n",
    "non_com =  (y_pred_binary == 1)\n",
    "not_non_comp_indices = (y_pred_binary == 0)\n",
    "\n",
    "# Use these indices to filter `X_val_filter` to get the corresponding instances\n",
    "X_test_filtered_for_multiclass = X_test_filter[not_non_comp_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict multiclass labels for the filtered data (instances not predicted as non-comp)\n",
    "y_pred_multiclass = multiclass_model.predict(X_test_filtered_for_multiclass)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X_test_filtered_for_multiclass is a DataFrame, and y_pred_multiclass are the predictions\n",
    "X_test_filtered_for_multiclass.loc[:, 'predictions'] = y_pred_multiclass\n",
    "\n",
    "# Now you have a DataFrame with the original features and the predictions as a new column\n",
    "X_test_filtered_for_multiclass.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the indices of the filtered dataset (i.e., the rows of X_test that were used for multiclass prediction)\n",
    "filtered_indices = X_test_filtered_for_multiclass.index\n",
    "\n",
    "# Now, you can map the predictions back to the original dataset by creating a new column in the original DataFrame\n",
    "X_test.loc[filtered_indices, 'predictions'] = y_pred_multiclass\n",
    "\n",
    "# Now you have the predictions in the original dataset at the corresponding rows\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the distribution of predicted classes\n",
    "predictions_df = pd.DataFrame(y_pred_multiclass, columns=[\"Predicted Label\"])\n",
    "print(\"Predicted Class Distribution:\")\n",
    "print(predictions_df[\"Predicted Label\"].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "claim_injury_type_mapping = {\n",
    "    '4. TEMPORARY': 0,\n",
    "    '5. PPD SCH LOSS': 1,\n",
    "    '3. MED ONLY': 2,\n",
    "    '6. PPD NSL': 3,\n",
    "    '1. CANCELLED': 4,\n",
    "    '8. DEATH': 5,\n",
    "    '7. PTD': 6\n",
    "}\n",
    "\n",
    "\n",
    "# Reverse the mapping\n",
    "reverse_claim_injury_type_mapping = {v: k for k, v in claim_injury_type_mapping.items()}\n",
    "\n",
    "# Use the reversed mapping to get the original labels\n",
    "X_test['predictions'] = X_test['predictions'].map(reverse_claim_injury_type_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN values in the 'predictions' column with the value '2. NON-COMP'\n",
    "X_test['predictions'] = X_test['predictions'].fillna('2. NON-COMP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test['predictions'].to_csv('test_predictions.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
