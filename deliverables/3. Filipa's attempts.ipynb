{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467cffd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#Scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    " #Correlation Heatmap\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "#Statistical Test\n",
    "from scipy import stats\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "#Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import KFold, RepeatedKFold, cross_val_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "707be7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can download the data in the source that is linked above the table of contents\n",
    "\n",
    "# Read in the data\n",
    "X_val_encoded = pd.read_csv('../project_data/X_val_encoded.csv', delimiter=',', index_col=0)\n",
    "X_train_encoded = pd.read_csv('../project_data/X_train_encoded.csv', delimiter=',', index_col=0)\n",
    "\n",
    "y_train = pd.read_csv('../project_data/y_train.csv',delimiter=',', index_col=0)\n",
    "y_val= pd.read_csv('../project_data/y_val.csv', delimiter=',', index_col=0)\n",
    "\n",
    "X_test_encoded = pd.read_csv('../project_data/X_test_encoded.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b0f7de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "382537\n",
      "382537\n"
     ]
    }
   ],
   "source": [
    "#NEW: just to check the shape\n",
    "print(X_train_encoded.shape[0])\n",
    "print(y_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "be5545a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "claim_injury_type_mapping = {\n",
    "    '4. TEMPORARY': 4,\n",
    "    '2. NON-COMP': 2,\n",
    "    '5. PPD SCH LOSS': 5,\n",
    "    '3. MED ONLY': 3,\n",
    "    '6. PPD NSL': 6,\n",
    "    '1. CANCELLED': 1,\n",
    "    '8. DEATH': 8,\n",
    "    '7. PTD': 7\n",
    "}\n",
    "\n",
    "y_train_encoded = y_train['Claim Injury Type'].map(claim_injury_type_mapping)\n",
    "y_val_encoded = y_val['Claim Injury Type'].map(claim_injury_type_mapping)\n",
    "#NEW\n",
    "y_train_encoded = pd.DataFrame(y_train_encoded, columns=['Claim Injury Type'])\n",
    "y_val_encoded = pd.DataFrame(y_val_encoded, columns=['Claim Injury Type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "70840467",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_columns = ['Age at Injury', 'Average Weekly Wage', 'Birth Year', 'IME-4 Count', 'Number of Dependents', 'Days_between_Acc_Assembyl']\n",
    "\n",
    "cat_columns = ['Alternative Dispute Resolution', 'Attorney/Representative', 'C-2 Date', 'C-3 Date', 'COVID-19 Indicator', 'Gender','First Hearing Date',\n",
    "               'CarrierGroup_Self-insured Private Entity', 'CarrierGroup_Self-insured Public Entity', 'CarrierGroup_Special Funds',\n",
    "               'CarrierGroup_State Insurance Fund', 'Industry Code_encoded_5. PPD SCH LOSS', 'Industry Code_encoded_2. NON-COMP', \n",
    "               'Industry Code_encoded_3. MED ONLY', 'Industry Code_encoded_4. TEMPORARY', 'Industry Code_encoded_1. CANCELLED', \n",
    "               'Industry Code_encoded_8. DEATH', 'Industry Code_encoded_6. PPD NSL', 'Industry Code_encoded_7. PTD',\n",
    "               'Injury_Cause_Category_encoded_5. PPD SCH LOSS', 'Injury_Cause_Category_encoded_2. NON-COMP', \n",
    "               'Injury_Cause_Category_encoded_3. MED ONLY', 'Injury_Cause_Category_encoded_4. TEMPORARY', \n",
    "               'Injury_Cause_Category_encoded_1. CANCELLED', 'Injury_Cause_Category_encoded_8. DEATH','Injury_Cause_Category_encoded_6. PPD NSL',\n",
    "                'Injury_Cause_Category_encoded_7. PTD', 'Nature_Injury_Occupational', 'Nature_Injury_Specific', 'Nature_Injury_Unknown',\n",
    "                'Part_Body_Category_encoded_5. PPD SCH LOSS', 'Part_Body_Category_encoded_2. NON-COMP', 'Part_Body_Category_encoded_3. MED ONLY',\n",
    "                'Part_Body_Category_encoded_4. TEMPORARY', 'Part_Body_Category_encoded_1. CANCELLED', 'Part_Body_Category_encoded_8. DEATH',\n",
    "                'Part_Body_Category_encoded_6. PPD NSL', 'Part_Body_Category_encoded_7. PTD']\n",
    "\n",
    "\n",
    "# Create subsets\n",
    "X_train_num = X_train_encoded[num_columns]\n",
    "X_train_cat = X_train_encoded[cat_columns]\n",
    "\n",
    "X_val_num = X_val_encoded[num_columns]\n",
    "X_val_cat = X_val_encoded[cat_columns]\n",
    "\n",
    "X_test_num=X_test_encoded[num_columns]\n",
    "X_test_cat=X_test_encoded[cat_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ae751336",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler().fit(X_train_num)\n",
    "X_train_num_scaled = scaler.transform(X_train_num)\n",
    "\n",
    "# Convert the array to a pandas dataframe\n",
    "X_train_num_scaled = pd.DataFrame(X_train_num_scaled, columns = X_train_num.columns).set_index(X_train_encoded.index)\n",
    "X_val_num_scaled = scaler.transform(X_val_num)\n",
    "X_val_num_scaled = pd.DataFrame(X_val_num_scaled, columns = X_val_num.columns).set_index(X_val_encoded.index)\n",
    "\n",
    "X_test_num_scaled = scaler.transform(X_test_num)\n",
    "X_test_num_scaled = pd.DataFrame(X_test_num_scaled, columns = X_test_num.columns).set_index(X_test_encoded.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "10d763ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remerge the numerical and categorical features\n",
    "X_train = pd.concat([X_train_num[['IME-4 Count']], X_train_cat], axis=1)\n",
    "X_val = pd.concat([X_val_num[['IME-4 Count']], X_val_cat], axis=1)\n",
    "X_test =pd.concat([X_test_num[['IME-4 Count']], X_test_cat], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b239f019",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c06c91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_score(model):\n",
    "    score_train = []\n",
    "    score_val = []\n",
    "    timer = []\n",
    "    f1_score_val = []\n",
    "    f1_score_train = []\n",
    "    \n",
    "    for i in range(10):\n",
    "        begin = time.perf_counter() # start counting time\n",
    "        model.fit(X_train, y_train) # fit your model to your training data\n",
    "        end = time.perf_counter() # stop counting time\n",
    "        \n",
    "        value_train = model.score(X_train, y_train) # mean accuracy for train\n",
    "        value_val = model.score(X_val, y_val) # mean accuracy for validation\n",
    "        score_train.append(value_train) # append the mean accuracy in train to your list score_train\n",
    "        score_val.append(value_val) # append the mean accuracy in validation to your list score_val\n",
    "        timer.append(end-begin) # append the time to your list timer\n",
    "        f1_score_val.append(f1_score(model.predict(X_val), y_val))\n",
    "        f1_score_train.append(f1_score(model.predict(X_train), y_train))\n",
    "        \n",
    "    \n",
    "    avg_time = round(np.mean(timer),3) # check the mean value of training time for your 10 models \n",
    "    avg_train = round(np.mean(score_train),3) # check the mean accuracy in train for your 10 models\n",
    "    avg_val = round(np.mean(score_val),3) # check the mean accuracy in validation for your 10 models\n",
    "    std_time = round(np.std(timer),2) # check the standard deviation of training time for your 10 models\n",
    "    std_train = round(np.std(score_train),2) # check the standard deviation of the mean accuracy in train for your 10 models\n",
    "    std_val = round(np.std(score_val),2) # check the standard deviation of the mean accuracy in validation for your 10 models\n",
    "    avg_f1_val = round(np.mean(f1_score_val),3)\n",
    "    avg_f1_train = round(np.mean(f1_score_train),3)\n",
    "    \n",
    "    return str(avg_time) + '+/-' + str(std_time), str(avg_train) + '+/-' + str(std_train),\\\n",
    "str(avg_val) + '+/-' + str(std_val), str(avg_f1_train), str(avg_f1_val)\n",
    "\n",
    "def show_results(df, models):\n",
    "    \"\"\"\n",
    "    Receive an empty dataframe and the different models and call the function avg_score\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    # for each model passed as argument\n",
    "    for model in models:\n",
    "        # obtain the results provided by avg_score\n",
    "        time, avg_train, avg_val, avg_f1_train, avg_f1_val = avg_score(model)\n",
    "        # store the results in the right row\n",
    "        df.iloc[count] = time, avg_train, avg_val, avg_f1_train, avg_f1_val\n",
    "        count+=1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6336d30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importações e conjuntos de dados permanecem fora da função\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Função para cálculo do score médio com validação cruzada\n",
    "def avg_score(model, X_train, y_train):\n",
    "    # Aplicar StratifiedKFold\n",
    "    skf = StratifiedKFold(n_splits=10)\n",
    "    \n",
    "    # Listas para armazenar os resultados\n",
    "    score_train = []\n",
    "    score_val = []\n",
    "    timer = []\n",
    "    n_iter = []\n",
    "    \n",
    "    for train_index, val_index in skf.split(X_train, y_train):\n",
    "        # Separar os dados em treino e validação\n",
    "        X_t, X_v = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "        y_t, y_v = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "        \n",
    "        # Iniciar contagem de tempo\n",
    "        begin = time.perf_counter()\n",
    "        \n",
    "        # Ajustar o modelo\n",
    "        model.fit(X_t, y_t)\n",
    "        \n",
    "        # Fim da contagem de tempo\n",
    "        end = time.perf_counter()\n",
    "        \n",
    "        # Avaliar o modelo\n",
    "        value_train = model.score(X_t, y_t)\n",
    "        value_val = model.score(X_v, y_v)\n",
    "        \n",
    "        # Armazenar resultados\n",
    "        score_train.append(value_train)\n",
    "        score_val.append(value_val)\n",
    "        timer.append(end - begin)\n",
    "        n_iter.append(getattr(model, 'n_iter_', 0))  # Evitar erro se o modelo não tiver `n_iter_`\n",
    "    \n",
    "    # Calcular médias e desvios padrão\n",
    "    avg_time = round(np.mean(timer), 3)\n",
    "    avg_train = round(np.mean(score_train), 3)\n",
    "    avg_val = round(np.mean(score_val), 3)\n",
    "    std_time = round(np.std(timer), 2)\n",
    "    std_train = round(np.std(score_train), 2)\n",
    "    std_val = round(np.std(score_val), 2)\n",
    "    avg_iter = round(np.mean(n_iter), 1)\n",
    "    std_iter = round(np.std(n_iter), 1)\n",
    "    \n",
    "    return (f\"{avg_time} +/- {std_time}\", f\"{avg_train} +/- {std_train}\", \n",
    "            f\"{avg_val} +/- {std_val}\", f\"{avg_iter} +/- {std_iter}\")\n",
    "\n",
    "# Criar o dataframe de resultados fora da função\n",
    "results_df = pd.DataFrame(columns=['Time', 'Train Accuracy', 'Validation Accuracy', 'Iterations'], index=['LogReg', 'RandomForest'])\n",
    "\n",
    "# Avaliar os modelos fora da função\n",
    "model_1 = LogisticRegression(max_iter=1000)\n",
    "model_2 = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "time1, train1, val1, iter1 = avg_score(model_1, X_train, y_train_encoded['Claim Injury Type'])\n",
    "results_df.loc['LogReg'] = [time1, train1, val1, iter1]\n",
    "\n",
    "time2, train2, val2, iter2 = avg_score(model_2, X_train, y_train_encoded['Claim Injury Type'])\n",
    "results_df.loc['RandomForest'] = [time2, train2, val2, iter2]\n",
    "\n",
    "# Exibir os resultados\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ffeb43",
   "metadata": {},
   "source": [
    "def avg_score(model):\n",
    "    # apply kfold\n",
    "    skf = StratifiedKFold(n_splits=10)\n",
    "    # create lists to store the results from the different models \n",
    "    score_train = []\n",
    "    score_val = []\n",
    "    timer = []\n",
    "    n_iter = []\n",
    "    \n",
    "    for train_index, val_index in skf.split(X,y):\n",
    "        # get the indexes of the observations assigned for each partition\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "        # start counting time\n",
    "        begin = time.perf_counter()\n",
    "        \n",
    "        \n",
    "\n",
    " #SCALING, e toda a pre processing       \n",
    "        \n",
    "        \n",
    "        \n",
    "        # fit the model to the data\n",
    "        model.fit(X_train, y_train)\n",
    "        # finish counting time\n",
    "        end = time.perf_counter()\n",
    "        # check the mean accuracy for the train\n",
    "        value_train = model.score(X_train, y_train)\n",
    "        # check the mean accuracy for the validation\n",
    "        value_val = model.score(X_val,y_val)\n",
    "        # append the accuracies, the time and the number of iterations in the corresponding list\n",
    "        score_train.append(value_train)\n",
    "        score_val.append(value_val)\n",
    "        timer.append(end-begin)\n",
    "        n_iter.append(model.n_iter_)\n",
    "    # calculate the average and the std for each measure (accuracy, time and number of iterations)\n",
    "    avg_time = round(np.mean(timer),3)\n",
    "    avg_train = round(np.mean(score_train),3)\n",
    "    avg_val = round(np.mean(score_val),3)\n",
    "    std_time = round(np.std(timer),2)\n",
    "    std_train = round(np.std(score_train),2)\n",
    "    std_val = round(np.std(score_val),2)\n",
    "    avg_iter = round(np.mean(n_iter),1)\n",
    "    std_iter = round(np.std(n_iter),1)\n",
    "    \n",
    "    return str(avg_time) + '+/-' + str(std_time), str(avg_train) + '+/-' + str(std_train),\\\n",
    "str(avg_val) + '+/-' + str(std_val), str(avg_iter) + '+/-' + str(std_iter)\n",
    "\n",
    "def show_results(df, *args):\n",
    "    \"\"\"\n",
    "    Receive an empty dataframe and the different models and call the function avg_score\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    # for each model passed as argument\n",
    "    for arg in args:\n",
    "        # obtain the results provided by avg_score\n",
    "        time, avg_train, avg_val, avg_iter = avg_score(arg)\n",
    "        # store the results in the right row\n",
    "        df.iloc[count] = time, avg_train, avg_val, avg_iter\n",
    "        count+=1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b5a4dd",
   "metadata": {},
   "source": [
    "Modelos Tradicionais de Machine Learning\n",
    "Logistic Regression Multiclasse\n",
    "\n",
    "Funcionamento: Extensão da regressão logística binária para prever várias categorias com técnicas como One-vs-Rest (OvR) ou Softmax.\n",
    "Vantagens: Simples de entender, eficiente em datasets pequenos.\n",
    "Limitações: Não lida bem com relações complexas entre as variáveis ou entre os rótulos.\n",
    "Escalabilidade: Precisa de dados normalizados.\n",
    "Decision Trees (DT)\n",
    "\n",
    "Funcionamento: Constrói árvores de decisão com base em critérios de impureza (como Gini ou Entropia).\n",
    "Vantagens: Funciona com Ordinal Encoding, não exige normalização.\n",
    "Limitações: Tende a sobreajustar em datasets pequenos.\n",
    "Random Forest (RF)\n",
    "\n",
    "Funcionamento: Ensemble de árvores de decisão para reduzir o sobreajuste.\n",
    "Vantagens: Robusto contra overfitting, lida bem com categorical encoding.\n",
    "Limitações: Pode ser mais lento em grandes datasets.\n",
    "Gradient Boosting (e.g., XGBoost, LightGBM)\n",
    "\n",
    "Funcionamento: Constrói modelos iterativamente, corrigindo erros das previsões anteriores.\n",
    "Vantagens: Excelente para dados tabulares com muitas categorias.\n",
    "Limitações: Exige Ordinal Encoding ou One-Hot Encoding.\n",
    "Modelos Baseados em Redes Neurais\n",
    "Multilayer Perceptron (MLP)\n",
    "\n",
    "Funcionamento: Redes totalmente conectadas com ativação Softmax para saída multicategoria.\n",
    "Vantagens: Flexível, pode capturar relações complexas.\n",
    "Limitações: Exige grande volume de dados, sensível à escala.\n",
    "Convolutional Neural Networks (CNNs)\n",
    "\n",
    "Funcionamento: Modelos baseados em convolução, ideais para imagens ou séries temporais.\n",
    "Vantagens: Excelente para dados espaciais.\n",
    "Limitações: Ineficiente para dados tabulares ou não estruturados.\n",
    "Recurrent Neural Networks (RNNs) e Transformers\n",
    "\n",
    "Funcionamento: Ideais para dados sequenciais (texto ou séries temporais).\n",
    "Vantagens: Capturam dependências temporais.\n",
    "Limitações: Mais complexos de treinar.\n",
    "Modelos de Similaridade e Vizinho Mais Próximo\n",
    "K-Nearest Neighbors (KNN)\n",
    "Funcionamento: Classifica com base nos rótulos dos vizinhos mais próximos.\n",
    "Vantagens: Simples de implementar, não precisa de treinamento explícito.\n",
    "Limitações: Sensível à escala (precisa de normalização).\n",
    "Modelos Estatísticos\n",
    "Naive Bayes\n",
    "Funcionamento: Calcula probabilidades assumindo independência entre atributos.\n",
    "Vantagens: Rápido, eficiente com dados textuais (TF-IDF).\n",
    "Limitações: Assume independência entre variáveis (o que nem sempre é verdade).\n",
    "Considerações sobre Pré-processamento\n",
    "Encoding de Variáveis Categóricas:\n",
    "Ordinal Encoding: Adequado para modelos baseados em árvores, mas não para métodos baseados em distância como KNN.\n",
    "One-Hot Encoding: Necessário para redes neurais e modelos lineares (Logistic Regression, SVM).\n",
    "Normalização:\n",
    "Essencial para algoritmos baseados em distância (KNN, SVM) e redes neurais.\n",
    "Dados Desequilibrados:\n",
    "Métodos como Oversampling (SMOTE) ou ajuste de pesos nas classes podem ser usados."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
