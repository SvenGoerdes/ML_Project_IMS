{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SR-FyvNSawdL"
   },
   "source": [
    "<div style=\"background-color: #bfd630; font-family: Calibri, sans-serif; padding: 20px;\">\n",
    "\n",
    "\n",
    "\n",
    "   <div style=\"text-align: center;\">\n",
    "      <img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTaJWG7PzF3toxaRMB1-JicpqMgJuEXATd0fg&\" style=\"width: 120px; margin-top: 20px; margin-bottom: 60px;\">\n",
    "   </div>\n",
    "\n",
    "   <div style=\"text-align: center; font-size: 24px; font-weight: bold; font-family: Calibri; color: #000000; margin-bottom: 10px;\">\n",
    "      Machine Learning Project | To Grant or Not to Grant\n",
    "   </div>\n",
    "   <div style=\"text-align: center; font-family: Calibri; font-size: 22px; color: #000000; font-weight: bold; margin-bottom: 20px;\">\n",
    "      2. Data Preprocessing | Feature Engineering \n",
    "   </div>\n",
    "\n",
    "   <div style=\"text-align: center; font-size: 18px; font-family: Calibri; font-weight: bold; color: #333333; margin-bottom: 5px;\">\n",
    "      Nova Information Management School\n",
    "   </div>\n",
    "\n",
    "   <div style=\"text-align: center; font-size: 18px; font-family: Calibri; font-weight: bold; color: #333333; margin-bottom: 20px;\">\n",
    "      Universidade Nova de Lisboa\n",
    "   </div>\n",
    "        <div style=\"text-align: center; font-size: 16px; font-family: Calibri; font-weight: bold; color: #333333; margin-bottom: 10px;\">\n",
    "      Master in Data Science and Advanced Analytics\n",
    "   </div>\n",
    " \n",
    "   <div style=\"text-align: center;\">\n",
    "      <img src=\"https://cdn.prod.website-files.com/617accb8b04ef2b3feffa61b/6581e90d485a9976c3576a46_how-does-workers-comp-work.jpg\" style=\"width: 350px; margin-top: 20px; margin-bottom: 60px;\">\n",
    "   </div>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "   <div style=\"text-align: center; font-family: Calibri; font-size: 16px; color: #333333; font-weight: bold; margin-bottom: 20px;\">\n",
    "      Project Group: 32\n",
    "   </div>\n",
    "\n",
    "   <div style=\"text-align: center; font-family: Calibri; font-size: 16px; color: #333333; margin-bottom: 40px\">\n",
    "      Filipa Pereira -  20240509 <br>\n",
    "      Klimentina Gilevska -  20240747 <br>\n",
    "      Maria Assunção -  20211605 <br>\n",
    "      Rita Matos -  20211642 <br>\n",
    "      Rita Wang -  20240551 <br>\n",
    "      Sven Goerdes -  20240503\n",
    "   </div>\n",
    "\n",
    "   <div style=\"text-align: center; font-family: Calibri; font-size: 16px; color: #333333; margin-bottom: 10px\">\n",
    "      Fall/Spring Semester 2024-2025\n",
    "   </div>\n",
    "\n",
    "  <div style=\"text-align: center; font-family: Calibri; font-size: 16px; color: #333333; margin-bottom: 20px;\">\n",
    "      11th November 2024\n",
    "   </div>\n",
    "\n",
    "   \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Important Information__\n",
    "> * We decided to split our work into three seperate Notebooks:\n",
    ">    * __1_Exploratory_Analysis.ipynb__\n",
    ">    * __2_Data_Preprocessing_Feature_Engineering.ipynb__\n",
    ">    * __3_Feature_Selection_and_Modeling.ipynb__\n",
    "> * It is not necessary to run 1. to be able to run 2.\n",
    "> * __Add the WCB and Test data into the project_data folder__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U6il9jY9awdc"
   },
   "source": [
    "<a id = \"toc\"></a>\n",
    "## Table of Contents\n",
    "\n",
    "* __[1. Introduction](#introduction)__\n",
    "    * [1.1. Metadata](#metadata)\n",
    "* __[2. Importing Libraries and Dataset](#importing-libraries-and-dataset)__\n",
    "    * [2.1. Import Necessary Libraries](#import-necessary-libraries)\n",
    "    * [2.2. Load the Dataset](#load-the-dataset)\n",
    "* __[3. Data Exploration](#data_exploration)__\n",
    "    * __[3.1. Understanding the Dataset Structure](#understanding-the-dataset-structure)__\n",
    "    * __[3.2. Descriptive Statistics](#descriptive-statistics)__\n",
    "    * __[3.3. Checking Incoherencies](#checking-for-incoherencies)__\n",
    "        * [3.3.1. Carrier Name & Carrier Type](#incoherences-carrier)\n",
    "        * [3.3.2. Industry Code & Description](#incoherences-industry)\n",
    "        * [3.3.3. WCIO Cause of Injury Code & Description](#incoherences-cause)\n",
    "        * [3.3.4. WCIO Nature of Injury Code & Description](#incoherences-nature)\n",
    "        * [3.3.5. WCIO Part Of Body Code & Description](#incoherences-body)\n",
    "        * [3.3.6. Average Weekly Wage](#incoherences-wage)\n",
    "        * [3.3.7. Birth Year](#incoherences-birth)\n",
    "        * [3.3.8. Number of Dependents](#incoherences-dependents)\n",
    "        * [3.3.9. Age at Injury](#incoherences-age)\n",
    "        * [3.3.10. Accident Date](#incoherences-accident)\n",
    "        * [3.3.11. Assembly Date](#incoherences-assembly)\n",
    "        * [3.3.12. C-2 Date](#incoherences-c2)\n",
    "        * [3.3.13. C-3 Date](#incoherences-c3)\n",
    "        * [3.3.14. First Hearing Date](#incoherences-1hearing)\n",
    "        * [3.3.15. COVID-19 Indicator](#incoherences-covid)\n",
    "* __[4. Data Splitting](#data-splitting)__\n",
    "* __[5. Data Cleaning and Pre-processing](#data_cleaning)__\n",
    "    *  __[5.1 Handling Missing Values](#handling-missing-values)__\n",
    "        * [5.1.1. IME-4 Count](#missing-ime)\n",
    "        * [5.1.2. First Hearing Date](#missing-first)\n",
    "        * [5.1.3. C-2 and C-3 Date](#missing-c2-c3)\n",
    "        * [5.1.4. Birth Year](#missing-birth)\n",
    "        * [5.1.5. Medical Fee Region](#missing-medical)\n",
    "        * [5.1.6. Zip Code](#missing-zip)\n",
    "        * [5.1.7. WCIO Part Of Body Code & Description](#missing-part-body)\n",
    "        * [5.1.8. WCIO Nature of Injury Code & Description](#missing-nature)\n",
    "        * [5.1.9. WCIO Cause of Injury Code & Description](#missing-cause)\n",
    "        * [5.1.10. Industry Code & Description](#missing-industry)\n",
    "        * [5.1.11. Average Weekly Wage](#missing-avg)\n",
    "        * [5.1.12. Accident Date](#missing-acc)\n",
    "        * [5.1.13. Age at Injury](#missing-age)\n",
    "        * [5.1.14. Gender](#missing-gender)\n",
    "        * [5.1.15. Carrier Type](#missing-carrier)\n",
    "        * [5.1.16. County of Injury](#missing-county)\n",
    "        * [5.1.17. Alternative Dispute Resolution](#missing-adr)\n",
    "    * __[5.2. Outlier Detection and Treatment](#outlier-detection-and-treatment)__\n",
    "        * [5.2.1. Date Outliers](#date-outliers)\n",
    "        * [5.2.2. Numerical Outliers](#numerical-outliers)\n",
    "        * [5.2.3. Applying Changes](#applying-changes)\n",
    "    * __[5.3. Dealing with Categorical Variables](#categorical-variables)__\n",
    "        * [5.3.1. Encoding Binary Variables](#binary-variables)\n",
    "        * [5.3.2.  Carrier Type Encoding](#carrier-type)\n",
    "        * [5.3.3. Industry Code](#industry-code)\n",
    "        * [5.3.4.  WCIO Codes encoding](#WCIO_Codes_Encoding)\n",
    "        * [5.3.5. Numerical Variables Decoding](#numerical-encoding)\n",
    "    * __[5.4 Feature Engineering](#feature-engineering)__\n",
    "    * __[5.5 Numerical Variables Decoding](#numerical-encoding)__\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SfSi0txYawde"
   },
   "source": [
    "## 1. Introduction<a class=\"anchor\" id=\"introduction\"></a>\n",
    "[Back to ToC](#toc)<br>\n",
    "\n",
    "* The New York Workers' Compensation Board (from now on referred as WCB) was created in 1914 with the purpose of “protecting the rights of employees and employers arising from the self-delivery of benefits and promoting compliance with the law\". [link](https://www.wcb.ny.gov/content/main/TheBoard/InfoAbout.jsp)<br>\n",
    "* The idea of creating some kind of support for injured workers, however, emerged in 1909 although at the time there was a lot of skepticism around it. It was only after the tragic fire at the Triangle shirt factory (1911) that the New York’s Workers’ Compensation Law and the Workmen’s Compensation Commission (WCC) were actually created, the latter of which would later become the WCB as it is currently known. <br>\n",
    "* Over the years, WCB has continuously adapted to meet current needs by reducing costs for workers, expanding the types of covered illnesses and injuries and others. In fact, this support has become crucial for those facing work-related injuries. [link](https://www.wcb.ny.gov/WCB_Centennial_Booklet.pdf)\n",
    "<br>\n",
    "\n",
    "### 1.1. Metadata <a class=\"anchor\" id=\"metadata\"></a>\n",
    "[Back to ToC](#toc)<br>\n",
    "\n",
    "|                              Column Name                            | Description                                                                                         |\n",
    "|:-------------------------------------------------------------------:|:----------------------------------------------------------------------------------------------------|\n",
    "|                        **Accident Date**                          | Injury date of the claim.                                                                           |\n",
    "|                          **Age at Injury**                        | Age of injured worker when the injury occurred.                                                     |\n",
    "|              **Alternative Dispute Resolution**                   | Adjudication processes external to the Board.                                                       |\n",
    "|                        **Assembly Date**                          | The date the claim was first assembled.                                                             |\n",
    "|                    **Attorney/Representative**                    | Is the claim being represented by an Attorney?                                                      |\n",
    "|                     **Average Weekly Wage**                       | The wage used to calculate workers’ compensation, disability, or Paid Leave wage replacement benefits.|\n",
    "|                            **Birth Year**                         | The reported year of birth of the injured worker.                                                   |\n",
    "|                            **C-2 Date**                           | Date of receipt of the Employer’s Report of Work-Related Injury/Illness or equivalent (formerly Form C-2).|\n",
    "|                             **C-3 Date**                          | Date Form C-3 (Employee Claim Form) was received.                                                   |\n",
    "|                           **Carrier Name**                        | Name of primary insurance provider responsible for providing workers’ compensation coverage to the injured worker’s employer.|\n",
    "|                           **Carrier Type**                        | Type of primary insurance provider responsible for providing workers’ compensation coverage.        |\n",
    "|                        **Claim Identifier**                       | Unique identifier for each claim, assigned by WCB.                                                  |\n",
    "|                        **County of Injury**                       | Name of the New York County where the injury occurred.                                              |\n",
    "|                      **COVID-19 Indicator**                       | Indication that the claim may be associated with COVID-19.                                          |\n",
    "|                            **District Name**                      | Name of the WCB district office that oversees claims for that region or area of the state.          |\n",
    "|                      **First Hearing Date**                       | Date the first hearing was held on a claim at a WCB hearing location. A blank date means the claim has not yet had a hearing held.|\n",
    "|                           **Gender**                              | The reported gender of the injured worker.                                                          |\n",
    "|                          **IME-4 Count**                          | Number of IME-4 forms received per claim. The IME-4 form is the “Independent Examiner's Report of Independent Medical Examination” form.|\n",
    "|                        **Industry Code**                          | NAICS code used to classify businesses according to their economic activity. Descriptions are available at https://www.naics.com. |\n",
    "|                 **Industry Code Description**                     | 2-digit NAICS industry code description used to classify businesses according to their economic activity. |\n",
    "|                     **Medical Fee Region**                        | Approximate region where the injured worker would receive medical service.                          |\n",
    "|                    **Number of Dependents**                       | Number of dependents when the injury occurred.|\n",
    "|           **OIICS Nature of Injury Description**                  | The OIICS nature of injury codes & descriptions are available at https://www.bls.gov/iif/oiics_manual_2007.pdf.|\n",
    "|                **WCIO Cause of Injury Code**                      | The WCIO cause of injury codes & descriptions are available at https://www.wcio.org/Active%20PNC/WCIO_Cause_Table.pdf.|\n",
    "|            **WCIO Cause of Injury Description**                   | Description of the cause of injury, as per WCIO codes.                                               |\n",
    "|                 **WCIO Nature of Injury Code**                    | The WCIO nature of injury codes are available at https://www.wcio.org/Active%20PNC/WCIO_Nature_Table.pdf.|\n",
    "|               **WCIO Nature of Injury Description**               | Description of the nature of injury, as per WCIO codes.                                             |\n",
    "|                **WCIO Part Of Body Code**                         | The WCIO part of body codes & descriptions are available at https://www.wcio.org/Active%20PNC/WCIO_Part_Table.pdf.|\n",
    "|              **WCIO Part Of Body Description**                    | Description of the body part injured, as per WCIO codes.                                            |\n",
    "|                           **Zip Code**                            | The reported ZIP code of the injured worker’s home address.                                          |\n",
    "|                     **Agreement Reached**                        | Binary variable: Yes if there is an agreement without the involvement of the WCB. Unknown at the start of a claim.|\n",
    "|                        **WCB Decision**                          | Multiclass variable: Decision of the WCB relative to the claim. “Accident” means workplace accident, “Occupational Disease” means illness from the workplace. Requires WCB deliberation, so it is unknown at start of claim.|\n",
    "|                     **Claim Injury Type**                        | Main target variable: Deliberation of the WCB relative to benefits awarded to the claim. Numbering indicates severity. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cohjk4Qaawdg"
   },
   "source": [
    "## 2. Importing Libraries and Dataset <a class=\"anchor\" id=\"importing-libraries-and-dataset\"></a>\n",
    "[Back to ToC](#toc)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eJB4Tjy_awdg"
   },
   "source": [
    "### 2.1. Import Necessary Libraries <a class=\"anchor\" id=\"import-necessary-libraries\"></a>\n",
    "[Back to ToC](#toc)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from category_encoders import TargetEncoder\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as msno # for missing values\n",
    "import sys\n",
    "sys.path.append('../helper_functions')\n",
    "\n",
    "# data partition\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import functions that are stored in the helper_functions directory. We do this to keep the notebook clean and easy to read\n",
    "from helper_functions import impute_with, days_between, target_encode_multiclass, invalid_entries, multiple_unique_values,  missing_data, impute_dates_with_difference, impute_prop, days_between, target_encode_multiclass, remove_outliers_iqr, iqr_date, log_remove_outliers_iqr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lFBMJUaUawdj"
   },
   "source": [
    "### 2.2. Load the Dataset <a class=\"anchor\" id=\"load-the-dataset\"></a>\n",
    "[Back to ToC](#toc)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rAljsQ4Iawdl"
   },
   "source": [
    "> * After attempting to load the dataset, we noticed that column 29 (Zip Code) has mixed data types, so we decided to change it to a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 538
    },
    "id": "b1EvzA8rawdl",
    "outputId": "cb506ce6-a940-4151-f855-7c7be47f772d"
   },
   "outputs": [],
   "source": [
    "WCB_original = pd.read_csv('../project_data/train_data.csv', delimiter=',',dtype={'Zip Code': str})\n",
    "X_test = pd.read_csv('../project_data/test_data.csv', delimiter=',',dtype={'Zip Code': str})\n",
    "\n",
    "# show all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "WCB_original.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oVg_Quf6awdn"
   },
   "source": [
    "> At first glance, we can observe that there are not only missing values in the dataset, but there is at least one row that contains an excessive number of missing entries. Additionally, we note that some workers have an average weekly wage of zero, which does not seem to make sense.     \n",
    "Let's investigate further.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DTM2ozboawdn"
   },
   "source": [
    "## 3. Data Exploration <a class=\"anchor\" id=\"data_exploration\"></a>\n",
    "[Back to ToC](#toc)<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fy6TJ_3Rawdn"
   },
   "source": [
    "### 3.1. Understanding the Dataset Structure <a class=\"anchor\" id=\"understanding-the-dataset-structure\"></a>\n",
    "[Back to ToC](#toc)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Check duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LSBf1u7nawdo",
    "outputId": "7e37a5d5-681f-4a79-9b61-ed3f43890581"
   },
   "outputs": [],
   "source": [
    "dup_id = WCB_original[WCB_original['Claim Identifier'].duplicated(keep=False)]\n",
    "dup_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Drop one of the duplicates  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WCB= WCB_original.copy() #This way we can access the original dataset if needed\n",
    "\n",
    "# It doesn't matter which one we drop\n",
    "WCB= WCB.drop(index=257901)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_id_test = X_test[X_test['Claim Identifier'].duplicated(keep=False)]\n",
    "dup_id_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Set the index column as index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T9V_EsrJawdp"
   },
   "outputs": [],
   "source": [
    "\n",
    "WCB.set_index('Claim Identifier', inplace=True)\n",
    "X_test.set_index('Claim Identifier', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jAhgKkUSawdp"
   },
   "source": [
    ">Here, we first identified and reviewed duplicates in the <code>Claim Identifier</code> column, finding two rows with almost all NaN values. We dropped one, ensuring each claim identifier is unique. Then we set <code>Claim Identifier</code> as the index in both WCB and X_test, which is more meaningful than using simple row numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ipGqL1UOawdp",
    "outputId": "1aee6f4d-ef74-4ec4-f1c0-4ee8e7426874"
   },
   "outputs": [],
   "source": [
    "WCB.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4-bZ03ofawdq"
   },
   "source": [
    "> __Missing Values:__\n",
    "> - All columns, except for the <code>Assembly Date</code> have missing values.    \n",
    "> - Since our target variable is <code>Claim Injury Type</code> (which refers to the WCB decision regarding the benefits awarded to the claim), we decided to drop rows where it is missing.<br>\n",
    "> - Additionally, the <code>OIICS Nature of Injury Description</code> column has zero non-null entries, so we drop it entirely.\n",
    "> \n",
    "> __Data Types:__\n",
    "> - <code>Accident Date</code>, <code>Assembly Date</code>, <code>C-2 Date</code>, <code>C-3 Date</code>, <code>First Hearing Date</code>: should likely be in datetime format for easier date manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4_oDPazGawdq"
   },
   "outputs": [],
   "source": [
    "WCB= WCB.dropna(subset=['Claim Injury Type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H-tXbxPFawdq"
   },
   "outputs": [],
   "source": [
    "WCB = WCB.drop(columns=['OIICS Nature of Injury Description'])\n",
    "X_test =  X_test.drop(columns=['OIICS Nature of Injury Description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R8-4wRlTawdq"
   },
   "outputs": [],
   "source": [
    "# Identify original missing values\n",
    "date_columns = ['Accident Date', 'Assembly Date','C-2 Date', 'C-3 Date', 'First Hearing Date']\n",
    "original_missing = WCB[date_columns].isna()\n",
    "\n",
    "# Convert columns to datetime\n",
    "for column in date_columns:\n",
    "    WCB[column] = pd.to_datetime(WCB[column], format='%Y-%m-%d', errors='coerce')\n",
    "    X_test[column] = pd.to_datetime(X_test[column], format='%Y-%m-%d', errors='coerce')\n",
    "\n",
    "    # Find new NaT values that weren't originally missing\n",
    "    invalid_dates = WCB[WCB[column].isna() & ~original_missing[column]]\n",
    "    if not invalid_dates.empty:\n",
    "        print(f\"Invalid dates found in column '{column}':\")\n",
    "        print(invalid_dates[[column]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0DKHonB1awdr"
   },
   "source": [
    "> All dates are in the correct format, meaning that there are no dates like: y-02-31 or y-14-02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BdeQlZVaawdr"
   },
   "source": [
    "> Let’s take a closer look at the missing values now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 551
    },
    "id": "HazmlOanawdr",
    "outputId": "3a51466c-3099-493f-fc2c-7eab3b637e9d"
   },
   "outputs": [],
   "source": [
    "missing_data(WCB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8eWhHVBaawds"
   },
   "source": [
    "By performing those drops, we were able to address the missing values in several variables (<code>Age at Injury</code>, <code>Alternative Dispute Resolution</code>, <code>Assembly Date</code>, <code>Attorney/Representative</code>, <code>Carrier Name</code>, <code>Carrier Type</code>, <code>Claim Injury Type</code>, <code>County of Injury</code>, <code>COVID-19 Indicator</code>, <code>District Name</code>, <code>Gender</code>, <code>Medical Fee Region</code>, <code>Agreement Reached</code>, <code>WCB Decision</code>, <code>Number of Dependents</code>).  \n",
    "\n",
    "Still, we're left with a lot of missing values.     \n",
    "The variables with the highest number of missing values are: <code>IME-4 Count</code>, <code>First Hearing Date</code>, and <code>C-3 Date</code>.      \n",
    "The following matrix helps us determine if missing data are related across different columns, indicating, for instance, whether the absence of values in one column correlates with missing values in another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 371
    },
    "id": "YWAa6NSSawds",
    "outputId": "afd7e17a-2829-4f3a-dc0e-562cd3beeee1"
   },
   "outputs": [],
   "source": [
    "msno.matrix(WCB, labels=True, sort=\"descending\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We can see that, for the most part, rows with missing values in the variable <code>C-3 Date</code> also have missing values in <code>First Hearing Date</code> and <code>IME-4 Count</code>, possibly because these processes never took place. We’ll investigate this further later on. It’s also noticeable that code variables are missing whenever their respective descriptions are missing, suggesting that direct imputation of these missing values may not be feasible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let’s take a closer look at duplicates now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 354
    },
    "id": "8Lfm5QGDawds",
    "outputId": "f25126b8-81d2-462e-c5a5-e2a87df73a3c"
   },
   "outputs": [],
   "source": [
    "# Find the duplicated rows excluding the 'Claim Identifier'\n",
    "duplicated_rows = WCB[WCB.duplicated(subset=WCB.columns.difference(['Claim Identifier']), keep=False)]\n",
    "duplicated_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n30doLEsawdt"
   },
   "source": [
    "> We identified two identical rows (excluding the Claim Identifier), suggesting they likely refer to the same injured worker.  Therefore, we will remove one of these duplicate rows to maintain data integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6KzBwVizawdt"
   },
   "outputs": [],
   "source": [
    "# It doesn't matter which one we drop\n",
    "WCB = WCB.drop(index=5686771)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kQyYYv4xawdu"
   },
   "source": [
    "> There are also rows where all variables are identical except for one. We believe it would take many coincidences for this to occur naturally, so we considered these rows as duplicates as well and decided to remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "keZ7mPY1hseK",
    "outputId": "3efb42bd-c36c-4bf1-faf1-59e1ecf659a9"
   },
   "outputs": [],
   "source": [
    "print(\"Before removing duplicates: \",WCB.shape[0],\"\\n\")\n",
    "\n",
    "# List of columns to check for duplicates\n",
    "columns_to_check = ['Age at Injury', 'Accident Date', 'Assembly Date', 'C-2 Date', 'C-3 Date',\n",
    "                    'Alternative Dispute Resolution', 'Attorney/Representative', 'Average Weekly Wage',\n",
    "                    'Carrier Name', 'Carrier Type', 'County of Injury', 'COVID-19 Indicator', 'District Name',\n",
    "                    'First Hearing Date', 'Birth Year', 'Gender', 'IME-4 Count', 'Industry Code',\n",
    "                    'Industry Code Description', 'Medical Fee Region', 'WCIO Cause of Injury Code',\n",
    "                    'WCIO Cause of Injury Description', 'WCIO Nature of Injury Code', 'WCIO Nature of Injury Description',\n",
    "                    'WCIO Part Of Body Code', 'WCIO Part Of Body Description', 'Zip Code', 'Agreement Reached',\n",
    "                    'WCB Decision', 'Claim Injury Type', 'Number of Dependents']\n",
    "\n",
    "# Iterate over each column, excluding one at a time\n",
    "for col in columns_to_check:\n",
    "    # Define the subset of columns to check in this iteration (excluding 'col')\n",
    "    cols_to_check_now = [c for c in columns_to_check if c != col]\n",
    "\n",
    "    # Identify duplicates based on these columns\n",
    "    duplicates = WCB[WCB.duplicated(subset=cols_to_check_now, keep=False)]\n",
    "\n",
    "    #Drop duplicates, keeping the first occurrence in each subset where one column can differ\n",
    "    WCB = WCB.drop_duplicates(subset=cols_to_check_now, keep='first')\n",
    "\n",
    "print(\"After removing duplicates: \",WCB.shape[0],\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let's look at the unique values of each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u_GITloYmMk0",
    "outputId": "fa4f19f6-21a6-4fcc-d452-b43beb4476d7"
   },
   "outputs": [],
   "source": [
    "# Get unique counts for each column\n",
    "unique_counts = WCB.nunique()\n",
    "\n",
    "# Filter columns with less than 100 unique values\n",
    "filtered_col = [col for col in unique_counts.index if unique_counts[col] < 100]\n",
    "\n",
    "# Display unique values for filtered columns\n",
    "print(\"Unique values for each column:\\n\")\n",
    "for col in filtered_col:\n",
    "    print(f\"{col} ({unique_counts[col]}): {WCB[col].unique()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> From these unique values, we can highlight a few points:   \n",
    "> - Some missing values are disguised as \"unknown.\"\n",
    "> - There are 8 distinct values for the target variable, <code>Claim Injury Type</code>, indicating a multi-class classification problem.\n",
    "> - <code>WCB Decision</code> only has a single value, which not only seems incorrect across all rows but also lacks variability, so we decided to drop this column.\n",
    "> - There are multiple categories within Part of Body, Nature of Injury, and Cause of Injury, suggesting we may need to group them in a meaningful way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KLHyJr2On3vu"
   },
   "outputs": [],
   "source": [
    "#Drop WCB Decision as it only has 1 value\n",
    "WCB = WCB.drop(columns=['WCB Decision'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zh1XfwvcmfZ8",
    "outputId": "0f26f29e-e446-4648-ccad-3dac938ea671"
   },
   "outputs": [],
   "source": [
    "#Print the name of variables with Unknown values\n",
    "WCB.columns[WCB.isin(['U', 'Unknown', 'UNKNOWN']).any()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a94MJpWimqKN",
    "outputId": "045d9c63-2a28-40c2-bcd6-67e11dfbbd4c"
   },
   "outputs": [],
   "source": [
    "unknown_values = {'Alternative Dispute Resolution': 'U',   'Carrier Type': 'UNKNOWN', 'County of Injury': 'UNKNOWN',\n",
    "    'Gender': 'U','Medical Fee Region': 'UK'\n",
    "}\n",
    "\n",
    "for col, unknown in unknown_values.items():\n",
    "  # Calculate the percentage of rows with the unknown value\n",
    "  unknown_percentage = (WCB[WCB[col] == unknown].shape[0] / WCB.shape[0]) * 10\n",
    "  print(f\"Percentage of rows where {col} == '{unknown}': {unknown_percentage:.5f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For now, we will set these values as missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hYJCxQofm5uw"
   },
   "outputs": [],
   "source": [
    "WCB.replace(unknown_values, np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KNi4A9h5awd7"
   },
   "source": [
    "### 3.2. Descriptive Statistics <a class=\"anchor\" id=\"descriptive-statistics\"></a>\n",
    "[Back to ToC](#toc)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "id": "bmBVcsCMawd7",
    "outputId": "59077a65-9797-4e57-b3cb-36bda5cbdfa8"
   },
   "outputs": [],
   "source": [
    "round(WCB.describe(),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Most of the accidents in our dataset occurred in recent years (since 2020), although there are cases dating back to 1961. It may not make sense to include these older cases in our analysis, as there have been numerous changes over the years in all aspects of the claims process and the operation of the WCB itself. For other date-related variables, we don’t see dates as far back as these.\n",
    "> - There are injured workers of all ages, which seems a bit unusual. It is particularly odd that the minimum recorded age for <code>Age at Injury</code> is 0 and the maximum is 117.\n",
    "> - It is concerning that more than half of the injured workers have an <code>Average Weekly Wage</code> of 0. The mean of AWW is much higher than the median, indicating a right-skewed distribution, meaning there is a longer tail to the right, with some extremely high values (outliers) pulling the mean upward.\n",
    "> - A <code>Birth Year</code> of 0 doesn’t make sense.\n",
    "> - The minimum value for <code>WCIO Part Of Body Code</code> is -9, which may not be a valid code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 589
    },
    "id": "MmlgWgxDawd7",
    "outputId": "9b63809f-56db-4442-af07-117f75afdf4f"
   },
   "outputs": [],
   "source": [
    "WCB.describe(include='object').T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lZTmJP6dawd8"
   },
   "source": [
    "> Among the various insights this provides, we can highlight:         \n",
    "• More than 99% of the rows that do not have <code>Alternative Dispute Resolution</code> as missing take the value 'N'.       \n",
    "• All counties in New York are represented here (also looking at the unique values listed before). [link](https://www.ny.gov/counties)     \n",
    "• About half of the claims fall under category 2: NON-COMP -> No indemnity benefits awarded for the claim, according to the [link](https://data.ny.gov/Government-Finance/Assembled-Workers-Compensation-Claims-Beginning-20/jshw-gkgu/about_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __NOTE:__ All exploratory data visualizations analyzing individual variables and multivariate relationships are in the notebook __1_Explorator_Analysis__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Checking Incoherencies<a class=\"anchor\" id=\"checking-for-incoherencies\"></a>\n",
    "[Back to ToC](#toc)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Checking if every WCIO code column has the same number of unique values as the corresponding description column\n",
    "> * We create several dictionaries here in case we need to access them later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary with WCIO Cause of Injury Code and Description. WCIO Nature of Injury Code and Description, WCIO Part of Body Code and Description, Industry Code and Description\n",
    "C_o_In_indicators = WCB[['WCIO Cause of Injury Code', 'WCIO Cause of Injury Description',]].drop_duplicates().set_index('WCIO Cause of Injury Code').sort_index()\n",
    "N_o_In_indicators = WCB[['WCIO Nature of Injury Code', 'WCIO Nature of Injury Description',]].drop_duplicates().set_index('WCIO Nature of Injury Code').sort_index()\n",
    "P_o_B_indicators = WCB[['WCIO Part Of Body Code', 'WCIO Part Of Body Description']].drop_duplicates().set_index('WCIO Part Of Body Code').sort_index()\n",
    "Ind_indicators = WCB[['Industry Code', 'Industry Code Description',]].drop_duplicates().set_index('Industry Code').sort_index()\n",
    "\n",
    "# check whether there are codes two times and print it with the len of the WCB\n",
    "print(len(C_o_In_indicators[C_o_In_indicators.index.duplicated()])) , len(N_o_In_indicators[N_o_In_indicators.index.duplicated()]) , len(P_o_B_indicators[P_o_B_indicators.index.duplicated()]) , len(Ind_indicators[Ind_indicators.index.duplicated()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "US70H3Ulawd9"
   },
   "source": [
    "> By examining the unique values of each column and delving deeper into each variable, we have identified inconsistencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_OndUAFoawd9",
    "outputId": "cf58e037-358d-41e6-8894-a9dc4da465f7"
   },
   "outputs": [],
   "source": [
    "# Replace the strange characters by NaN\n",
    "strange_values = [\"!\",\"$\",\"%\",\"?\",\"*\",\"+\",\"_\",\"@\",\"€\",\" \",\"{\"]\n",
    "# Count the number of cases where any column contains strange characters\n",
    "strange_cases = WCB[WCB.isin(strange_values).any(axis=1)]\n",
    "strange_cases.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sRZ1oE6iawd9"
   },
   "source": [
    "> We had already seen with the unique values that there were no strange characters, but this way we confirm it better. If there were any, we would change the values to NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CM_x029Sawd-"
   },
   "source": [
    "#### 3.3.1. Carrier Name & Carrier Type<a class=\"anchor\" id=\"incoherences-carrier\"></a>\n",
    "[Back to ToC](#toc)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "voT2tJWhawd-"
   },
   "source": [
    "> Let's verify if each 'Carrier Name' is associated with only a single 'Carrier Type'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PSRchgpZawd-",
    "outputId": "1846270d-c614-4f0b-9694-b0dda693c7d7"
   },
   "outputs": [],
   "source": [
    "multiple_unique_values(WCB,'Carrier Name','Carrier Type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cTYhY4OHaweB"
   },
   "source": [
    "This carrier name doesn't make sense. It actually corresponds to the carrier type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O-xdcYacaweB"
   },
   "outputs": [],
   "source": [
    "#special_funds_rows = WCB[WCB['Carrier Type'] == '5A. SPECIAL FUND - CONS. COMM. (SECT. 25-A)']\n",
    "#print(special_funds_rows)\n",
    "\n",
    "# Update the Carrier Type for rows where Carrier Name is 'SPECIAL FUNDS SEC 25-A'\n",
    "WCB.loc[WCB['Carrier Name'] == 'SPECIAL FUNDS SEC 25-A', 'Carrier Type'] = '5A. SPECIAL FUND - CONS. COMM. (SECT. 25-A)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FByURo5aaweC"
   },
   "source": [
    "> Theoretically, we should also change the Carrier Name, as it is technically incorrect. However, since there are no other Carrier Names associated with this Carrier Type ('5A. SPECIAL FUND - CONS. COMM. (SECT. 25-A)') and we are unlikely to use the Carrier Name variable in our models, we will leave Carrier Name as is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dwm3PdoZaweC"
   },
   "source": [
    "#### 3.3.2. Industry Code & Description<a class=\"anchor\" id=\"incoherences-industry\"></a>\n",
    "[Back to ToC](#toc)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9i7sqFZmaweD",
    "outputId": "78817ce5-b476-473f-eb45-d15675322220"
   },
   "outputs": [],
   "source": [
    "multiple_unique_values(WCB,'Industry Code','Industry Code Description')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_unique_values(WCB,'Industry Code Description','Industry Code')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Each code has only one description. But each description doesn’t always have the same code. Given our [source](https://www.naics.com/search/), we assume that these are not errors:      \n",
    "31-33 Manufacturing    \n",
    "44-45 Retail Trade    \n",
    "48-49 Transportation and Warehousing   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FEREp29DaweF"
   },
   "source": [
    "#### 3.3.3. WCIO Cause of Injury Code & Description<a class=\"anchor\" id=\"incoherences-cause\"></a>\n",
    "[Back to ToC](#toc)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-iS2wcfkaweG",
    "outputId": "d0b3fc05-9a83-4936-9215-131475f271ea"
   },
   "outputs": [],
   "source": [
    "multiple_unique_values(WCB,'WCIO Cause of Injury Code','WCIO Cause of Injury Description')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0_cWK7fgaweG",
    "outputId": "823ae158-0e9a-461f-a36f-c996d2b61919"
   },
   "outputs": [],
   "source": [
    "multiple_unique_values(WCB,'WCIO Cause of Injury Description','WCIO Cause of Injury Code')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zJuvBqwiaweG"
   },
   "source": [
    "> Each code has only one description. But each description doesn’t always have the same code. Given our [source](https://www.guarantysupport.com/wp-content/uploads/2024/02/WCIO-Legacy.pdf), we assume that these are not errors and indeed represent different types of injury:    \n",
    "17- Object Being Lifted or Handled; Includes being cut, punctured or scraped by a person or object being lifted or handled;      \n",
    "66- Object Being Lifted or Handled;    \n",
    "79- Object Being Lifted or Handled; Includes dropping object on body part;    \n",
    "94- Repetitive Motion; Caused by repeated rubbing or abrading; applies to non-impact cases in which the injury was produced by pressure, vibration or friction between the person and the source of injury. Includes callous, blister;    \n",
    "97- Repetitive Motion; Cumulative injury or condition caused by continual, repeated motions; strain by excessive use. Includes Carpal Tunnel Syndrome.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TcDDBQSUaweH"
   },
   "source": [
    "#### 3.3.4. WCIO Nature of Injury Code & Description<a class=\"anchor\" id=\"incoherences-nature\"></a>\n",
    "[Back to ToC](#toc)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J6_LfUkgaweI",
    "outputId": "a435bcc3-8cf0-437f-952d-3b09227597f5"
   },
   "outputs": [],
   "source": [
    "multiple_unique_values(WCB,'WCIO Nature of Injury Code', 'WCIO Nature of Injury Description')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UA1E-Y5VaweI",
    "outputId": "9b0d2a69-cc84-4a62-d9a5-a74e4aee2918"
   },
   "outputs": [],
   "source": [
    "multiple_unique_values(WCB,'WCIO Nature of Injury Description','WCIO Nature of Injury Code')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RiENvzH0aweI"
   },
   "source": [
    "For this variable, each code has one and only one description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I7AvOIDZaweJ"
   },
   "source": [
    "#### 3.3.5. WCIO Part Of Body Code & Description<a class=\"anchor\" id=\"incoherences-body\"></a>\n",
    "[Back to ToC](#toc)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6ZVrmS2GaweJ",
    "outputId": "6e7f0ce1-ce33-4671-fead-c15905cac1bf"
   },
   "outputs": [],
   "source": [
    "multiple_unique_values(WCB, 'WCIO Part Of Body Code','WCIO Part Of Body Description')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gsT1g2BTaweL",
    "outputId": "121ea6b0-996d-4244-e028-008ff235b083"
   },
   "outputs": [],
   "source": [
    "multiple_unique_values(WCB, 'WCIO Part Of Body Description', 'WCIO Part Of Body Code')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xwl9SYjZaweM"
   },
   "source": [
    "> Each code has only one description. But each description doesn’t always have the same code. Given our [Source](https://www.guarantysupport.com/wp-content/uploads/2024/02/WCIO-Legacy.pdf), we assume that these are not errors and indeed represent different parts of the body injured:  \n",
    "22- Disc; Includes: spinal column cartilage, \"cervical segment\";  \n",
    "43- Disc; Spinal column cartilage other than cervical segment;   \n",
    "18- Soft Tissue;    \n",
    "25- Soft Tissue; Other than larynx or trachea;   \n",
    "23- Spinal Cord; Includes: nerve tissue, \"cervical segment\";    \n",
    "47- Spinal Cord; Nerve tissue other than cervical segment.     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DS6URRz5aweM"
   },
   "outputs": [],
   "source": [
    "# Update the rows where 'WCIO Part Of Body Code' is -9\n",
    "WCB.loc[WCB['WCIO Part Of Body Code'] == -9, 'WCIO Part Of Body Code'] = 90\n",
    "WCB.loc[WCB['WCIO Part Of Body Code'] == 90, 'WCIO Part Of Body Description'] = 'MULTIPLE BODY PARTS'\n",
    "\n",
    "# do the same for the test data\n",
    "X_test.loc[X_test['WCIO Part Of Body Code'] == -9, 'WCIO Part Of Body Code'] = 90\n",
    "X_test.loc[X_test['WCIO Part Of Body Code'] == 90, 'WCIO Part Of Body Description'] = 'MULTIPLE BODY PARTS'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s7RJrDZTaweM"
   },
   "source": [
    "We identified that there was a code -9 with a description similar to that of code 90, so we combined the two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kATBA3fJaweM"
   },
   "source": [
    "#### 3.3.6. Average Weekly Wage<a class=\"anchor\" id=\"incoherences-wage\"></a>\n",
    "[Back to ToC](#toc)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tEGnhlkcaweM",
    "outputId": "ead7ae10-6520-40a6-a46d-b8e322cdf7da"
   },
   "outputs": [],
   "source": [
    "# 10 lowest 'Average Weekly Wage'\n",
    "bottom_10_AWW = WCB['Average Weekly Wage'].drop_duplicates().sort_values(ascending=True).head(10)\n",
    "print(bottom_10_AWW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mmK1FJ-WaweN",
    "outputId": "98bb624e-43f3-4f05-b2aa-d17521a8c82d"
   },
   "outputs": [],
   "source": [
    "rows_wage_0 = WCB[WCB['Average Weekly Wage'] == 0].shape[0]/WCB.shape[0]*100\n",
    "print('There are', round(rows_wage_0,2), '% of rows with an Average Weekly Wage of 0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This variable has a significant number of zeros (error), which may be due to workers' reluctance to disclose their Average Weekly Wage. We will put these as missing, understanding that this will still leave a substantial amount of missing data. However, given the variable's potential importance in predicting the Claim Injury Type, we will make efforts to later impute the missing values as effectively as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ZsX4qdWaweN"
   },
   "outputs": [],
   "source": [
    "# Replace all instances of Average Weekly Wage == 0 with NaN\n",
    "WCB.loc[WCB['Average Weekly Wage'] == 0, 'Average Weekly Wage'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z0n1vDGTaweO",
    "outputId": "5d02fba8-6342-4c6e-9c54-d622b14df282"
   },
   "outputs": [],
   "source": [
    "AWW_mean = WCB['Average Weekly Wage'].mean()\n",
    "print(f'The mean AWW (excluding AWW = 0) is: {AWW_mean}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We know that the New York State Average Weekly Wage (NYSAWW) for the calendar year 2023 was [$1,757.19](https://dol.ny.gov/new-york-state-average-weekly-wage-nysaww-0), and for 2019 it was [$1,450.17](https://dol.ny.gov/new-york-state-average-weekly-wage-nysaww).   \n",
    "\n",
    "> Given that we have cases with accidents occurring long before 2019, when minimum wages were significantly lower, and considering that the average AWW  (excluding values of 0) is close to these reference amounts, we will assume all non-zero AWW values are correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WUZ0eFl7aweO"
   },
   "source": [
    "#### 3.3.7. Birth Year<a class=\"anchor\" id=\"incoherences-birth\"></a>\n",
    "[Back to ToC](#toc)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J9VeoEK0aweP",
    "outputId": "2ac017ea-92e3-42ab-a4de-aa330b9f7237"
   },
   "outputs": [],
   "source": [
    "# Top 10 oldest 'Birth Year'\n",
    "top_10_oldest = WCB['Birth Year'].drop_duplicates().sort_values(ascending=True).head(10)\n",
    "print(top_10_oldest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Similarly, Birth Year = 0 is not a valid value so we replace it with NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_TwdV6VUaweP"
   },
   "outputs": [],
   "source": [
    "# Replace all instances of Birth Year == 0 with NaN\n",
    "WCB.loc[WCB['Birth Year'] == 0, 'Birth Year'] = np.nan\n",
    "\n",
    "X_test.loc[X_test['Birth Year'] == 0, 'Birth Year'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "D1j5KM40aweP",
    "outputId": "c843f580-589b-451c-af2d-ff9913314db9"
   },
   "outputs": [],
   "source": [
    "filtered_rows = WCB[(WCB['Accident Date'] > '2020-01-01') & (WCB['Birth Year'] < 1940)] #these injured workers should be at least 80 at the time of the accident\n",
    "filtered_rows.shape[0]/WCB.shape[0]*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0BCDWuhuaweP",
    "outputId": "7de4d621-9c0f-4a8c-842a-56afd561e1e7"
   },
   "source": [
    "> There is a higher percentage of injured workers over 80 years old than would be expected, as the vast majority of people are already retired for several years. However, perhaps the few individuals who continue to work at this age have a higher probability of getting injured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x1AKFzZCaweQ",
    "outputId": "8057bfe4-a50d-434e-c687-379dea1d973b"
   },
   "outputs": [],
   "source": [
    "WCB[(WCB['Birth Year'] < 1940)].shape[0]/WCB.shape[0]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WCB['Birth Year'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "keT4vdNZaweQ"
   },
   "source": [
    "> The current retirement age in New York is around [60](https://www.osc.ny.gov/retirement/members/about-benefit-reductions) years, however, in cities like New York, many older individuals are compelled to continue working because their pensions are insufficient to cover their living expenses. Given this context and the low percentage of cases where <code>Birth Year</code> is less than 1940, we will not consider these instances as errors for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q-0lYq-gaweQ"
   },
   "source": [
    "#### 3.3.8. Number of Dependents<a class=\"anchor\" id=\"incoherences-dependents\"></a>\n",
    "[Back to ToC](#toc)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xudDsL3QaweQ"
   },
   "source": [
    "> This variable is not listed in the variable description provided to us, which raises the question of whether it was a mistake or if it might not be very important for prediction. We will investigate this further during the feature selection process.    \n",
    "We assume that this variable represents the number of dependents the individual had at the time of the accident"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "gtB4pGHLaweR",
    "outputId": "22c9a16f-76dd-44d0-e067-c1e5ac16d5d3"
   },
   "outputs": [],
   "source": [
    "WCB[(WCB['Number of Dependents'] > 0) & (WCB['Age at Injury'].between(1,15))].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b0r-U_mcaweR",
    "outputId": "cc618e24-7d00-4864-c2c5-61e5a7395933"
   },
   "outputs": [],
   "source": [
    "WCB[(WCB['Number of Dependents'] > 0) & (WCB['Age at Injury'].between(1,15))& (WCB['Accident Date']<'1985-01-01')].shape[0]\n",
    "#As we know teen birthrate used to be bigger back then, but here we see that these are all recent cases which goes agains the tendency we have experieced\n",
    "#We believe this is probably a mistake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YL-ikLsoaweR"
   },
   "source": [
    "> There are 179 cases where individuals under the age of 16 (excluding  zero years) are recorded as having dependents. Given that these cases are relatively recent, we believe these may be data entry errors, specifically within the Number of Dependents field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aLOZPVPXaweS"
   },
   "outputs": [],
   "source": [
    "WCB.loc[(WCB['Number of Dependents'] > 0) & (WCB['Age at Injury'].between(1, 15)),'Number of Dependents']= 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mrdax4xeaweS"
   },
   "source": [
    "#### 3.3.9. Age at Injury<a class=\"anchor\" id=\"incoherences-age\"></a>\n",
    "[Back to ToC](#toc)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sTh5IZQzaweS",
    "outputId": "ed20f768-c21b-43a6-cb21-3d59c1853e2d"
   },
   "outputs": [],
   "source": [
    "# Top 10 youngest 'Age at Injury'\n",
    "top_10_youngest = WCB['Age at Injury'].drop_duplicates().sort_values(ascending=True).head(10)\n",
    "print(top_10_youngest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P5NpX3sUaweT",
    "outputId": "c5dadbb6-eb4a-4730-bd98-991da38cad4e"
   },
   "outputs": [],
   "source": [
    "WCB[(WCB['Age at Injury'] == 0)].shape[0]/WCB.shape[0]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SpxbFg1BaweU"
   },
   "outputs": [],
   "source": [
    "WCB.loc[WCB['Age at Injury']== 0,'Age at Injury']= np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9eOt63ROaweU"
   },
   "source": [
    "> It is not reasonable for there to be injured workers who are 0 years old, therefore we decided to set those values to NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NdruexCHaweV",
    "outputId": "da58698c-7f32-4ce7-f5a0-6a45883b3dc2"
   },
   "outputs": [],
   "source": [
    "under_14 = WCB[WCB['Age at Injury']< 14]\n",
    "percentage_under_14 = (under_14.shape[0] / WCB.shape[0]) * 100\n",
    "\n",
    "print(f\"Number of rows with age under 14: {under_14.shape[0]} ({percentage_under_14:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MAPo1stYaweW"
   },
   "source": [
    "> In New York, the minimum employment age is generally 14, but children can work at 11 for newspaper delivery and 12 on farm, [link](https://www.job-applications.com/how-old-to-work/new-york/).    \n",
    "For this reason, we will not consider these values as errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MqxJ0FZBaweY"
   },
   "source": [
    "---\n",
    "> Before checking for inconsistencies in each date variable, it's important to note that we have already verified that all dates are valid. This means that, for example, there are no cases like \"2020-02-31\". We confirmed this when converting these variables to datetime format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HUulz7_faweY"
   },
   "source": [
    "#### 3.3.10. Accident Date<a class=\"anchor\" id=\"incoherences-accident\"></a>\n",
    "[Back to ToC](#toc)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ef_T3EwNaweY"
   },
   "source": [
    "Let's check whether the \"Accident Date,\" \"Age at Injury,\" and \"Birth Year\" variables are consistent with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "id": "KWh6A6VUaweY",
    "outputId": "b85ba6ee-bd4e-4c56-d600-1665639d9b88"
   },
   "outputs": [],
   "source": [
    "# Create a new column to check the conditions\n",
    "WCB['Age_BirthYear_Check'] = (\n",
    "    (WCB['Age at Injury'] + WCB['Birth Year'] == WCB['Accident Date'].dt.year) |\n",
    "    (WCB['Age at Injury'] + WCB['Birth Year'] == WCB['Accident Date'].dt.year - 1) |\n",
    "    (WCB['Age at Injury'] + WCB['Birth Year'] == WCB['Accident Date'].dt.year + 1)  #Even though it doesn't make sense, it is also not that relevant because is just one more year and doing this we catch all the cases\n",
    ")\n",
    "\n",
    "# Filter to get rows where the conditions are not met\n",
    "rows_not_matching = WCB[~WCB['Age_BirthYear_Check']]\n",
    "WCB = WCB.drop(columns=['Age_BirthYear_Check'])\n",
    "\n",
    "# Drop rows where 'Accident Date', 'Age at Injury', or 'Birth Year' are missing\n",
    "rows_not_matching = rows_not_matching.dropna(subset=['Accident Date', 'Age at Injury', 'Birth Year'])\n",
    "rows_not_matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8dCnP22MaweY"
   },
   "source": [
    "There are no relevant inconsistencies here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RFn4axNnaweZ",
    "outputId": "c057e679-28d8-4339-c29d-27da3ade4eec"
   },
   "outputs": [],
   "source": [
    "WCB[WCB['Accident Date']<'1980-01-01'].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8uxVieGlaweZ"
   },
   "source": [
    "> We identified that some Accident Dates could be excluded as they occurred a long time ago, during a period when different processes were in place that are no longer applicable today. These cases may not be helpful in predicting the outcome of new cases. However, we have left this decision open for the outliers section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9_NFMXcGaweZ"
   },
   "source": [
    "#### 3.3.11. Assembly Date<a class=\"anchor\" id=\"incoherences-assembly\"></a>\n",
    "[Back to ToC](#toc)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_entries_acc_ass=invalid_entries(WCB,'Accident Date','Assembly Date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U-jWOu2Hawea"
   },
   "source": [
    "> It only makes sense for the Assembly Date to occur after the Accident Date. Therefore, in cases where this is not true, we will swap the two dates, making the previous Assembly Date the new Accident Date, and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 943
    },
    "id": "LkzTrbQnawea",
    "outputId": "8bd93322-d1f4-42c8-ba70-102cd8d37fe5"
   },
   "outputs": [],
   "source": [
    "# Swap Accident Date and Assembly Date for invalid entries\n",
    "WCB.loc[invalid_entries_acc_ass.index, ['Accident Date', 'Assembly Date']] = WCB.loc[invalid_entries_acc_ass.index, ['Assembly Date', 'Accident Date']].values\n",
    "\n",
    "display(WCB.loc[invalid_entries_acc_ass.index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_entries_acc_ass = invalid_entries(WCB,'Accident Date','Assembly Date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YiqJ-bvgaweb"
   },
   "source": [
    "#### 3.3.12. C-2 Date<a class=\"anchor\" id=\"incoherences-c2\"></a>\n",
    "[Back to ToC](#toc)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t3BJBtinaweb"
   },
   "source": [
    "> It only makes sense for the C-2 Date to follow the Accident Date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_entries(WCB,'Accident Date','C-2 Date').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WLTGVmhRawec"
   },
   "source": [
    "#### 3.3.13. C-3 Date<a class=\"anchor\" id=\"incoherences-c3\"></a>\n",
    "[Back to ToC](#toc)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bACfNK3eawec"
   },
   "source": [
    "> It only makes sense for the C-3 Date to follow the Accident Date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_entries(WCB, 'Accident Date', 'C-3 Date').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IynLiFQcawec"
   },
   "source": [
    "#### 3.3.14. First Hearing Date<a class=\"anchor\" id=\"incoherences-1hearing\"></a>\n",
    "[Back to ToC](#toc)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> It only makes sense for the First Hearing Date to follow the Accident Date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_entries(WCB, 'Accident Date', 'First Hearing Date').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1moYxgCGaweg"
   },
   "source": [
    "#### 3.3.15. COVID-19 Indicator<a class=\"anchor\" id=\"incoherences-covid\"></a>\n",
    "[Back to ToC](#toc)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "bI0McLawaweh",
    "outputId": "a7b29663-aba5-4f3a-ebb6-cd4313434e92"
   },
   "outputs": [],
   "source": [
    "filtered_data = WCB[(WCB['COVID-19 Indicator'] == 'Y') & (WCB['Accident Date'] < \"2020-03-01\")]\n",
    "filtered_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mcwjq3Rgaweh"
   },
   "source": [
    "> The first case of COVID-19 in New York State was on March 1, 2020. [link](https://www.investopedia.com/historical-timeline-of-covid-19-in-new-york-city-5071986)    \n",
    " However, given that we have only a few cases before this date and that the corresponding accident dates were no later than the end of 2019, we will still consider these cases as potentially associated with a COVID-19 claim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2TCJ46jVawem"
   },
   "source": [
    "## 4. Data Splitting <a class=\"anchor\" id=\"data-splitting\"></a>\n",
    "[Back to ToC](#toc)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jc0q0I0Iawen"
   },
   "outputs": [],
   "source": [
    "X = WCB.drop(columns=['Claim Injury Type'])\n",
    "y = WCB[['Claim Injury Type']]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y,\n",
    "                                                 test_size = 0.3,\n",
    "                                                 shuffle = True,\n",
    "                                                 random_state = 0,\n",
    "                                                 stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are, ', X_train.shape[0], 'rows in the train dataset')\n",
    "print('There are, ', X_val.shape[0], 'rows in the val dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sz3wuaE6aweo"
   },
   "source": [
    "## 5. Data Cleaning and Pre-processing <a class=\"anchor\" id=\"data_cleaning\"></a>\n",
    "[Back to ToC](#toc)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ASofIff5awep"
   },
   "source": [
    "### 5.1. Handling Missing Values<a class=\"anchor\" id=\"handling-missing-values\"></a>\n",
    "[Back to ToC](#toc)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data(WCB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Lets take a look at a correlation matrix. We use the package missingno for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 490
    },
    "id": "KiaZtnWEawev",
    "outputId": "95308534-5a71-461d-f0f7-beb4d4a295f5"
   },
   "outputs": [],
   "source": [
    "msno.heatmap(WCB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_A4lrEo3awev"
   },
   "source": [
    "#### 5.1.1. IME-4 Count<a class=\"anchor\" id=\"missing-ime\"></a>\n",
    "[Back to ToC](#toc)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "slpRDXhpawew"
   },
   "source": [
    "> Given that the <code>IME-4 Count</code> refers to the number of \"Independent Examiner's Report of Independent Medical Examination\" forms received, we consider that it makes sense that the missing values actually refer to none, so we'll convert the missing values to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w0N0TLcyawew"
   },
   "outputs": [],
   "source": [
    "X_train['IME-4 Count'] = X_train['IME-4 Count'].fillna(0)\n",
    "X_val['IME-4 Count'] = X_val['IME-4 Count'].fillna(0)\n",
    "X_test['IME-4 Count'] = X_test['IME-4 Count'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uJMNGb7bawew"
   },
   "source": [
    "#### 5.1.2. First Hearing Date<a class=\"anchor\" id=\"missing-first\"></a>\n",
    "[Back to ToC](#toc)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S7ags-sQawew"
   },
   "source": [
    "> As stated in the project description, a blank date simply means that the hearing hasn't happened yet. We will keep it as NaT (Not a Time) for now and later create a boolean column during feature engineering to represent this condition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4dhIY4JVawex"
   },
   "source": [
    "#### 5.1.3. C-2 and C-3 Date<a class=\"anchor\" id=\"missing-c2-c3\"></a>\n",
    "[Back to ToC](#toc)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OhRGsqGhawey"
   },
   "source": [
    "> As we saw before, it only makes sense for the <code>C-2 Date</code> to follow the <code>Accident Date</code>. Therefore, in cases where this is not true, we will assume that the <code>Accident Date</code> is correct and impute a new value for the <code>C-2 Date</code>. This new <code>C-2 Date</code> will be set to the <code>Accident Date</code> plus a certain number of days, corresponding to the median time difference (in days) between the Accident Date and C-2 Date, calculated only for X_train and cases where the <code>Accident Date</code> is earlier than the <code>C-2 Date</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_entries_c2t = invalid_entries(X_train, 'Accident Date', 'C-2 Date')\n",
    "invalid_entries_c2v = invalid_entries(X_val, 'Accident Date', 'C-2 Date')\n",
    "invalid_entries_c2te = invalid_entries(X_test, 'Accident Date', 'C-2 Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['Date Difference'] = X_train.apply(\n",
    "    lambda row: row['C-2 Date'] - row['Accident Date'] if row['C-2 Date'] > row['Accident Date'] else pd.NaT, axis=1\n",
    ")\n",
    "\n",
    "# Calculate the median of the differences in days for X_train\n",
    "median_difference_2 = X_train['Date Difference'].dropna().median().days\n",
    "X_train = X_train.drop(columns=['Date Difference'])\n",
    "\n",
    "X_train = impute_dates_with_difference(X_train, 'C-2 Date', 'Accident Date', 'Accident Date', median_difference_2)\n",
    "X_val = impute_dates_with_difference(X_val, 'C-2 Date', 'Accident Date', 'Accident Date', median_difference_2)\n",
    "X_test = impute_dates_with_difference(X_test, 'C-2 Date', 'Accident Date', 'Accident Date', median_difference_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_entries_c2t = invalid_entries(X_train, 'Accident Date', 'C-2 Date')\n",
    "invalid_entries_c2v = invalid_entries(X_val, 'Accident Date', 'C-2 Date')\n",
    "invalid_entries_c2te = invalid_entries(X_test, 'Accident Date', 'C-2 Date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gbt1JENCawez"
   },
   "source": [
    "> Now for C-3 Date:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['Date Difference'] = X_train.apply(\n",
    "    lambda row: row['C-3 Date'] - row['Accident Date'] if row['C-3 Date'] > row['Accident Date'] else pd.NaT, axis=1\n",
    ")\n",
    "\n",
    "# Calculate the median of the differences in days for X_train\n",
    "median_difference_3 = X_train['Date Difference'].dropna().median().days\n",
    "X_train = X_train.drop(columns=['Date Difference'])\n",
    "\n",
    "X_train = impute_dates_with_difference(X_train, 'C-3 Date', 'Accident Date', 'Accident Date', median_difference_3)\n",
    "X_val = impute_dates_with_difference(X_val, 'C-3 Date', 'Accident Date', 'Accident Date', median_difference_3)\n",
    "X_test = impute_dates_with_difference(X_test, 'C-3 Date', 'Accident Date', 'Accident Date', median_difference_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_entries_c2t = invalid_entries(X_train, 'Accident Date', 'C-3 Date')\n",
    "invalid_entries_c2v = invalid_entries(X_val, 'Accident Date', 'C-3 Date')\n",
    "invalid_entries_c2test = invalid_entries(X_test, 'Accident Date', 'C-3 Date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xg7kcQ2Jawe0"
   },
   "source": [
    "> <code>C-3 Date</code> refers to the date that the Employee Claim Form was received, given the large percentage of missing values, we've decided to assume that the blank values mean that there was no form received. <br> <br>\n",
    "<code>C-2 Date</code> indicates the date of receipt of the Employer's Report of Work-Related Injury/Illness. Similarly, we will assume that any missing values reflect the absence of a report, and we will keep these entries as NaT (Not a Time)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ZfE5XThawe5"
   },
   "source": [
    "#### 5.1.4. Birth Year<a class=\"anchor\" id=\"missing-birth\"></a>\n",
    "[Back to ToC](#toc)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eUy2kIaXawe5"
   },
   "source": [
    "> There are cases where we can calculate the <code>Birth Year</code> deterministically, assuming a margin of error of 1 year due to not knowing whether the person had their birthday that year at the time of the accident. However, this should not significantly affect the overall analysis. <br>\n",
    "The rest that is missing we will fill with the median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uhUgY5A0awe6",
    "outputId": "5cce3137-be00-4885-f15f-8a3b5415e130"
   },
   "outputs": [],
   "source": [
    "WCB[(WCB['Birth Year'].isna()) & (WCB['Accident Date'].notna()) & (WCB['Age at Injury'].notna())].shape[0]\n",
    "# Birth Year is missing: 54155 rows\n",
    "# Birth Year is missing and Accident Date not missing: 53651 rows\n",
    "# Birth Year is missing, Accident Date and Age at Injury not missing: 51971 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KBHstArSawe6"
   },
   "outputs": [],
   "source": [
    "# Rows to replace\n",
    "filtered_rows_train =X_train[(X_train['Birth Year'].isna()) & (X_train['Accident Date'].notna()) & (X_train['Age at Injury'].notna())]\n",
    "\n",
    "X_train.loc[filtered_rows_train.index, 'Birth Year'] = (\n",
    "    X_train.loc[filtered_rows_train.index, 'Accident Date'].dt.year - X_train.loc[filtered_rows_train.index, 'Age at Injury']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gWv-P4GAawe6"
   },
   "outputs": [],
   "source": [
    "# Rows to replace for validation set\n",
    "filtered_rows_val= X_val[(X_val['Birth Year'].isna()) & (X_val['Accident Date'].notna()) & (X_val['Age at Injury'].notna())]\n",
    "\n",
    "X_val.loc[filtered_rows_val.index, 'Birth Year'] = (\n",
    "    X_val.loc[filtered_rows_val.index, 'Accident Date'].dt.year - X_val.loc[filtered_rows_val.index, 'Age at Injury']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rows to replace for test set\n",
    "filtered_rows_test= X_test[(X_test['Birth Year'].isna()) & (X_test['Accident Date'].notna()) & (X_test['Age at Injury'].notna())]\n",
    "\n",
    "X_test.loc[filtered_rows_test.index, 'Birth Year'] = (\n",
    "    X_test.loc[filtered_rows_test.index, 'Accident Date'].dt.year - X_test.loc[filtered_rows_test.index, 'Age at Injury']\n",
    ")\n",
    "\n",
    "# put the median of the birth year in the missing values (from X_train) redundant \n",
    "# X_test['Birth Year'] = X_test['Birth Year'].fillna(X_train['Birth Year'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b_UZSi8Fawe6",
    "outputId": "fe442682-5984-4eef-d101-29e2a2f5d0c4"
   },
   "outputs": [],
   "source": [
    "display(X_train[(X_train['Birth Year'].isna())].shape[0])\n",
    "display(X_val[(X_val['Birth Year'].isna())].shape[0])\n",
    "display(X_test[(X_test['Birth Year'].isna())].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5I1Omc13awe6",
    "outputId": "962dcf6c-b0ea-4635-c4f9-af5eb2dfa577"
   },
   "outputs": [],
   "source": [
    "# Calculate the median of Birth Year\n",
    "birth_median_train= X_train['Birth Year'].median()\n",
    "print(f'The median Birth Year in training data is: {birth_median_train}')\n",
    "\n",
    "#Impute missing values\n",
    "X_train.loc[X_train['Birth Year'].isna(),'Birth Year']= birth_median_train\n",
    "X_val.loc[X_val['Birth Year'].isna(), 'Birth Year'] = birth_median_train\n",
    "X_test.loc[X_test['Birth Year'].isna(), 'Birth Year'] = birth_median_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The number of null birth years in X_train: {X_train[(X_train[\"Birth Year\"].isna())].shape[0]}')\n",
    "print(f'The number of null birth years in X_val: {X_val[(X_val[\"Birth Year\"].isna())].shape[0]}')\n",
    "print(f'The number of null birth years in X_test: {X_test[(X_test[\"Birth Year\"].isna())].shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KrFbP2w6awe7"
   },
   "source": [
    "#### 5.1.5. Medical Fee Region<a class=\"anchor\" id=\"missing-medical\"></a>\n",
    "[Back to ToC](#toc)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JovcQjbUawe7"
   },
   "source": [
    "> We believe that \"Unknown\" indicates a failure to fill in the region. For this reason, we will impute the missing values using a proportional allocation based on the distribution of known values within the <code>Medical Fee Region</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract proportions as a dictionary to ensure correct association\n",
    "prop = X_train['Medical Fee Region'].value_counts(normalize=True)\n",
    "regions = prop.index.tolist()  \n",
    "proportions_train = prop.values \n",
    "\n",
    "print(\"Proportions array:\", proportions_train)\n",
    "print(\"Regions list:\", regions)\n",
    "\n",
    "# Impute missing values in X_train and X_val\n",
    "X_train = impute_prop(regions, proportions_train, X_train, 'Medical Fee Region')\n",
    "X_val = impute_prop(regions, proportions_train, X_val, 'Medical Fee Region')\n",
    "\n",
    "# Impute missing values in X_test | redundant as we are dropping the region column\n",
    "X_test = impute_prop(regions, proportions_train, X_test, 'Medical Fee Region')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "75GSn1neawe9"
   },
   "source": [
    "#### 5.1.6. Zip Code<a class=\"anchor\" id=\"missing-zip\"></a>\n",
    "[Back to ToC](#toc)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "njari91Jawe9"
   },
   "source": [
    "> We don’t anticipate that this variable will be highly relevant to the model. In the absence of a better option, we will, for now, impute the missing values with \"Unknown\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "35UmXZDHawe-"
   },
   "outputs": [],
   "source": [
    "X_train.fillna({'Zip Code':'UNKNOWN'}, inplace=True)\n",
    "X_val.fillna({'Zip Code':'UNKNOWN'}, inplace=True)\n",
    "\n",
    "# Redundant as we are dropping the zip column\n",
    "X_test.fillna({'Zip Code':'UNKNOWN'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['Zip Code'].nunique(), X_train['Zip Code'].notna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b6pZd_qnawfD"
   },
   "source": [
    "#### 5.1.7. WCIO Part Of Body Code & Description<a class=\"anchor\" id=\"missing-part-body\"></a>\n",
    "[Back to ToC](#toc)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gT_ZK3JcawfD"
   },
   "source": [
    "> We already know from the heatmap and the calculations below that the cases with missing <code>Part of Body Code</code> correspond to the same instances where the <code>WCIO Part of Body Description</code> is also absent. Additionally, in 14,751 cases, both the <code>WCIO Cause of Injury Code</code> and its description, as well as the <code>WCIO Part Of Body Code</code> and its description, and the <code>WCIO Nature of Injury Code</code> and its description are missing. For cases with missing values across these categories, we will impute the code as 100 and the description as \"Unknown.\" We suspect that these cases may represent instances where no workplace accident occurred or where there is insufficient information regarding these claims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EaOpZS83awfD",
    "outputId": "132d9f7c-8343-48e8-8a79-50d48b1fad7e"
   },
   "outputs": [],
   "source": [
    "WCB[(WCB['WCIO Cause of Injury Description'].isna()) & (WCB['WCIO Nature of Injury Description'].isna()) & (WCB['WCIO Part Of Body Code'].isna())].shape[0] \n",
    "\n",
    "#17069 rows where WCIO Part Of Body Description is missing\n",
    "#17069 rows where WCIO Part Of Body Code is missing\n",
    "#17069 rows where WCIO Part Of Body Code and WCIO Part Of Body Description are missing\n",
    "\n",
    "#15643 rows where WCIO Nature of Injury Description is missing\n",
    "#15643 rows where WCIO Nature of Injury Code is missing\n",
    "#15643 rows where WCIO Nature of Injury Code and WCIO Nature of Injury Description are missing\n",
    "#14752 rows where WCIO Part Of Body Code and Description and WCIO Nature of Injury Code and Description are missing\n",
    "\n",
    "#15626 rows where WCIO Cause of Injury Code is missing\n",
    "#15626 rows where WCIO Cause of Injury Description is missing\n",
    "#15626 rows where WCIO Cause of Injury Description and WCIO Cause of Injury Code are missing\n",
    "#15580 rows where WCIO Cause of Injury Code and Description and WCIO Nature of Injury Code and Description are missing\n",
    "#14751 rows where WCIO Cause of Injury Code and Description and WCIO Part Of Body Code and Description are missing\n",
    "#14751 rows where WCIO Cause of Injury Code and Description and WCIO Part Of Body Code and Description and WCIO Nature of Injury Code and Description are missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "riLPu4TMawfE"
   },
   "outputs": [],
   "source": [
    "default_value_code = 100\n",
    "default_value_desc = \"Unknown\"\n",
    "\n",
    "# Columns with codes and descriptions\n",
    "columns_code = ['WCIO Part Of Body Code', 'WCIO Cause of Injury Code', 'WCIO Nature of Injury Code']\n",
    "columns_desc = ['WCIO Part Of Body Description', 'WCIO Cause of Injury Description', 'WCIO Nature of Injury Description']\n",
    "\n",
    "# check if all thse columns are NaN for X_train and X_val\n",
    "missing_condition_train = X_train[columns_code + columns_desc].isna().all(axis=1)\n",
    "missing_condition_val = X_val[columns_code + columns_desc].isna().all(axis=1)\n",
    "missing_condition_test = X_test[columns_code + columns_desc].isna().all(axis=1)\n",
    "\n",
    "# Apply default values to X_train and X_val where conditions are met\n",
    "for col in columns_code:\n",
    "    X_train.loc[missing_condition_train, col] = default_value_code\n",
    "    X_val.loc[missing_condition_val, col] = default_value_code\n",
    "    X_test.loc[missing_condition_test, col] = default_value_code\n",
    "\n",
    "for col in columns_desc:\n",
    "    X_train.loc[missing_condition_train, col] = default_value_desc\n",
    "    X_val.loc[missing_condition_val, col] = default_value_desc\n",
    "    X_test.loc[missing_condition_test, col] = default_value_desc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EIU9bCFnawfE"
   },
   "source": [
    "> We still have some Nan to impute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8K59of-fawfG",
    "outputId": "27713e48-6958-491d-c3e5-258dddd5451c"
   },
   "outputs": [],
   "source": [
    "X_train[(X_train['WCIO Part Of Body Description'].isna()) & (X_train['WCIO Cause of Injury Description'].notna())].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q7PqpOF-awfG"
   },
   "source": [
    "> For the remaining cases where both the <code>WCIO Part Of Body Code</code> and its description are missing, we believe there may have been a system error that resulted in the loss of these values. Therefore, we will impute the most common <code>Part Of Body Code</code> corresponding to the respective <code>Cause of Injury Code</code> for each of these cases. We do this because we believe that the two are correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = impute_with(X_train, 'WCIO Part Of Body Code', 'WCIO Cause of Injury Code', metric='mode')\n",
    "X_val = impute_with(X_val, 'WCIO Part Of Body Code', 'WCIO Cause of Injury Code',reference_df=X_train, metric='mode')\n",
    "X_test = impute_with(X_test, 'WCIO Part Of Body Code', 'WCIO Cause of Injury Code',reference_df=X_train, metric='mode')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rmbjo4IhawfI"
   },
   "source": [
    "> Now that we have filled in all the WCIO Part Of Body Code values, it becomes straightforward to populate the WCIO Part Of Body Description as well. By establishing a mapping between the codes and their corresponding descriptions, we can efficiently fill in any missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame with unique WCIO Part Of Body Codes and their descriptions (without missing descriptions)\n",
    "body_code_description_train = X_train[['WCIO Part Of Body Code', 'WCIO Part Of Body Description']].drop_duplicates()\n",
    "body_code_description_train = body_code_description_train[body_code_description_train['WCIO Part Of Body Description'].notna()]\n",
    "\n",
    "# Dictionary to map WCIO Part Of Body Code to its description\n",
    "description_dict = dict(zip(body_code_description_train['WCIO Part Of Body Code'], body_code_description_train['WCIO Part Of Body Description']))\n",
    "\n",
    "# Fill missing descriptions in X_train, X_val, X_test using the mapping | This is redundant as we dropo the description column later\n",
    "X_train['WCIO Part Of Body Description'] = X_train['WCIO Part Of Body Description'].fillna(X_train['WCIO Part Of Body Code'].map(description_dict))\n",
    "X_val['WCIO Part Of Body Description'] = X_val['WCIO Part Of Body Description'].fillna(X_val['WCIO Part Of Body Code'].map(description_dict))\n",
    "X_test['WCIO Part Of Body Description'] = X_test['WCIO Part Of Body Description'].fillna(X_test['WCIO Part Of Body Code'].map(description_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all values are filled\n",
    "X_train['WCIO Part Of Body Code'].isna().sum(), X_val['WCIO Part Of Body Code'].isna().sum(), X_test['WCIO Part Of Body Code'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zlsHFHUJawfJ"
   },
   "source": [
    "#### 5.1.8. WCIO Nature of Injury Code & Description<a class=\"anchor\" id=\"missing-nature\"></a>\n",
    "[Back to ToC](#toc)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For cases where both the <code>WCIO Nature of Injury Code</code> and its description are missing, we believe there may have been a system error that resulted in the loss of these values. Therefore, we will impute the most common <code>WCIO Nature of Injury Code</code> corresponding to the respective <code>WCIO Part Of Body Code</code> for each of these cases. We do this because we believe that the two are correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F40O3UzuawfK",
    "outputId": "fe695c40-10f8-452a-a406-058f60037846"
   },
   "outputs": [],
   "source": [
    "X_train[(X_train['WCIO Nature of Injury Description'].isna()) & (X_train['WCIO Nature of Injury Code'].isna())].shape[0]\n",
    "#605 rows in X_train where WCIO Nature of Injury Code is missing\n",
    "#605 rows in X_train where WCIO Nature of Injury Description is missing\n",
    "#605 rows in X_train where WCIO Nature of Injury Description and WCIO Nature of Injury Code are missing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = impute_with(X_train, 'WCIO Nature of Injury Code', 'WCIO Part Of Body Code', metric='mode')\n",
    "X_val = impute_with(X_val, 'WCIO Nature of Injury Code', 'WCIO Part Of Body Code',reference_df=X_train, metric='mode')\n",
    "X_test = impute_with(X_test, 'WCIO Nature of Injury Code', 'WCIO Part Of Body Code',reference_df=X_train, metric='mode')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now that we have filled in all the WCIO Nature of Injury Code values, it becomes straightforward to populate the WCIO Nature of Injury Description as well. By establishing a mapping between the codes and their corresponding descriptions, we can efficiently fill in any missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_beo1oylawfL"
   },
   "outputs": [],
   "source": [
    "# DataFrame with unique WCIO Nature of Injury Codes and their descriptions (in X_train and without missing descriptions)\n",
    "nature_code_description_train = X_train[['WCIO Nature of Injury Code', 'WCIO Nature of Injury Description']].drop_duplicates()\n",
    "nature_code_description_train = nature_code_description_train[nature_code_description_train['WCIO Nature of Injury Description'].notna()]\n",
    "\n",
    "# Dictionary to map WCIO Nature of Injury Code to its description\n",
    "description_dict = dict(zip(nature_code_description_train['WCIO Nature of Injury Code'], nature_code_description_train['WCIO Nature of Injury Description']))\n",
    "\n",
    "# Fill missing descriptions in the original DataFrame using the mapping | redundant as we drop the values later \n",
    "X_train['WCIO Nature of Injury Description'] = X_train['WCIO Nature of Injury Description'].fillna(X_train['WCIO Nature of Injury Code'].map(description_dict))\n",
    "X_val['WCIO Nature of Injury Description'] = X_val['WCIO Nature of Injury Description'].fillna(X_val['WCIO Nature of Injury Code'].map(description_dict))\n",
    "X_test['WCIO Nature of Injury Description'] = X_test['WCIO Nature of Injury Description'].fillna(X_test['WCIO Nature of Injury Code'].map(description_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all values are filled\n",
    "X_train['WCIO Nature of Injury Description'].isna().sum(), X_val['WCIO Nature of Injury Description'].isna().sum(), X_test['WCIO Nature of Injury Description'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X6ImGZrEawfL"
   },
   "source": [
    "#### 5.1.9. WCIO Cause of Injury Code & Description<a class=\"anchor\" id=\"missing-cause\"></a>\n",
    "[Back to ToC](#toc)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For cases where both the <code>WCIO Cause of Injury Code</code> and its description are missing, we believe there may have been a system error that resulted in the loss of these values. Therefore, we will impute the most common <code>WCIO Cause of Injury Code</code> corresponding to the respective <code>WCIO Part Of Body Code</code> for each of these cases. We do this because we believe that the two are correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NMspAvxjawfM",
    "outputId": "789899ee-6dca-4858-a72c-7bf075206f6d"
   },
   "outputs": [],
   "source": [
    "X_train[(X_train['WCIO Cause of Injury Code'].isna()) & (X_train['WCIO Cause of Injury Description'].isna()) ].shape[0]\n",
    "#593 rows in X_train where WCIO Cause of Injury Code is missing\n",
    "#593 rows in X_train where WCIO Cause of Injury Description is missing\n",
    "#593 rows in X_train where WCIO Cause of Injury Description and WCIO Cause of Injury Code are missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = impute_with(X_train, 'WCIO Cause of Injury Code', 'WCIO Part Of Body Code', metric='mode')\n",
    "X_val = impute_with(X_val, 'WCIO Cause of Injury Code', 'WCIO Part Of Body Code',reference_df=X_train, metric='mode')\n",
    "X_test = impute_with(X_test, 'WCIO Cause of Injury Code', 'WCIO Part Of Body Code',reference_df=X_train, metric='mode')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > Now that we have filled in all the WCIO Cause of Injury Code values, it becomes straightforward to populate the WCIO Cause of Injury Description as well. By establishing a mapping between the codes and their corresponding descriptions, we can efficiently fill in any missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Il9o7HOoawfN"
   },
   "outputs": [],
   "source": [
    "# DataFrame with unique WCIO Cause of Injury Codes and their descriptions (in X_train and without missing descriptions)\n",
    "cause_code_description_train = X_train[['WCIO Cause of Injury Code', 'WCIO Cause of Injury Description']].drop_duplicates()\n",
    "cause_code_description_train = cause_code_description_train[cause_code_description_train['WCIO Cause of Injury Description'].notna()]\n",
    "\n",
    "# Dictionary to map WCIO Cause of Injury Code to its description (based on X_train)\n",
    "description_dict_train = dict(zip(cause_code_description_train['WCIO Cause of Injury Code'], cause_code_description_train['WCIO Cause of Injury Description']))\n",
    "\n",
    "# Fill missing descriptions in X_train and X_val using the mapping\n",
    "X_train['WCIO Cause of Injury Description'] = X_train['WCIO Cause of Injury Description'].fillna(X_train['WCIO Cause of Injury Code'].map(description_dict_train))\n",
    "X_val['WCIO Cause of Injury Description'] = X_val['WCIO Cause of Injury Description'].fillna(X_val['WCIO Cause of Injury Code'].map(description_dict_train))\n",
    "X_test['WCIO Cause of Injury Description'] = X_test['WCIO Cause of Injury Description'].fillna(X_test['WCIO Cause of Injury Code'].map(description_dict_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all values are filled\n",
    "X_train['WCIO Cause of Injury Code'].isna().sum(), X_val['WCIO Cause of Injury Code'].isna().sum(), X_test['WCIO Cause of Injury Code'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nyk5fBX6awfN"
   },
   "source": [
    "#### 5.1.10. Industry Code & Description<a class=\"anchor\" id=\"missing-industry\"></a>\n",
    "[Back to ToC](#toc)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We will impute the missing values using a proportional allocation based on the distribution of known values within the <code>Industry Code</code>.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7skpKW1YawfO",
    "outputId": "636ec776-2b95-49b4-ee13-815cf26a92e4"
   },
   "outputs": [],
   "source": [
    "X_train[(X_train['Industry Code Description'].isna()) & (X_train['Industry Code'].isna())].shape[0]\n",
    "\n",
    "#6904 rows in X_train where Industry Code is missing\n",
    "#6904 rows in X_train where Industry Code Description is missing\n",
    "#6904 rows in X_train where Industry Code and Industry CodeDescription are missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g6Un1ODeawfQ"
   },
   "outputs": [],
   "source": [
    "# Extract proportions for the top 10 Industry Code values only\n",
    "top_industries = X_train['Industry Code'].value_counts(normalize=True).head(10)\n",
    "industries = top_industries.index.tolist() \n",
    "proportions_train = top_industries.values   \n",
    "proportions_train = proportions_train / proportions_train.sum()\n",
    "\n",
    "# Print debug information\n",
    "print(\"Top 10 Proportions array:\", proportions_train)\n",
    "print(\"Top 10 Industry Codes list:\", industries)\n",
    "\n",
    "# Impute missing values in X_train and X_val\n",
    "X_train = impute_prop(industries, proportions_train, X_train, 'Industry Code')\n",
    "X_val = impute_prop(industries, proportions_train, X_val, 'Industry Code')\n",
    "X_test = impute_prop(industries, proportions_train, X_test, 'Industry Code')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > Now that we have filled in all the Industry Codes, it becomes straightforward to populate the Industry Code Description as well. By establishing a mapping between the codes and their corresponding descriptions, we can efficiently fill in any missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P6A_Qr86awfR"
   },
   "outputs": [],
   "source": [
    "# DataFrame with unique Industry Codes and their descriptions (in X_train and without missing descriptions)\n",
    "industry_code_description_train = X_train[['Industry Code', 'Industry Code Description']].drop_duplicates()\n",
    "industry_code_description_train = industry_code_description_train[industry_code_description_train['Industry Code Description'].notna()]\n",
    "\n",
    "# Dictionary to map Industry Code to its description\n",
    "description_dict_train = dict(zip(industry_code_description_train['Industry Code'], industry_code_description_train['Industry Code Description']))\n",
    "\n",
    "# Fill missing descriptions in X_train using the mapping\n",
    "X_train['Industry Code Description'] = X_train['Industry Code Description'].fillna(X_train['Industry Code'].map(description_dict_train))\n",
    "X_val['Industry Code Description'] = X_val['Industry Code Description'].fillna(X_val['Industry Code'].map(description_dict_train))\n",
    "X_test['Industry Code Description'] = X_test['Industry Code Description'].fillna(X_test['Industry Code'].map(description_dict_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all values are filled\n",
    "X_train['Industry Code'].isna().sum(), X_val['Industry Code'].isna().sum(), X_test['Industry Code'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T0TqaYHiboBZ"
   },
   "source": [
    "#### 5.1.11. Average Weekly Wage<a class=\"anchor\" id=\"missing-avg\"></a>\n",
    "[Back to ToC](#toc)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bVa6td1PboBa"
   },
   "source": [
    "> For cases where the <code>Average Weekly Wage</code> is missing, we will impute the missing values using the mean of the <code>Average Weekly Wage</code> within each <code>Industry Code</code> group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "id": "t6KrbcWFboBa",
    "outputId": "7ab241b4-ef22-4efa-d62a-f7fb8edb08a0"
   },
   "outputs": [],
   "source": [
    "WCB['Average Weekly Wage'].describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Have a lot of zero values in the Average Weekly Wage for the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test['Average Weekly Wage'].describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill zero with na \n",
    "X_test['Average Weekly Wage'] = X_test['Average Weekly Wage'].replace(0, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zq4zSgnZboBb"
   },
   "outputs": [],
   "source": [
    "# impute for training dataset\n",
    "X_train['Average Weekly Wage'] = X_train.groupby('Industry Code', observed=False)['Average Weekly Wage'].transform(lambda x: x.fillna(x.mean()))\n",
    "# calculate mean for each industry code in training dataset\n",
    "industry_mean_train = X_train.groupby('Industry Code')['Average Weekly Wage'].mean()\n",
    "\n",
    "# impute for validation dataset\n",
    "X_val['Average Weekly Wage'] = X_val.groupby('Industry Code')['Average Weekly Wage'].transform(\n",
    "    lambda x: x.fillna(industry_mean_train.get(x.name, float('nan')))\n",
    ")\n",
    "\n",
    "# impute for test dataset\n",
    "X_test['Average Weekly Wage'] = X_test.groupby('Industry Code')['Average Weekly Wage'].transform(\n",
    "    lambda x: x.fillna(industry_mean_train.get(x.name, float('nan')))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "id": "Aw0z0T_nboBc",
    "outputId": "f15789eb-2c40-4b2a-e0e2-e4c885d68664"
   },
   "outputs": [],
   "source": [
    "#AWW after imputing\n",
    "X_train['Average Weekly Wage'].describe().round(2), X_val['Average Weekly Wage'].describe().round(2), X_test['Average Weekly Wage'].describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u1vMWbEBCjpi"
   },
   "source": [
    "#### 5.1.12. Accident Date<a class=\"anchor\" id=\"missing-acc\"></a>\n",
    "[Back to ToC](#toc)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We are imputing missing values in the Accident Date column by using the Assembly Date as a reference. Specifically, we calculate the difference in days between the Assembly Date and Accident Date for rows where the Assembly Date is later than the Accident Date. The calculated date difference is used as a reference to impute missing Accident Date values, ensuring consistency between the two dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "id": "zMISsN7GCjpj",
    "outputId": "0d39490c-e817-4da1-ebeb-9e8f897b320e"
   },
   "outputs": [],
   "source": [
    "X_train[(X_train['Accident Date'].isna()) & (X_train['Assembly Date'].notna())].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Difference between Assembly Date and Accident Date for X_train (only for valid cases)\n",
    "X_train['Date Difference'] = X_train.apply(\n",
    "    lambda row: row['Assembly Date'] - row['Accident Date'] if row['Assembly Date'] > row['Accident Date'] else pd.NaT, axis=1\n",
    ")\n",
    "\n",
    "# Calculate the median of the differences in days for X_train\n",
    "median_difference_train = X_train['Date Difference'].dropna().median().days\n",
    "\n",
    "# Condition to impute missing Accident Dates in X_train and X_val\n",
    "condition_train = (X_train['Accident Date'].isna()) & (X_train['Assembly Date'].notna())\n",
    "condition_val = (X_val['Accident Date'].isna()) & (X_val['Assembly Date'].notna())\n",
    "condition_test = (X_test['Accident Date'].isna()) & (X_test['Assembly Date'].notna())\n",
    "\n",
    "# Impute Accident Date for X_train\n",
    "X_train.loc[condition_train, 'Accident Date'] = X_train.loc[condition_train, 'Assembly Date'] - pd.Timedelta(days=median_difference_train)\n",
    "X_val.loc[condition_val, 'Accident Date'] = X_val.loc[condition_val, 'Assembly Date'] - pd.Timedelta(days=median_difference_train)\n",
    "X_test.loc[condition_test, 'Accident Date'] = X_test.loc[condition_test, 'Assembly Date'] - pd.Timedelta(days=median_difference_train)\n",
    "\n",
    "X_train = X_train.drop(columns=['Date Difference'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if all values are filled\n",
    "X_train['Accident Date'].isna().sum(), X_val['Accident Date'].isna().sum(), X_test['Accident Date'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DoEt3zFUawfR"
   },
   "source": [
    "#### 5.1.13. Age at Injury<a class=\"anchor\" id=\"missing-age\"></a>\n",
    "[Back to ToC](#toc)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9cPAEJu1awfR"
   },
   "source": [
    "> There are cases where we can calculate the Age at Injury deterministically, assuming a margin of error of 1 year due to not knowing whether the person had their birthday that year at the time of the accident. However, this should not significantly affect the overall analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cVz22tboawfS",
    "outputId": "2f57a7b4-2591-441c-b5db-b2bc23a9bbbf"
   },
   "outputs": [],
   "source": [
    "X_train[(X_train['Age at Injury'].isna()) & (X_train['Accident Date'].notna())].shape[0]\n",
    "# Age at Injury is missing and Accident Date not missing (in X_train): 3775 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UFV-42CJawfS"
   },
   "outputs": [],
   "source": [
    "# Rows to replace\n",
    "filtered_rows_train = X_train[(X_train['Age at Injury'].isna()) & (X_train['Accident Date'].notna())]\n",
    "\n",
    "# Calculate and replace Age at Injury in X_train\n",
    "X_train.loc[filtered_rows_train.index, 'Age at Injury'] = (\n",
    "    X_train.loc[filtered_rows_train.index, 'Accident Date'].dt.year - X_train.loc[filtered_rows_train.index, 'Birth Year']\n",
    ")\n",
    "\n",
    "#Repeat the process for X_val\n",
    "filtered_rows_val = X_val[(X_val['Age at Injury'].isna()) & (X_val['Accident Date'].notna())]\n",
    "\n",
    "X_val.loc[filtered_rows_val.index, 'Age at Injury'] = (\n",
    "    X_val.loc[filtered_rows_val.index, 'Accident Date'].dt.year - X_val.loc[filtered_rows_val.index, 'Birth Year']\n",
    ")\n",
    "\n",
    "#Repeat the process for X_test\n",
    "filtered_rows_test = X_test[(X_test['Age at Injury'].isna()) & (X_test['Accident Date'].notna())]\n",
    "\n",
    "X_test.loc[filtered_rows_test.index, 'Age at Injury'] = (\n",
    "    X_test.loc[filtered_rows_test.index, 'Accident Date'].dt.year - X_test.loc[filtered_rows_test.index, 'Birth Year']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the difference in days for X_train\n",
    "X_train['Date Difference'] = (X_train['Assembly Date'] - X_train['Accident Date']).dt.days\n",
    "\n",
    "# Compute the median and mean of the difference in days for X_train\n",
    "median_diff_train = X_train['Date Difference'].median()\n",
    "mean_diff_train = X_train['Date Difference'].mean()\n",
    "\n",
    "X_train = X_train.drop(columns=['Date Difference'])\n",
    "median_diff_train, mean_diff_train,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9CxRk5TeawfT"
   },
   "source": [
    "> Since both the mean and median of the number of days between the Assembly Date and Accident Date are well below one year, we will use the year from the Assembly Date along with the Birth Year to calculate any remaining missing values for Age at Injury."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1tDujGW7awfT"
   },
   "outputs": [],
   "source": [
    "# Rows to replace in X_train\n",
    "filtered_rows_train = X_train[(X_train['Age at Injury'].isna()) & (X_train['Accident Date'].isna()) & (X_train['Assembly Date'].notna())]\n",
    "\n",
    "# Calculate and replace Age at Injury in X_train\n",
    "X_train.loc[filtered_rows_train.index, 'Age at Injury'] = (\n",
    "    X_train.loc[filtered_rows_train.index, 'Assembly Date'].dt.year - X_train.loc[filtered_rows_train.index, 'Birth Year']\n",
    ")\n",
    "\n",
    "# Doing the same for X_val\n",
    "filtered_rows_val = X_val[(X_val['Age at Injury'].isna()) & (X_val['Accident Date'].isna()) & (X_val['Assembly Date'].notna())]\n",
    "\n",
    "X_val.loc[filtered_rows_val.index, 'Age at Injury'] = (\n",
    "    X_val.loc[filtered_rows_val.index, 'Assembly Date'].dt.year - X_val.loc[filtered_rows_val.index, 'Birth Year']\n",
    ")\n",
    "\n",
    "# Doing the same for X_test \n",
    "filtered_rows_test = X_test[(X_test['Age at Injury'].isna()) & (X_test['Accident Date'].isna()) & (X_test['Assembly Date'].notna())]\n",
    "\n",
    "X_test.loc[filtered_rows_test.index, 'Age at Injury'] = (\n",
    "    X_test.loc[filtered_rows_test.index, 'Assembly Date'].dt.year - X_test.loc[filtered_rows_test.index, 'Birth Year']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if all values are filled\n",
    "X_train['Age at Injury'].isna().sum(), X_val['Age at Injury'].isna().sum(), X_test['Age at Injury'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if all values are filled\n",
    "X_train['Age at Injury'].isna().sum(), X_val['Age at Injury'].isna().sum(), X_test['Age at Injury'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y8z0LuSxawfV"
   },
   "source": [
    "#### 5.1.14. Gender<a class=\"anchor\" id=\"missing-gender\"></a>\n",
    "[Back to ToC](#toc)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "36mD5NwMawfW"
   },
   "source": [
    "> Given that the percentage of 'Gender = X' is very low, we have decided to treat this case as missing. For all missing values in the <code>Gender</code> column, we will impute the most common <code>Gender</code> corresponding to the respective <code>Industry Code</code>, based on the idea that certain industries are predominantly associated with a specific gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_percentage = X_train['Gender'].value_counts(normalize=True) * 100\n",
    "gender_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 263
    },
    "id": "E3cwhh35awfa",
    "outputId": "6bdf91c1-15fc-4e2f-ebb7-7e99ec311206"
   },
   "outputs": [],
   "source": [
    "# impute with mode based on the Industry Code\n",
    "X_train = impute_with(X_train, 'Gender', 'Industry Code', ['X'], metric = 'mode')\n",
    "\n",
    "# Use reference DataFrame X_train to impute missing values in X_val to avoid data leakage\n",
    "X_val = impute_with(X_val, 'Gender', 'Industry Code', ['X'], reference_df=X_train, metric = 'mode')\n",
    "\n",
    "# Use reference DataFrame X_train to impute missing values in X_test to avoid data leakage\n",
    "X_test = impute_with(X_test, 'Gender', 'Industry Code', ['U','X'], reference_df = X_train, metric = 'mode')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AynR3P7uawfh"
   },
   "source": [
    "#### 5.1.15. Carrier Type<a class=\"anchor\" id=\"missing-carrier\"></a>\n",
    "[Back to ToC](#toc)<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D8B23cfOawfh"
   },
   "source": [
    "> For the missing values in the <code>Carrier Type</code> variable, we decided to impute the most frequent <code>Carrier Type</code> based on the corresponding <code>Industry Code</code>, as we believe that the area in which the worker operates impacts the <code>Carrier Type</code>, especially in the USA.    \n",
    "Here, we can also see that there are <code>Carrier Name</code> entries labeled *** CARRIER UNDETERMINED ***. However, we will not take any action regarding these because we believe that, in a way, \"Carrier Type\" provides us with the same information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 504
    },
    "id": "58xJU8lMawfi",
    "outputId": "e812c08f-b48b-41dc-9944-c405e7bd898e"
   },
   "outputs": [],
   "source": [
    "X_train[(X_train['Carrier Type'].isna())].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E4A6uKB7awfi",
    "outputId": "b064a757-6291-4588-dfbc-e2bd158dd32f"
   },
   "outputs": [],
   "source": [
    "X_train = impute_with(X_train, target_column='Carrier Type',group_column='Industry Code', metric='mode')\n",
    "X_val = impute_with(X_val, target_column='Carrier Type', group_column='Industry Code',reference_df=X_train, metric='mode')\n",
    "X_test = impute_with(X_test, target_column='Carrier Type', group_column='Industry Code',reference_df=X_train, metric='mode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if all values are filled\n",
    "X_train['Carrier Type'].isna().sum(), X_val['Carrier Type'].isna().sum(), X_test['Carrier Type'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E1l7-vuWawfj"
   },
   "source": [
    "#### 5.1.16. County of Injury<a class=\"anchor\" id=\"missing-county\"></a>\n",
    "[Back to ToC](#toc)<br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m6ljP_u2awfj"
   },
   "source": [
    "> For the missing values in the <code>County of Injury</code> variable, we decided to impute the most frequent <code>County of Injury</code> based on the corresponding <code>District Name</code>, as the <code>District Name</code> represents the WCB district office responsible for overseeing claims in that specific region or area of the state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NlUDmKHDawfj",
    "outputId": "020c1113-9512-4e12-abaf-eaa338a57cf8"
   },
   "outputs": [],
   "source": [
    "X_train = impute_with(df=X_train, target_column='County of Injury', group_column='District Name', metric='mode')\n",
    "X_val= impute_with(df=X_val, target_column='County of Injury', group_column='District Name',reference_df=X_train, metric='mode')\n",
    "X_test= impute_with(df=X_test, target_column='County of Injury', group_column='District Name',reference_df=X_train, metric='mode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if all values are filled\n",
    "X_train['County of Injury'].isna().sum(), X_val['County of Injury'].isna().sum(), X_test['County of Injury'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xc5-8auNawfk"
   },
   "source": [
    "#### 5.1.17. Alternative Dispute Resolution<a class=\"anchor\" id=\"missing-adr\"></a>\n",
    "[Back to ToC](#toc)<br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JXr58P-dawfl"
   },
   "source": [
    "> * We will impute theses missing values using a proportional allocation based on the distribution of known values within the <code>Alternative Dispute Resolution</code>.\n",
    "> * We will use another approach in the second project phase but because its only a few values its fine. \n",
    "> * For the __X_test there is no missing value__. Therefore we dont need to impute. However, in case of new data that needs to be checked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y8F0IJ1eawfl",
    "outputId": "d81b2469-0f6e-4b2b-fb7d-9da1cafb6d9f"
   },
   "outputs": [],
   "source": [
    "missing_count_val = X_val['Alternative Dispute Resolution'].isnull().sum()\n",
    "\n",
    "missing_count_train = X_train['Alternative Dispute Resolution'].isnull().sum()\n",
    "missing_count_test = X_test['Alternative Dispute Resolution'].isnull().sum()\n",
    "\n",
    "missing_count_train, missing_count_val, missing_count_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "id": "kZ755ZeMawfm",
    "outputId": "f6262505-8021-49df-9c11-759e74d1b392"
   },
   "outputs": [],
   "source": [
    "percentage_ADR_train = X_train['Alternative Dispute Resolution'].value_counts(normalize=True) * 100\n",
    "percentage_ADR_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H8c2mA54awfn"
   },
   "outputs": [],
   "source": [
    "# Extract proportions for ADR values\n",
    "top_industries = X_train['Alternative Dispute Resolution'].value_counts(normalize=True)\n",
    "adr = top_industries.index.tolist()\n",
    "proportions_train = top_industries.values  \n",
    "\n",
    "print(\"Proportions array:\", proportions_train)\n",
    "print(\"Industry Codes list:\", adr)\n",
    "\n",
    "# Impute missing values in X_train and X_val\n",
    "X_train = impute_prop(adr, proportions_train, X_train, 'Alternative Dispute Resolution')\n",
    "X_val = impute_prop(adr, proportions_train, X_val, 'Alternative Dispute Resolution')\n",
    "# this is redundant with our current dataset\n",
    "X_test = impute_prop(adr, proportions_train, X_test, 'Alternative Dispute Resolution')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if all values are filled\n",
    "X_train['Alternative Dispute Resolution'].isna().sum(), X_val['Alternative Dispute Resolution'].isna().sum(), X_test['Alternative Dispute Resolution'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "tKi_XDAnawfp",
    "outputId": "1fb38a71-a7ce-4ecc-c0b5-d977e6bff09b"
   },
   "outputs": [],
   "source": [
    "display(missing_data(X_train) )\n",
    "display(missing_data(X_val))\n",
    "display(missing_data(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V3zGWod8awfq"
   },
   "source": [
    "> Before we go further, let's drop the variable Agreement Reached, as it is unknown at the start of a claim and therefore cannot be used in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EnalUuwgawfq"
   },
   "outputs": [],
   "source": [
    "X_train = X_train.drop(columns=['Agreement Reached'])\n",
    "X_val = X_val.drop(columns=['Agreement Reached'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Outlier Detection and Treatment<a class=\"anchor\" id=\"outlier-detection-and-treatment\"></a>\n",
    "[Back to ToC](#toc)<br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We decided to create a copy of X_train and X_val to avoid mistakes.\n",
    "X_train_copy = X_train.copy() \n",
    "X_val_copy = X_val.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.1. Date Outliers<a class=\"anchor\" id=\"date-outliers\"></a>\n",
    "[Back to ToC](#toc)<br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We'll start by checking and trreating existing outliers in dates.\n",
    "> * <code>Accident Date</code>\n",
    "> * <code>Assembly Date</code>\n",
    "> * <code>C-2 Date</code>\n",
    "> * <code>C-3 Date</code>\n",
    "> * <code>First Hearing Date</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list with all the date variables\n",
    "dates=['Accident Date','Assembly Date','C-2 Date','C-3 Date','First Hearing Date']\n",
    "X_train_copy[dates].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"darkgrid\")\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, col in enumerate(dates, 1):\n",
    "    plt.subplot(3, 2, i) \n",
    "    sns.histplot(X_train_copy[col], kde=False, bins=30,color='#69b9ff')\n",
    "    plt.title(f'{col} Distribution')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> It's clear to see that there are outliers in <code>Accident Date</code>, <code>C-2 Date</code>, and <code>C-3 Date</code>, with some entries dating as far back as 1961 in Accident Date. Given that these three columns exhibit a left-skewed distribution, we'll use the Interquartile Range (IQR) method to identify and handle the outliers. The IQR will help us find the lower bound for these dates, allowing us to filter out unrealistic or extreme values that fall outside the expected range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List with the date columns with outliers\n",
    "dates_with_outliers = ['Accident Date','C-2 Date','C-3 Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for date in dates_with_outliers:\n",
    "    iqr_date(X_train_copy,date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For all three columns, the upper bound date exceeds the actual maximum date. Therefore, we'll focus on removing the values below the lower bound. However, we will only consider dates before 2018-01-01, as the lower bound for all three columns is set to a date in 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for date in dates_with_outliers:\n",
    "  X_train_copy = X_train_copy[X_train_copy[date].ge('2018-01-01') | X_train_copy[date].isna()]\n",
    "  X_val_copy = X_val_copy[X_val_copy[date].ge('2018-01-01') | X_val_copy[date].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_copy[dates].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> After dropping the outliers, we can see that the minimum year is 2018 and the maximum year is 2024, keeping only the data that is within a reasonable and expected time range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of date outliers dropped from X_train_copy:{ X_train.shape[0] - X_train_copy.shape[0]}')\n",
    "print(f'Number of date outliers dropped from X_val_copy:{ X_val.shape[0] - X_val_copy.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Days Between Accident Date and Assembly Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep track of number of rows dropped\n",
    "initial_count_train = X_train_copy.shape[0]\n",
    "initial_count_val = X_val_copy.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a column in X_train_copy and X_val_copy that calculates the days between Accident Date and Assembly Date\n",
    "X_train_copy.loc[:, 'days_between_accident_assembly'] = days_between(X_train_copy, 'Accident Date', 'Assembly Date')\n",
    "X_val_copy.loc[:, 'days_between_accident_assembly'] = days_between(X_val_copy, 'Accident Date', 'Assembly Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_copy['days_between_accident_assembly'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> After calculating the number of days between the <code>Accident Date</code> and <code>Assembly Date</code>, we observe that some claims have a significantly high difference in days. Given the skewed distribution of the number of days, we will apply a log transformation to the data. After that, we'll find an upper bound for the transformed values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add log transformation column in X_train\n",
    "X_train_copy['log_days_between_accident_assembly'] = np.log1p(X_train_copy['days_between_accident_assembly'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the remove_outliers_iqr function created in helper_functions we'll \n",
    "lower, upper, X_train_copy = log_remove_outliers_iqr(X_train_copy,'log_days_between_accident_assembly',2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> After calculating the interquartile range (IQR) to detect outliers in the <code>days between accident and assembly</code> we observed that the lower bound for the IQR was negative. Given that the number of days between events cannot logically be negative, we decided to discard the negative lower bound.\n",
    "\n",
    "> For the upper bound, we opted to use the IQR method, as it identifies extreme values on the higher end of the distribution. While we also considered using the 99th percentile to set an upper limit, it resulted in an upper bound of 297 days, which we felt would remove too much data. Therefore, we chose to proceed with the IQR-based upper bound, which better aligns with the distribution of the data and allows us to retain more meaningful observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the same upper bound to X_val_copy\n",
    "initial_count = X_val_copy.shape[0]\n",
    "\n",
    "X_val_copy = X_val_copy[X_val_copy['days_between_accident_assembly']<upper]\n",
    "\n",
    "final_count = X_val_copy.shape[0]\n",
    "\n",
    "print(f\"Number of rows removed: {initial_count-final_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the columns created\n",
    "X_train_copy = X_train_copy.drop('days_between_accident_assembly', axis=1)\n",
    "X_train_copy = X_train_copy.drop('log_days_between_accident_assembly', axis=1)\n",
    "X_val_copy = X_val_copy.drop('days_between_accident_assembly', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of outliers removed from X_train_copy: {initial_count_train- X_train_copy.shape[0]}')\n",
    "print(f'Number of outliers removed from X_val_copy: {initial_count_val- X_val_copy.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.2. Numerical outliers<a class=\"anchor\" id=\"numerical-outliers\"></a>\n",
    "[Back to ToC](#toc)<br> \n",
    "\n",
    "> * <code>Age at Injury</code>\n",
    "> * <code>Average Weekly Wage</code>\n",
    "> * <code>Birth Year</code>\n",
    "> * <code>IME-4 Count</code>\n",
    "> * '<code>Number of Dependents'</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting only the numerical columns\n",
    "numerical_cols = ['Age at Injury','Average Weekly Wage','Birth Year','IME-4 Count','Number of Dependents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_copy[numerical_cols].describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme()\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, col in enumerate(numerical_cols, 1):\n",
    "    plt.subplot(3, 2, i) \n",
    "    sns.boxplot(x=X_train_copy[col], color='#69b9ff')\n",
    "    plt.title(f'{col} Boxplot')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Value')\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> No significant issues were found with the <code>Birth Year</code> and <code>Number of Dependents</code> variables. Therefore, no further treatment will be applied to thise columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Age at Injury and Birth Year\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>Age at Injury</code> \n",
    "> * __maximum age of 120__\n",
    "> * __minimum age of 1__\n",
    "\n",
    "The distribution is slightly right-skewed, indicating that there are a few extreme values. We will use the Interquartile Range (IQR) method to identify and remove outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_count_train = X_train_copy.shape[0]\n",
    "initial_count_val = X_val_copy.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outlier in X_train, obtain lower and upper bound and apply to X_val\n",
    "lower, upper, X_train_copy = remove_outliers_iqr(X_train_copy,'Age at Injury')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> After applying the IQR method to detect outliers, we found that the upper bound for <code>Age at Injury</code> is 88.5, which we will apply to both X_train_copy and X_val_copy. The lower bound, however, is -3.5, so we won't consider it. Instead, we'll set the minimum to 11 years old, as it is the age we considered would be acceptable in section 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the same upper bound to X_val_copy\n",
    "X_val_copy = X_val_copy[X_val_copy['Age at Injury']<upper]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply minimum of 11 years old to X_train_copy and X_val_copy\n",
    "X_train_copy = X_train_copy[X_train_copy['Age at Injury']>11]\n",
    "X_val_copy = X_val_copy[X_val_copy['Age at Injury']>=11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of outliers removed from X_train_copy: {initial_count_train- X_train_copy.shape[0]}')\n",
    "print(f'Number of outliers removed from X_val_copy: {initial_count_val- X_val_copy.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Average Weekly Wage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_count_train = X_train_copy.shape[0]\n",
    "initial_count_val = X_val_copy.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 99.99 percentile for Average Weekly Wage\n",
    "percentile_99 = X_train_copy['Average Weekly Wage'].quantile(0.9999)\n",
    "print(round(percentile_99,2))\n",
    "print(f'Number of cases where the Average Weekly Wage above the 99.99 percentile : {(X_train_copy['Average Weekly Wage'] > percentile_99).sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * <code>Average Weekly</code> Wage has __high outliers__, due to individuals with extremely high earnings,\n",
    ">     *  a maximum Average Weekly Wage value reaching 2,659,398.\n",
    "> * To manage these extremes, we will first remove values above the 99.99th percentile. Following this, we will apply a log transformation and then use the IQR method to identify and remove any remaining outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply conditions to the training set first\n",
    "X_train_copy = X_train_copy[(X_train_copy['Average Weekly Wage'] < percentile_99)]\n",
    "\n",
    "# Apply the same conditions to the validation set\n",
    "X_val_copy = X_val_copy[(X_val_copy['Average Weekly Wage'] < percentile_99)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add log transformation column\n",
    "X_train_copy['log_avg_weekly_wage'] = np.log1p(X_train_copy['Average Weekly Wage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold of 2 for a more conservative range\n",
    "lower,upper,X_train_copy = log_remove_outliers_iqr(X_train_copy,'log_avg_weekly_wage',2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply upper bound to X_train_copy and X_val_copy\n",
    "X_val_copy = X_val_copy[(X_val_copy['Average Weekly Wage'] < upper)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of outliers removed from X_train_copy: {initial_count_train- X_train_copy.shape[0]}')\n",
    "print(f'Number of outliers removed from X_val_copy: {initial_count_val- X_val_copy.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### IME-4 Count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Most of the claims have an IME-4 Count of zero. Given this, we will only look to remove the clear outliers from observing the boxplot, which are above 40."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_count_train = X_train_copy.shape[0]\n",
    "initial_count_val = X_val_copy.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_copy = X_train_copy[(X_train_copy['IME-4 Count'] < 40)]\n",
    "X_val_copy = X_val_copy[(X_val_copy['IME-4 Count'] < 40)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of outliers removed from X_train_copy: {initial_count_train- X_train_copy.shape[0]}')\n",
    "print(f'Number of outliers removed from X_val_copy: {initial_count_val- X_val_copy.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.3. Applying Changes<a class=\"anchor\" id=\"applying-changes\"></a>\n",
    "[Back to ToC](#toc)<br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Total of {X_train.shape[0] - X_train_copy.shape[0]} cases in X_train')\n",
    "print(f'Total of {X_val.shape[0] - X_val_copy.shape[0]} cases in X_val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_copy.copy()\n",
    "X_val = X_val_copy.copy()\n",
    "\n",
    "# after we have cleaned the data we need to exclude the index out of the y_train and y_val \n",
    "y_train = y_train.loc[X_train.index]\n",
    "y_val = y_val.loc[X_val.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> It is important to highlight that the location-related variables (County of Injury, District Name, Medical Fee Region, Zip Code) will not be considered in this model for the first submission, as we believe they may be less relevant. Therefore, there will be no need to perform encoding for these variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k0K04KFVawfu"
   },
   "source": [
    "### 5.3. Dealing with Categorical Variables<a class=\"anchor\" id=\"categorical-variables\"></a>\n",
    "[Back to ToC](#toc)<br> \n",
    ">\n",
    ">- Identify categorical features in the dataset.\n",
    ">- Discuss encoding techniques (one-hot encoding, label encoding, etc.).\n",
    ">- Apply the chosen encoding method to transform categorical variables.\n",
    "\n",
    "> It is important to highlight that the __location-related variables__ (County of Injury, District Name, Medical Fee Region, Zip Code) __will not be considered in this model for the first submission__, as we want to avoid redlining/discrimination in our model. A similiar argument can be used for gender."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cxfjPKhnawfu"
   },
   "source": [
    "> **Binary Variables:**\n",
    "> - Alternative Dispute Resolution\n",
    "> - COVID-19 Indicator\n",
    "> - Attorney/Representative\n",
    "> - Gender\n",
    "> - C-2 Date           | their missing values will be encoded as 0:  10210 rows are missing\n",
    "> - C-3 Date           | their missing values will be encoded as 0: 270526 rows are missing\n",
    "> - First Hearing Date | their missing values will be encoded as 0: 296236 rows are missing\n",
    "> \n",
    "> **Low Cardinality (less than 10 unique values):**\n",
    "> - Carrier Type (lets handle this feature with Carrier Name) --> Drop carrier name. This should not be relevant\n",
    "> \n",
    "> **Multiple values more than 10:**\n",
    "> - Industry code\n",
    "> - WCIO Cause of Inj Code\n",
    "> - WCIO Nature of Inj Code\n",
    "> - WCIO Part of Body Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create some copies of the train dataframes\n",
    "X_train_encoded = X_train.copy()\n",
    "X_val_encoded = X_val.copy()\n",
    "X_test_encoded = X_test.copy()\n",
    "\n",
    "\n",
    "\n",
    "X_train_encoded_original_index = X_train.index\n",
    "X_val_encoded_original_index = X_val.index\n",
    "X_test_encoded_original_index = X_test.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.1. Encoding Binary variables<a class=\"anchor\" id=\"binary-variables\"></a>\n",
    " [Back to ToC](#toc)<br> \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the unique values for binary columns and how often the appear\n",
    "\n",
    "cat_variables_binary = ['Alternative Dispute Resolution', 'COVID-19 Indicator', 'Attorney/Representative', 'Gender']\n",
    "\n",
    "for value in cat_variables_binary:\n",
    "    print(f'Unique values for {value}:')\n",
    "    print(X_train_encoded[value].value_counts(dropna= False), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print how many na and non na values are in the following columns C-2 Date, C-3 Date, First Hearing Date\n",
    "date_binary = ['C-2 Date', 'C-3 Date', 'First Hearing Date']\n",
    "for col in date_binary:\n",
    "    na_count = X_train_encoded[col].isna().sum()\n",
    "    non_na_count = X_train_encoded[col].notna().sum()\n",
    "    print(f\"Column '{col}':\")\n",
    "    print(f\"  NA values: {na_count}\")\n",
    "    print(f\"  Non-NA values: {non_na_count}\\n\")\n",
    "\n",
    "\n",
    "# encode the variables and put 1 for non na and 0 for na for the training set\n",
    "X_train_encoded['C-2 Date'] = X_train_encoded['C-2 Date'].notna().astype(int)\n",
    "X_train_encoded['C-3 Date'] = X_train_encoded['C-3 Date'].notna().astype(int)\n",
    "X_train_encoded['First Hearing Date'] = X_train_encoded['First Hearing Date'].notna().astype(int)\n",
    "\n",
    "# encode the variables and put 1 for non na and 0 for na for the validation set\n",
    "X_val_encoded['C-2 Date'] = X_val_encoded['C-2 Date'].notna().astype(int)\n",
    "X_val_encoded['C-3 Date'] = X_val_encoded['C-3 Date'].notna().astype(int)\n",
    "X_val_encoded['First Hearing Date'] = X_val_encoded['First Hearing Date'].notna().astype(int)\n",
    "\n",
    "# encode the variables and put 1 for non na and 0 for na for the validation set\n",
    "X_test_encoded['C-2 Date'] = X_test_encoded['C-2 Date'].notna().astype(int)\n",
    "X_test_encoded['C-3 Date'] = X_test_encoded['C-3 Date'].notna().astype(int)\n",
    "X_test_encoded['First Hearing Date'] = X_test_encoded['First Hearing Date'].notna().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the binary variables and mapping\n",
    "binary_vars = ['Alternative Dispute Resolution', 'Attorney/Representative', 'COVID-19 Indicator']\n",
    "binary_mapping = {'Y': 1, 'N': 0}\n",
    "\n",
    "# Apply mapping to binary variables\n",
    "\n",
    "for var in binary_vars:\n",
    "    X_train_encoded[var] = X_train_encoded[var].map(binary_mapping)\n",
    "\n",
    "# Apply for validation set\n",
    "for var in binary_vars:\n",
    "    X_val_encoded[var] = X_val_encoded[var].map(binary_mapping)\n",
    "\n",
    "# Apply for validation set\n",
    "for var in binary_vars:\n",
    "    X_test_encoded[var] = X_test_encoded[var].map(binary_mapping)\n",
    "\n",
    "# convert all columns into int values\n",
    "# X_train_encoded[binary_vars] = X_train_encoded[binary_vars].astype(int)\n",
    "\n",
    "# Gender to binary mapping\n",
    "gender_mapping = {'M': 1, 'F': 0}\n",
    "X_train_encoded['Gender'] = X_train_encoded['Gender'].map(gender_mapping)\n",
    "X_val_encoded['Gender'] = X_val_encoded['Gender'].map(gender_mapping)\n",
    "X_test_encoded['Gender'] = X_test_encoded['Gender'].map(gender_mapping )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_encoded[binary_vars].isna().sum()\n",
    "X_test_encoded[binary_vars].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.2. Carrier Type Encoding<a class=\"anchor\" id=\"carrier-type\"></a>\n",
    " [Back to ToC](#toc)<br> \n",
    "> * We currently have several variable names for the carrier type\n",
    "> * We want to encode the carrier type into catgeories of similiar carriers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_encoded['Carrier Type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Group Carrier Type into smaller buckets\n",
    "> * Create dummy variables for every group. Drop the first column to avoid multicolinarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mapping\n",
    "carrier_type_mapping = {\n",
    "    '1A. PRIVATE': 'Private Insurance Carrier',\n",
    "    '2A. SIF': 'State Insurance Fund',\n",
    "    '3A. SELF PUBLIC': 'Self-insured Public Entity',\n",
    "    '4A. SELF PRIVATE': 'Self-insured Private Entity',\n",
    "    '5A. SPECIAL FUND - CONS. COMM. (SECT. 25-A)': 'Special Funds',\n",
    "    '5C. SPECIAL FUND - POI CARRIER WCB MENANDS': 'Special Funds',\n",
    "    '5D. SPECIAL FUND - UNKNOWN': 'Special Funds',\n",
    "    'UNKNOWN': 'Unknown'\n",
    "}\n",
    "\n",
    "# apply for training set\n",
    "    # Step 2: Apply the mapping to create 'Carrier Group'\n",
    "if 'Carrier Type' in X_train.columns:\n",
    "\n",
    "    X_train_encoded['Carrier Group'] = X_train_encoded['Carrier Type'].map(carrier_type_mapping)\n",
    "    X_train_encoded['Carrier Group'] =X_train_encoded['Carrier Group'].fillna('Other')\n",
    "\n",
    "\n",
    "    # Step 3: Encode 'Carrier Group' using One-Hot Encoding\n",
    "X_train_encoded =   pd.concat([X_train_encoded, pd.get_dummies(X_train_encoded['Carrier Group'],\n",
    "                        columns=['Carrier Group'],\n",
    "                        prefix='CarrierGroup',\n",
    "                        dtype=int,\n",
    "                        drop_first=True)], # Drop the first column to avoid multicollinearity\n",
    "                        axis=1\n",
    "                    )\n",
    "\n",
    "\n",
    "# apply for valdiation set\n",
    "    # Step 4: Apply the mapping to create 'Carrier Group' for x_val_encoded\n",
    "if 'Carrier Type' in X_val.columns:\n",
    "\n",
    "    X_val_encoded['Carrier Group'] = X_val_encoded['Carrier Type'].map(carrier_type_mapping)\n",
    "    X_val_encoded['Carrier Group'] =X_val_encoded['Carrier Group'].fillna('Other')\n",
    "\n",
    "\n",
    "    # Step 5: Encode 'Carrier Group' using One-Hot Encoding for X_val_encoded\n",
    "X_val_encoded = pd.concat([X_val_encoded, pd.get_dummies(X_val_encoded['Carrier Group'],\n",
    "                        columns=['Carrier Group'],\n",
    "                        prefix='CarrierGroup',\n",
    "                        dtype=int,\n",
    "                        drop_first=True)], # Drop the first column to avoid multicollinearity\n",
    "                        axis=1\n",
    "                )\n",
    "\n",
    "# apply for test set\n",
    "# Step 4: Apply the mapping to create 'Carrier Group' for x_test_encoded\n",
    "if 'Carrier Type' in X_test.columns:\n",
    "\n",
    "    X_test_encoded['Carrier Group'] = X_test_encoded['Carrier Type'].map(carrier_type_mapping)\n",
    "    X_test_encoded['Carrier Group'] = X_test_encoded['Carrier Group'].fillna('Other')\n",
    "\n",
    "\n",
    "    # Step 5: Encode 'Carrier Group' using One-Hot Encoding for X_test_encoded\n",
    "X_test_encoded = pd.concat([X_test_encoded, pd.get_dummies(X_test_encoded['Carrier Group'],\n",
    "                        columns=['Carrier Group'],\n",
    "                        prefix='CarrierGroup',\n",
    "                        dtype=int,\n",
    "                        drop_first=True)], # Drop the first column to avoid multicollinearity\n",
    "                        axis=1\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5u-DRwFwawfu"
   },
   "source": [
    "#### 5.3.3. Industry Code<a class=\"anchor\" id=\"industry-code\"></a>\n",
    "[Back to ToC](#toc)<br> \n",
    "\n",
    "\n",
    "* We currently have several variable names for the carrier type\n",
    "* We want to encode the carrier type into catgeories of similiar carriers.\n",
    "* Check values which are rare in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4escPivBawfu"
   },
   "outputs": [],
   "source": [
    "# apply the target encoding to the industry code column | target encoding is defined in the helper functions    \n",
    "X_train_encoded, X_val_encoded = target_encode_multiclass(X_train_encoded, X_val_encoded,  y_train, 'Industry Code', 'Claim Injury Type')\n",
    "\n",
    "# set the index back to the original index\n",
    "X_train_encoded.set_index(X_train_encoded_original_index, inplace=True)\n",
    "X_val_encoded.set_index(X_val_encoded_original_index, inplace=True)\n",
    "\n",
    "# apply to  test set / This is not working yet because we probably need to clean the test set first\n",
    "X_test_encoded = target_encode_multiclass(X_train_encoded, X_test_encoded,  y_train, 'Industry Code', 'Claim Injury Type')[1]\n",
    "# set index back to the original index\n",
    "X_test_encoded.set_index(X_test_encoded_original_index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_encoded[['Industry Code']].shape[0], y_train[['Claim Injury Type']].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.4. WCIO Codes encoding<a class=\"anchor\" id=\"WCIO_Codes_Encoding\"></a>\n",
    " [Back to ToC](#toc)<br> \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "> * In the WCIO Codes encoding section we define dictionaries that map specific WCIO Codes values to broader, generalized injury categories. \n",
    "> * This mapping is applied to columns in the training, validation, and test datasets\n",
    "> * This is used to decode the injury/cause/part into more genral buckets which we can use for dummies and TargetEncoding and for an increase in interpretability\n",
    "> * This approach might be adjusted later on\n",
    "> * We use the following table for our encoding [Decoding WCIO Codes References](https://www.guarantysupport.com/wp-content/uploads/2024/02/WCIO-Legacy.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __WCIO Cause of Injury Code__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_encoded['WCIO Cause of Injury Code'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_train_encoded['WCIO Cause of Injury Code'].value_counts()).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the column into int from float for training set\n",
    "X_train_encoded['WCIO Cause of Injury Code'] = X_train_encoded['WCIO Cause of Injury Code'].astype(int)\n",
    "\n",
    "# transform the column into int from float for validation set\n",
    "X_val_encoded['WCIO Cause of Injury Code'] = X_val_encoded['WCIO Cause of Injury Code'].astype(int)\n",
    "\n",
    "# transform the column into int from float for test set\n",
    "X_test_encoded['WCIO Cause of Injury Code'] = X_test_encoded['WCIO Cause of Injury Code'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_train_encoded['WCIO Cause of Injury Code'].value_counts(normalize = True).sort_index()).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Create a __dictionary__ based on the table from the WCIO references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping WCIO Cause of Injury codes to more general categories for modeling\n",
    "\n",
    "# Define the mapping dictionary\n",
    "wcio_cause_of_injury_map = {\n",
    "\n",
    "    # Category: Burn or Scald\n",
    "    # Injury types related to exposure to heat, cold, chemicals, or radiation\n",
    "    1: \"Burn_or_Scald\", 2: \"Burn_or_Scald\", 3: \"Burn_or_Scald\",\n",
    "    4: \"Burn_or_Scald\", 5: \"Burn_or_Scald\", 6: \"Burn_or_Scald\",\n",
    "    7: \"Burn_or_Scald\", 8: \"Burn_or_Scald\", 9: \"Burn_or_Scald\",\n",
    "    11: \"Burn_or_Scald\", 14: \"Burn_or_Scald\", 84: \"Burn_or_Scald\",\n",
    "\n",
    "    # Category: Caught In\n",
    "    # Injuries caused by being caught in or under machinery or objects\n",
    "    10: \"Caught_In\", 12: \"Caught_In\", 13: \"Caught_In\",\n",
    "    20: \"Caught_In\",\n",
    "\n",
    "    # Category: Cut, Puncture, Scrape\n",
    "    # Injuries involving cuts, punctures, and abrasions\n",
    "    15: \"Cut_Puncture_Scrape\", 16: \"Cut_Puncture_Scrape\", 17: \"Cut_Puncture_Scrape\",\n",
    "    18: \"Cut_Puncture_Scrape\", 19: \"Cut_Puncture_Scrape\",\n",
    "\n",
    "    # Category: Fall, Slip or Trip\n",
    "    # Injuries due to falls, slips, or trips at various levels\n",
    "    25: \"Fall_Slip_or_Trip\", 26: \"Fall_Slip_or_Trip\", 27: \"Fall_Slip_or_Trip\",\n",
    "    28: \"Fall_Slip_or_Trip\", 29: \"Fall_Slip_or_Trip\", 30: \"Fall_Slip_or_Trip\",\n",
    "    31: \"Fall_Slip_or_Trip\", 32: \"Fall_Slip_or_Trip\", 33: \"Fall_Slip_or_Trip\",\n",
    "\n",
    "    # Category: Motor Vehicle\n",
    "    # Injuries related to motor vehicle incidents\n",
    "    40: \"Motor_Vehicle\", 41: \"Motor_Vehicle\", 45: \"Motor_Vehicle\",\n",
    "    46: \"Motor_Vehicle\", 47: \"Motor_Vehicle\", 48: \"Motor_Vehicle\",\n",
    "    50: \"Motor_Vehicle\",\n",
    "\n",
    "    # Category: Strain or Injury\n",
    "    # Injuries resulting from physical strain, overuse, or repetitive motions\n",
    "    52: \"Strain_or_Injury\", 53: \"Strain_or_Injury\", 54: \"Strain_or_Injury\",\n",
    "    55: \"Strain_or_Injury\", 56: \"Strain_or_Injury\", 57: \"Strain_or_Injury\",\n",
    "    58: \"Strain_or_Injury\", 59: \"Strain_or_Injury\", 60: \"Strain_or_Injury\",\n",
    "    61: \"Strain_or_Injury\", 97: \"Strain_or_Injury\",\n",
    "\n",
    "    # Category: Striking Against or Stepping On\n",
    "    # Injuries from striking against or stepping on objects\n",
    "    65: \"Striking_Against_or_Stepping_On\", 66: \"Striking_Against_or_Stepping_On\", 67: \"Striking_Against_or_Stepping_On\",\n",
    "    68: \"Striking_Against_or_Stepping_On\", 69: \"Striking_Against_or_Stepping_On\", 70: \"Striking_Against_or_Stepping_On\",\n",
    "\n",
    "    # Category: Struck or Injured By\n",
    "    # Injuries caused by being struck by objects or individuals\n",
    "    74: \"Struck_or_Injured_By\", 75: \"Struck_or_Injured_By\", 76: \"Struck_or_Injured_By\",\n",
    "    77: \"Struck_or_Injured_By\", 78: \"Struck_or_Injured_By\", 79: \"Struck_or_Injured_By\",\n",
    "    80: \"Struck_or_Injured_By\", 81: \"Struck_or_Injured_By\", 85: \"Struck_or_Injured_By\",\n",
    "    86: \"Struck_or_Injured_By\",\n",
    "\n",
    "    # Category: Rubbed or Abraded By\n",
    "    # Injuries caused by rubbing or abrasion, such as callouses or blisters\n",
    "    94: \"Rubbed_or_Abraded_By\", 95: \"Rubbed_or_Abraded_By\",\n",
    "\n",
    "    # Category: Miscellaneous Causes\n",
    "    # Other injury causes not otherwise classified\n",
    "    82: \"Miscellaneous_Causes\", 83: \"Miscellaneous_Causes\",  87: \"Miscellaneous_Causes\", 88: \"Miscellaneous_Causes\",\n",
    "    89: \"Miscellaneous_Causes\", 90: \"Miscellaneous_Causes\", 91: \"Miscellaneous_Causes\", 93: \"Miscellaneous_Causes\",\n",
    "    94: \"Miscellaneous_Causes\", 95: \"Miscellaneous_Causes\", 96: \"Miscellaneous_Causes\", 98: \"Miscellaneous_Causes\",\n",
    "    99: \"Miscellaneous_Causes\",\n",
    "\n",
    "    # Category: Unknown\n",
    "    100: \"Unknown\"\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Map <code>Injury_cause_Category</code> based on the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mapping\n",
    "X_train_encoded['Injury_Cause_Category'] = X_train_encoded['WCIO Cause of Injury Code'].map(wcio_cause_of_injury_map)\n",
    "X_val_encoded['Injury_Cause_Category'] = X_val_encoded['WCIO Cause of Injury Code'].map(wcio_cause_of_injury_map)\n",
    "X_test_encoded['Injury_Cause_Category'] = X_test_encoded['WCIO Cause of Injury Code'].map(wcio_cause_of_injury_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Create __dummy variables__ based on the  <code>Injury_cause_Category</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dummy variables based on the Injury_cause_Category\n",
    "X_train_encoded = pd.concat([X_train_encoded, pd.get_dummies(X_train_encoded['Injury_Cause_Category'],\n",
    "                        columns=['Injury_Cause_Category'],\n",
    "                        prefix='Injury_Cause',\n",
    "                        dtype=int,\n",
    "                        drop_first=True)], # Drop the first column to avoid multicollinearity\n",
    "    axis=1)\n",
    "\n",
    "X_train_encoded.head().T\n",
    "\n",
    "X_val_encoded = pd.concat([X_val_encoded, pd.get_dummies(X_val_encoded['Injury_Cause_Category'],\n",
    "                        columns=['Injury_Cause_Category'],\n",
    "                        prefix='Injury_Cause',\n",
    "                        dtype=int,\n",
    "                        drop_first=True)], # Drop the first column to avoid multicollinearity\n",
    "    axis=1)\n",
    "\n",
    "X_test_encoded = pd.concat([X_test_encoded, pd.get_dummies(X_test_encoded['Injury_Cause_Category'],\n",
    "                        columns=['Injury_Cause_Category'],\n",
    "                        prefix='Injury_Cause',\n",
    "                        dtype=int,\n",
    "                        drop_first=True)], # Drop the first column to avoid multicollinearity\n",
    "    axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Create __target encoding variables__ based on the  <code>Injury_cause_Category</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_encoded, X_val_encoded = target_encode_multiclass(X_train_encoded, X_val_encoded, y_train, 'Injury_Cause_Category', 'Claim Injury Type')\n",
    "\n",
    "\n",
    "# set the index back to the original index\n",
    "X_train_encoded.set_index(X_train_encoded_original_index, inplace=True)\n",
    "X_val_encoded.set_index(X_val_encoded_original_index, inplace=True)\n",
    "\n",
    "\n",
    "X_test_encoded = target_encode_multiclass(X_train_encoded, X_test_encoded, y_train, 'Injury_Cause_Category', 'Claim Injury Type')[1]\n",
    "# set the index back to the original index\n",
    "X_test_encoded.set_index(X_test_encoded_original_index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __WCIO Nature of Injury Code__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_encoded['WCIO Nature of Injury Code'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_train_encoded['WCIO Nature of Injury Code'].value_counts()).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the column into int from float for training set\n",
    "X_train_encoded['WCIO Nature of Injury Code'] = X_train_encoded['WCIO Nature of Injury Code'].astype(int)\n",
    "\n",
    "# transform the column into int from float for validation set\n",
    "X_val_encoded['WCIO Nature of Injury Code'] = X_val_encoded['WCIO Nature of Injury Code'].astype(int)\n",
    "\n",
    "# transform the column into int from float for test set\n",
    "X_test_encoded['WCIO Nature of Injury Code'] = X_test_encoded['WCIO Nature of Injury Code'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Create a __dictionary__ based on the table from the WCIO references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping WCIO Nature of Injury codes to general categories\n",
    "wcionature_of_injury_mapping = {\n",
    "\n",
    "    # Category: Specific\n",
    "    # Injuries categorized as specific, meaning they are distinct and isolated injuries\n",
    "    1: \"Specific\", 2: \"Specific\", 3: \"Specific\",\n",
    "    4: \"Specific\", 7: \"Specific\", 10: \"Specific\",\n",
    "    13: \"Specific\", 16: \"Specific\", 19: \"Specific\",\n",
    "    22: \"Specific\", 25: \"Specific\", 28: \"Specific\",\n",
    "    30: \"Specific\", 31: \"Specific\", 32: \"Specific\",\n",
    "    34: \"Specific\", 36: \"Specific\", 37: \"Specific\",\n",
    "    38: \"Specific\", # 38 refers to ADVERSE REACTION TO A VACCINATION OR INOCULATION classify as Specific\n",
    "    40: \"Specific\", 41: \"Specific\", 42: \"Specific\",\n",
    "    43: \"Specific\", 46: \"Specific\", 47: \"Specific\",\n",
    "    49: \"Specific\", 52: \"Specific\", 53: \"Specific\",\n",
    "    54: \"Specific\", 55: \"Specific\", 58: \"Specific\",\n",
    "    59: \"Specific\",\n",
    "\n",
    "    # Category: Occupational\n",
    "    # Injuries related to occupational disease or cumulative trauma\n",
    "    60: \"Occupational\", 61: \"Occupational\", 62: \"Occupational\",\n",
    "    63: \"Occupational\", 64: \"Occupational\", 65: \"Occupational\",\n",
    "    66: \"Occupational\", 67: \"Occupational\", 68: \"Occupational\",\n",
    "    69: \"Occupational\", 70: \"Occupational\", 71: \"Occupational\",\n",
    "    72: \"Occupational\", 73: \"Occupational\", 74: \"Occupational\",\n",
    "    75: \"Occupational\", 76: \"Occupational\", 77: \"Occupational\",\n",
    "    78: \"Occupational\", 79: \"Occupational\", 80: \"Occupational\",\n",
    "    83: \"Occupational\", # 83 refers to covid. I classified it as occupational\n",
    "\n",
    "    # Category: Multiple\n",
    "    # Injuries that involve multiple types, either physical or both physical and psychological\n",
    "    90: \"Multiple\", 91: \"Multiple\",\n",
    "\n",
    "    # Category: Unknown\n",
    "    100: \"Unknown\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Map <code>WCIO Nature of Injury Code</code> based on the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map for validation set\n",
    "X_train_encoded['Injury_Nature_Category'] = X_train_encoded['WCIO Nature of Injury Code'].map(wcionature_of_injury_mapping)\n",
    "\n",
    "# map for validation set\n",
    "X_val_encoded['Injury_Nature_Category'] = X_val_encoded['WCIO Nature of Injury Code'].map(wcionature_of_injury_mapping)\n",
    "\n",
    "# map for test set\n",
    "X_test_encoded['Injury_Nature_Category'] = X_test_encoded['WCIO Nature of Injury Code'].map(wcionature_of_injury_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(X_test_encoded[X_test_encoded['Injury_Nature_Category'].isna()]['WCIO Nature of Injury Code'].unique())\n",
    "display(X_train_encoded[X_train_encoded['Injury_Nature_Category'].isna()]['WCIO Nature of Injury Description'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Create __dummy variables__ based on the  <code>Injury_Nature_Category</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dummy variables out of Injury_Nature_Category column\n",
    "\n",
    "\n",
    "# create a dummy varialbes for the Injury Category for training set\n",
    "X_train_encoded = pd.concat([X_train_encoded, pd.get_dummies(X_train_encoded['Injury_Nature_Category'],\n",
    "                        columns=['Nature_Injury'],\n",
    "                        prefix='Nature_Injury',\n",
    "                        dtype=int,\n",
    "                        drop_first=True)], # Drop the first column to avoid multicollinearity\n",
    "    axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# create a dummy varialbes for the Injury Category for validation set\n",
    "X_val_encoded = pd.concat([X_val_encoded, pd.get_dummies(X_val_encoded['Injury_Nature_Category'],\n",
    "                        columns=['Nature_Injury'],\n",
    "                        prefix='Nature_Injury',\n",
    "                        dtype=int,\n",
    "                        drop_first=True)], # Drop the first column to avoid multicollinearity\n",
    "    axis=1)\n",
    "\n",
    "\n",
    "# create a dummy varialbes for the Injury Category for validation set\n",
    "X_test_encoded = pd.concat([X_test_encoded, pd.get_dummies(X_test_encoded['Injury_Nature_Category'],\n",
    "                        columns=['Nature_Injury'],\n",
    "                        prefix='Nature_Injury',\n",
    "                        dtype=int,\n",
    "                        drop_first=True)], # Drop the first column to avoid multicollinearity\n",
    "                        axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Create __target encoding variables__ based on the  <code>Injury_Nature_Category</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply encoding for Injury_Nature_Category\n",
    "X_train_encoded, X_val_encoded = target_encode_multiclass(X_train_encoded, X_val_encoded, y_train, 'Injury_Nature_Category', 'Claim Injury Type')\n",
    "\n",
    "# set the index back to the original index\n",
    "X_train_encoded.set_index(X_train_encoded_original_index, inplace=True)\n",
    "X_val_encoded.set_index(X_val_encoded_original_index, inplace=True)\n",
    "\n",
    "\n",
    "# apply encoding for Injury_Nature_Category to the test set\n",
    "X_test_encoded = target_encode_multiclass(X_train_encoded, X_test_encoded, y_train, 'Injury_Nature_Category', 'Claim Injury Type')[1]\n",
    "# set the index back to the original index\n",
    "X_test_encoded.set_index(X_test_encoded_original_index, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __WCIO Part of Body Code__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the column into int from float for training set\n",
    "X_train_encoded['WCIO Part Of Body Code'] = X_train_encoded['WCIO Part Of Body Code'].astype(int)\n",
    "\n",
    "# transform the column into int from float for validation set\n",
    "X_val_encoded['WCIO Part Of Body Code'] = X_val_encoded['WCIO Part Of Body Code'].astype(int)\n",
    "\n",
    "# transform the column into int from float for validation set\n",
    "X_test_encoded['WCIO Part Of Body Code'] = X_test_encoded['WCIO Part Of Body Code'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Create a __dictionary__ based on the table from the WCIO references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_of_body_map = {\n",
    "    # Head and Neck\n",
    "    10: 'Head_and_Neck', 14: 'Head_and_Neck', 17: 'Head_and_Neck',\n",
    "    18: 'Head_and_Neck', 15: 'Head_and_Neck', 13: 'Head_and_Neck',\n",
    "    12: 'Head_and_Neck', 16: 'Head_and_Neck', 20: 'Head_and_Neck',\n",
    "    25: 'Head_and_Neck', 24: 'Head_and_Neck', 26: 'Head_and_Neck',\n",
    "    11: 'Head_and_Neck', 19: 'Head_and_Neck', 21:'Head_and_Neck',\n",
    "    22:'Head_and_Neck',23:'Head_and_Neck',\n",
    "\n",
    "    # Upper Extremities\n",
    "    30: 'Upper_Extremities', 31: 'Upper_Extremities', 32: 'Upper_Extremities',\n",
    "    33: 'Upper_Extremities', 34: 'Upper_Extremities', 35: 'Upper_Extremities',\n",
    "    36: 'Upper_Extremities', 37: 'Upper_Extremities', 38: 'Upper_Extremities',\n",
    "    39: 'Upper_Extremities',\n",
    "\n",
    "    # Trunk\n",
    "    40: 'Trunk', 41: 'Trunk', 42: 'Trunk', 43: 'Trunk', 44: 'Trunk',\n",
    "    45: 'Trunk', 46: 'Trunk', 47: 'Trunk', 48: 'Trunk', 49: 'Trunk',\n",
    "    60: 'Trunk', 61: 'Trunk', 62: 'Trunk', 63: 'Trunk',\n",
    "\n",
    "    # Lower Extremities\n",
    "    50: 'Lower_Extremities', 51: 'Lower_Extremities', 52: 'Lower_Extremities',\n",
    "    53: 'Lower_Extremities', 54: 'Lower_Extremities', 55: 'Lower_Extremities',\n",
    "    56: 'Lower_Extremities', 57: 'Lower_Extremities', 58: 'Lower_Extremities',\n",
    "\n",
    "    # Multiple Body Parts\n",
    "    64: 'Multiple_Body Parts', 65: 'Multiple_Body Parts', 66: 'Multiple_Body Parts',\n",
    "    90: 'Multiple_Body Parts', 91: 'Multiple_Body Parts', 99: 'Multiple_Body Parts',\n",
    "\n",
    "    # Unknown\n",
    "    100: 'Unknown'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map for training set\n",
    "X_train_encoded['Part_Body_Category'] = X_train_encoded['WCIO Part Of Body Code'].map(part_of_body_map)\n",
    "\n",
    "# map for training set\n",
    "X_val_encoded['Part_Body_Category'] = X_val_encoded['WCIO Part Of Body Code'].map(part_of_body_map)\n",
    "\n",
    "# map for training set\n",
    "X_test_encoded['Part_Body_Category'] = X_test_encoded['WCIO Part Of Body Code'].map(part_of_body_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Create __dummy variables__ based on the  <code>Part_Body_Category</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dummy varialbes for the Part of Body Category for training set\n",
    "X_train_encoded = pd.concat([X_train_encoded, pd.get_dummies(X_train_encoded['Part_Body_Category'],\n",
    "                        columns=['Part_Body_Category'],\n",
    "                        prefix='Body_Part',\n",
    "                        dtype=int,\n",
    "                        drop_first=True)], # Drop the first column to avoid multicollinearity\n",
    "    axis=1)\n",
    "\n",
    "\n",
    "# create a dummy variable for the Part of Body Category for val set\n",
    "X_val_encoded = pd.concat([X_val_encoded, pd.get_dummies(X_val_encoded['Part_Body_Category'],\n",
    "                        columns=['Part_Body_Category'],\n",
    "                        prefix='Body_Part',\n",
    "                        dtype=int,\n",
    "                        drop_first=True)], # Drop the first column to avoid multicollinearity\n",
    "    axis=1)\n",
    "\n",
    "# create a dummy varialbes for the Part of Body Category for training set\n",
    "X_test_encoded = pd.concat([X_test_encoded, pd.get_dummies(X_test_encoded['Part_Body_Category'],\n",
    "                        columns=['Part_Body_Category'],\n",
    "                        prefix='Body_Part',\n",
    "                        dtype=int,\n",
    "                        drop_first=True)], # Drop the first column to avoid multicollinearity\n",
    "    axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Create __target encoding variables__ based on the  <code>Part_Body_Category</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply encoding for Injury_Nature_Category\n",
    "X_train_encoded, X_val_encoded = target_encode_multiclass(X_train_encoded, X_val_encoded, y_train, 'Part_Body_Category', 'Claim Injury Type')\n",
    "\n",
    "# set the index back to the original index\n",
    "X_train_encoded.set_index(X_train_encoded_original_index, inplace=True)\n",
    "X_val_encoded.set_index(X_val_encoded_original_index, inplace=True)\n",
    "\n",
    "# apply encoding for Injury_Nature_Category to the test set\n",
    "X_test_encoded = target_encode_multiclass(X_train_encoded, X_test_encoded, y_train, 'Part_Body_Category', 'Claim Injury Type')[1]\n",
    "\n",
    "# set the index back to the original index\n",
    "X_test_encoded.set_index(X_test_encoded_original_index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.5. Numerical Variables Decoding<a class=\"anchor\" id=\"numerical-encoding\"></a>\n",
    " [Back to ToC](#toc)<br> \n",
    "\n",
    " \n",
    "> * Decode numerical variables\n",
    "> * Add new fatures "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_encoded.loc[:,'Days_between_Acc_Assembyl'] = days_between(X_train_encoded, 'Accident Date','Assembly Date')\n",
    "X_val_encoded.loc[:,'Days_between_Acc_Assembyl'] = days_between(X_val_encoded, 'Accident Date','Assembly Date')\n",
    "X_test_encoded.loc[:,'Days_between_Acc_Assembyl'] = days_between(X_test_encoded, 'Accident Date','Assembly Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_encoded[['Days_between_Acc_Assembyl']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. Feature engineering <a class=\"anchor\" id=\"feature-engineering\"></a>\n",
    " [Back to ToC](#toc)<br> \n",
    "\n",
    "* here we create new features\n",
    "    * out of old ones\n",
    "    * through new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Seasonality__\n",
    "* Months of the year or seasons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Weather__\n",
    "* adding the weather history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Additional events?__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5. Dropping Columns & Exporting DataFrames<a class=\"anchor\" id=\"dropping-exporting\"></a>\n",
    " [Back to ToC](#toc)<br> \n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __drop columns that we have encoded__\n",
    "> * For now we will work with the following columns for our feature selection and modelling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = ['Age at Injury', 'Alternative Dispute Resolution','Attorney/Representative', 'Average Weekly Wage', 'Birth Year', 'C-2 Date', 'C-3 Date', 'COVID-19 Indicator', 'First Hearing Date', 'Gender', 'IME-4 Count', 'Number of Dependents', 'CarrierGroup_Self-insured Private Entity', 'CarrierGroup_Self-insured Public Entity', 'CarrierGroup_Special Funds', 'CarrierGroup_State Insurance Fund', 'Industry Code_encoded_5. PPD SCH LOSS','Industry Code_encoded_2. NON-COMP','Industry Code_encoded_3. MED ONLY', 'Industry Code_encoded_4. TEMPORARY','Industry Code_encoded_1. CANCELLED', 'Industry Code_encoded_8. DEATH', 'Industry Code_encoded_6. PPD NSL', 'Industry Code_encoded_7. PTD', 'Injury_Cause_Caught_In', 'Injury_Cause_Cut_Puncture_Scrape', 'Injury_Cause_Fall_Slip_or_Trip',\n",
    "'Injury_Cause_Miscellaneous_Causes', 'Injury_Cause_Motor_Vehicle','Injury_Cause_Strain_or_Injury',\n",
    " 'Injury_Cause_Striking_Against_or_Stepping_On', 'Injury_Cause_Struck_or_Injured_By', 'Injury_Cause_Unknown',\n",
    "       'Injury_Cause_Category_encoded_5. PPD SCH LOSS',\n",
    "       'Injury_Cause_Category_encoded_2. NON-COMP',\n",
    "       'Injury_Cause_Category_encoded_3. MED ONLY',\n",
    "       'Injury_Cause_Category_encoded_4. TEMPORARY',\n",
    "       'Injury_Cause_Category_encoded_1. CANCELLED',\n",
    "       'Injury_Cause_Category_encoded_8. DEATH',\n",
    "       'Injury_Cause_Category_encoded_6. PPD NSL',\n",
    "       'Injury_Cause_Category_encoded_7. PTD', \n",
    "       'Nature_Injury_Occupational', 'Nature_Injury_Specific',\n",
    "       'Nature_Injury_Unknown',\n",
    "       'Injury_Nature_Category_encoded_5. PPD SCH LOSS',\n",
    "       'Injury_Nature_Category_encoded_2. NON-COMP',\n",
    "       'Injury_Nature_Category_encoded_3. MED ONLY',\n",
    "       'Injury_Nature_Category_encoded_4. TEMPORARY',\n",
    "       'Injury_Nature_Category_encoded_1. CANCELLED',\n",
    "       'Injury_Nature_Category_encoded_8. DEATH',\n",
    "       'Injury_Nature_Category_encoded_6. PPD NSL',\n",
    "       'Injury_Nature_Category_encoded_7. PTD',\n",
    "       'Body_Part_Lower_Extremities', 'Body_Part_Multiple_Body Parts',\n",
    "       'Body_Part_Trunk', 'Body_Part_Unknown', 'Body_Part_Upper_Extremities',\n",
    "       'Part_Body_Category_encoded_5. PPD SCH LOSS',\n",
    "       'Part_Body_Category_encoded_2. NON-COMP',\n",
    "       'Part_Body_Category_encoded_3. MED ONLY',\n",
    "       'Part_Body_Category_encoded_4. TEMPORARY',\n",
    "       'Part_Body_Category_encoded_1. CANCELLED',\n",
    "       'Part_Body_Category_encoded_8. DEATH',\n",
    "       'Part_Body_Category_encoded_6. PPD NSL',\n",
    "       'Part_Body_Category_encoded_7. PTD', 'Days_between_Acc_Assembyl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if all the columns are in each dataset \n",
    "no_missing_col_train = len((set(columns_to_keep) - set(X_train_encoded.columns)))\n",
    "no_missing_col_val = len(set(columns_to_keep) - set(X_val_encoded.columns))\n",
    "no_missing_col_test = len(set(columns_to_keep) - set(X_test_encoded.columns))\n",
    "# print for ever df the amount of missing columns in each dataset\n",
    "\n",
    "print(f'There are: {no_missing_col_train} columns missing in the training set')\n",
    "print(f'There are: {no_missing_col_val} columns missing in the validation set')\n",
    "print(f'There are: {no_missing_col_test} columns missing in the test set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __Save to csv__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_encoded[columns_to_keep].to_csv('../project_data/X_train_encoded.csv')\n",
    "X_val_encoded[columns_to_keep].to_csv('../project_data/X_val_encoded.csv')\n",
    "X_test_encoded[columns_to_keep].to_csv('../project_data/X_test_encoded.csv')\n",
    "\n",
    "y_train.to_csv('../project_data/y_train.csv')\n",
    "y_val.to_csv('../project_data/y_val.csv')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
